{"models":[{"id":"afm-4.5b","aliases":["arcee-ai/afm-4.5b","afm-4.5b"],"name":"Arcee AI: AFM 4.5B","description":{"en":"AFM-4.5B is a 4.5 billion parameter instruction-tuned language model developed by Arcee AI. The model was pretrained on approximately 8 trillion tokens, including 6.5 trillion tokens of general data and 1.5 trillion tokens with an emphasis on mathematical reasoning and code generation.","de":"AFM-4.5B ist ein von Arcee AI entwickeltes Sprachmodell mit 4,5 Milliarden Parametern, das auf Anweisungen abgestimmt ist. Das Modell wurde mit ca. 8 Billionen Token trainiert, darunter 6,5 Billionen Token mit allgemeinen Daten und 1,5 Billionen Token mit Schwerpunkt auf mathematischen Schlussfolgerungen und Codegenerierung."},"knowledge":"2025-09-16","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":65536,"price":{"usd":{"currency":"usd","input":"0.048","output":"0.15"},"eur":{"currency":"eur","input":"0.0414101472","output":"0.12940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.763Z","deprecated":true},{"id":"ai21-jamba-1.5-large","aliases":["ai21-labs/ai21-jamba-1.5-large","ai21-jamba-1.5-large"],"name":"AI21 Jamba 1.5 Large","reasoning":true,"toolCalling":true,"knowledge":"2024-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":256000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.726Z"},{"id":"ai21-jamba-1.5-mini","aliases":["ai21-labs/ai21-jamba-1.5-mini","ai21-jamba-1.5-mini"],"name":"AI21 Jamba 1.5 Mini","reasoning":true,"toolCalling":true,"knowledge":"2024-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":256000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.726Z"},{"id":"ai21.jamba-1-5-large-v1:0","aliases":["ai21.jamba-1-5-large-v1:0"],"name":"Jamba 1.5 Large","toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":256000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.7254228","output":"6.9016912"}}}],"lastImportedAt":"2025-11-19T12:06:32.758Z"},{"id":"ai21.jamba-1-5-mini-v1:0","aliases":["ai21.jamba-1-5-mini-v1:0"],"name":"Jamba 1.5 Mini","toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":256000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.40"},"eur":{"currency":"eur","input":"0.17254228","output":"0.34508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.760Z"},{"id":"aion-1.0","aliases":["aion-labs/aion-1.0","aion-1.0"],"name":"AionLabs: Aion-1.0","description":{"en":"Aion-1.0 is a multi-model system designed for high performance across various tasks, including reasoning and coding. It is built on DeepSeek-R1, augmented with additional models and techniques such as Tree of Thoughts (ToT) and Mixture of Experts (MoE). It is Aion Lab's most powerful reasoning model.","de":"Aion-1.0 ist ein Multi-Modell-System, das für eine hohe Leistung bei verschiedenen Aufgaben, einschließlich Schlussfolgerungen und Kodierung, ausgelegt ist. Es basiert auf DeepSeek-R1 und wurde um zusätzliche Modelle und Techniken wie Tree of Thoughts (ToT) und Mixture of Experts (MoE) erweitert. Es ist das leistungsfähigste Denkmodell von Aion Lab."},"knowledge":"2025-02-04","reasoning":true,"input":["text"],"output":["text"],"parameters":["include_reasoning","max_tokens","reasoning","temperature","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"4","output":"8"},"eur":{"currency":"eur","input":"3.4508456","output":"6.9016912"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"aion-1.0-mini","aliases":["fuseai/fuseo1-deepseekr1-qwq-skyt1-32b-preview","fuseo1-deepseekr1-qwq-skyt1-32b-preview","aion-labs/aion-1.0-mini","aion-1.0-mini"],"name":"AionLabs: Aion-1.0-Mini","description":{"en":"Aion-1.0-Mini 32B parameter model is a distilled version of the DeepSeek-R1 model, designed for strong performance in reasoning domains such as mathematics, coding, and logic. It is a modified variant of a FuseAI model that outperforms R1-Distill-Qwen-32B and R1-Distill-Llama-70B, with benchmark results available on its [Hugging Face page](https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview), independently replicated for verification.","de":"Das Parametermodell Aion-1.0-Mini 32B ist eine destillierte Version des DeepSeek-R1-Modells, das für starke Leistungen in Bereichen wie Mathematik, Codierung und Logik entwickelt wurde. Es ist eine modifizierte Variante eines FuseAI-Modells, das die Modelle R1-Distill-Qwen-32B und R1-Distill-Llama-70B übertrifft. Benchmark-Ergebnisse sind auf der Seite [Hugging Face] (https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview) verfügbar und wurden zur Überprüfung unabhängig repliziert."},"knowledge":"2025-02-04","reasoning":true,"input":["text"],"output":["text"],"parameters":["include_reasoning","max_tokens","reasoning","temperature","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.7","output":"1.4"},"eur":{"currency":"eur","input":"0.60389798","output":"1.20779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"aion-rp-llama-3.1-8b","aliases":["aion-labs/aion-rp-llama-3.1-8b","aion-rp-llama-3.1-8b"],"name":"AionLabs: Aion-RP 1.0 (8B)","description":{"en":"Aion-RP-Llama-3.1-8B ranks the highest in the character evaluation portion of the RPBench-Auto benchmark, a roleplaying-specific variant of Arena-Hard-Auto, where LLMs evaluate each other’s responses. It is a fine-tuned base model rather than an instruct model, designed to produce more natural and varied writing.","de":"Aion-RP-Llama-3.1-8B belegt den höchsten Rang im Charakterbewertungsteil des RPBench-Auto-Benchmarks, einer rollenspielspezifischen Variante von Arena-Hard-Auto, bei der LLMs die Antworten der anderen bewerten. Es handelt sich um ein fein abgestimmtes Basismodell und nicht um ein Instruktionsmodell, das darauf ausgelegt ist, eine natürlichere und abwechslungsreichere Schreibweise zu erzeugen."},"knowledge":"2025-02-04","input":["text"],"output":["text"],"parameters":["max_tokens","temperature","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.2","output":"0.2"},"eur":{"currency":"eur","input":"0.17254228","output":"0.17254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"alpha-doubao-seed-code","aliases":["alpha-doubao-seed-code"],"name":"Doubao Seed Code (alpha)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"opencode","contextLength":256000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.729Z"},{"id":"alpha-gd4","aliases":["alpha-gd4"],"name":"Alpha GD4","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"opencode","contextLength":262144,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.43","output":"1.74"}}}],"lastImportedAt":"2025-11-27T00:20:23.107Z","outputLimit":32768,"contextLength":262144},{"id":"alpha-gd5","aliases":["alpha-gd5"],"name":"Code GD5 (alpha)","reasoning":true,"toolCalling":true,"knowledge":"2025-01-01","input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"opencode","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.50","output":"15.00"},"eur":{"currency":"eur","input":"1.3","output":"13.03"}}}],"lastImportedAt":"2025-11-24T00:22:13.913Z","outputLimit":128000,"contextLength":400000,"deprecated":true},{"id":"alpha-kimi-k2-thinking","aliases":["alpha-kimi-k2-thinking"],"name":"Kimi K2 Thinking (alpha)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"opencode","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.51762684","output":"2.1567785"}}}],"lastImportedAt":"2025-11-19T12:06:32.728Z","deprecated":true},{"id":"alpha-minimax-m2","aliases":["alpha-minimax-m2"],"name":"MiniMax M2 (alpha)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"opencode","contextLength":204800,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}}],"lastImportedAt":"2025-11-19T12:06:32.729Z"},{"id":"amazon.nova-lite-v1:0","aliases":["amazon.nova-lite-v1:0"],"name":"Nova Lite","toolCalling":true,"knowledge":"2024-10-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":300000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.24"},"eur":{"currency":"eur","input":"0.051762684","output":"0.207050736"}}}],"lastImportedAt":"2025-11-19T12:06:32.760Z"},{"id":"amazon.nova-micro-v1:0","aliases":["amazon.nova-micro-v1:0"],"name":"Nova Micro","toolCalling":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.14"},"eur":{"currency":"eur","input":"0.034508456","output":"0.120779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.758Z"},{"id":"amazon.nova-premier-v1:0","aliases":["amazon.nova-premier-v1:0"],"name":"Nova Premier","reasoning":true,"toolCalling":true,"knowledge":"2024-10-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":1000000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.50","output":"12.50"},"eur":{"currency":"eur","input":"2.1567785","output":"10.7838925"}}}],"lastImportedAt":"2025-11-19T12:06:32.760Z"},{"id":"amazon.nova-pro-v1:0","aliases":["amazon.nova-pro-v1:0"],"name":"Nova Pro","toolCalling":true,"knowledge":"2024-10-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":300000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.80","output":"3.20"},"eur":{"currency":"eur","input":"0.69016912","output":"2.76067648"}}}],"lastImportedAt":"2025-11-19T12:06:32.758Z"},{"id":"anthropic--claude-3-haiku","aliases":["anthropic--claude-3-haiku"],"name":"anthropic--claude-3-haiku","toolCalling":true,"knowledge":"2023-08-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"sap-ai-core","contextLength":200000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.25"},"eur":{"currency":"eur","input":"0.21","output":"1.07"}}}],"lastImportedAt":"2025-12-05T12:09:20.592Z","outputLimit":4096,"contextLength":200000},{"id":"anthropic--claude-3-opus","aliases":["anthropic--claude-3-opus"],"name":"anthropic--claude-3-opus","toolCalling":true,"knowledge":"2023-08-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"sap-ai-core","contextLength":200000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.86","output":"64.3"}}}],"lastImportedAt":"2025-12-05T12:09:20.957Z","outputLimit":4096,"contextLength":200000},{"id":"anthropic--claude-3-sonnet","aliases":["anthropic--claude-3-sonnet"],"name":"anthropic--claude-3-sonnet","toolCalling":true,"knowledge":"2023-08-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"sap-ai-core","contextLength":200000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.86"}}}],"lastImportedAt":"2025-12-05T12:09:20.681Z","outputLimit":4096,"contextLength":200000},{"id":"anthropic--claude-3.5-sonnet","aliases":["anthropic--claude-3.5-sonnet"],"name":"anthropic--claude-3.5-sonnet","toolCalling":true,"knowledge":"2024-04-30","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"sap-ai-core","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.86"}}}],"lastImportedAt":"2025-12-05T12:09:20.417Z","outputLimit":8192,"contextLength":200000},{"id":"anthropic--claude-3.7-sonnet","aliases":["anthropic--claude-3.7-sonnet"],"name":"anthropic--claude-3.7-sonnet","reasoning":true,"toolCalling":true,"knowledge":"2024-10-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"sap-ai-core","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.86"}}}],"lastImportedAt":"2025-12-05T12:09:20.769Z","outputLimit":64000,"contextLength":200000},{"id":"anthropic--claude-4-opus","aliases":["anthropic--claude-4-opus"],"name":"anthropic--claude-4-opus","reasoning":true,"toolCalling":true,"knowledge":"2025-01-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"sap-ai-core","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.86","output":"64.3"}}}],"lastImportedAt":"2025-12-05T12:09:20.505Z","outputLimit":32000,"contextLength":200000},{"id":"anthropic--claude-4-sonnet","aliases":["anthropic--claude-4-sonnet"],"name":"anthropic--claude-4-sonnet","reasoning":true,"toolCalling":true,"knowledge":"2025-01-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"sap-ai-core","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.86"}}}],"lastImportedAt":"2025-12-05T12:09:21.047Z","outputLimit":64000,"contextLength":200000},{"id":"anthropic--claude-4.5-sonnet","aliases":["anthropic--claude-4.5-sonnet"],"name":"anthropic--claude-4.5-sonnet","reasoning":true,"toolCalling":true,"knowledge":"2025-01-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"sap-ai-core","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.86"}}}],"lastImportedAt":"2025-12-05T12:09:20.856Z","outputLimit":64000,"contextLength":200000},{"id":"anthropic.claude-instant-v1","aliases":["anthropic.claude-instant-v1"],"name":"Claude Instant","knowledge":"2023-08-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"amazon-bedrock","contextLength":100000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.80","output":"2.40"},"eur":{"currency":"eur","input":"0.69016912","output":"2.07050736"}}}],"lastImportedAt":"2025-11-19T12:06:32.759Z"},{"id":"anthropic.claude-v2","aliases":["anthropic.claude-v2"],"name":"Claude 2","knowledge":"2023-08-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"amazon-bedrock","contextLength":100000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"8.00","output":"24.00"},"eur":{"currency":"eur","input":"6.9016912","output":"20.7050736"}}}],"lastImportedAt":"2025-11-19T12:06:32.757Z"},{"id":"anthropic.claude-v2:1","aliases":["anthropic.claude-v2:1"],"name":"Claude 2.1","knowledge":"2023-08-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"amazon-bedrock","contextLength":200000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"8.00","output":"24.00"},"eur":{"currency":"eur","input":"6.9016912","output":"20.7050736"}}}],"lastImportedAt":"2025-11-19T12:06:32.757Z"},{"id":"anubis-70b-v1.1","aliases":["thedrummer/anubis-70b-v1.1","anubis-70b-v1.1"],"name":"TheDrummer: Anubis 70B V1.1","description":{"en":"TheDrummer's Anubis v1.1 is an unaligned, creative Llama 3.3 70B model focused on providing character-driven roleplay & stories. It excels at gritty, visceral prose, unique character adherence, and coherent narratives, while maintaining the instruction following Llama 3.3 70B is known for.","de":"TheDrummer's Anubis v1.1 ist ein bündnisfreies, kreatives Llama 3.3 70B Modell, das sich auf charaktergetriebenes Rollenspiel und Geschichten konzentriert. Es zeichnet sich durch düstere, viszerale Prosa, einzigartige Charaktere und kohärente Erzählungen aus, während es die Anweisungen beibehält, für die Llama 3.3 70B bekannt ist."},"knowledge":"2025-06-29","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.75","output":"1"},"eur":{"currency":"eur","input":"0.65","output":"0.86"}}}],"lastImportedAt":"2025-11-27T12:08:37.149Z","contextLength":131072},{"id":"aura-1","aliases":["workers-ai/aura-1","aura-1"],"name":"@cf/deepgram/aura-1","openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.01","output":"0.01"},"eur":{"currency":"eur","input":"0.01","output":"0.01"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.386Z","outputLimit":16384,"contextLength":128000},{"id":"aura-2-en","aliases":["workers-ai/aura-2-en","aura-2-en"],"name":"aura 2 en","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.263Z","outputLimit":16384,"contextLength":128000},{"id":"aura-2-es","aliases":["workers-ai/aura-2-es","aura-2-es"],"name":"aura 2 es","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.232Z","outputLimit":16384,"contextLength":128000},{"id":"auto","aliases":["openrouter/auto","auto"],"description":{"en":"Your prompt will be processed by a meta-model and routed to one of dozens of models (see below), optimizing for the best possible output. To see which model was used, visit [Activity](/activity), or read the `model` attribute of the response. Your response will be priced at the same rate as the routed model. The meta-model is powered by [Not Diamond](https://docs.notdiamond.ai/docs/how-not-diamond-works). Learn more in our [docs](/docs/model-routing). Requests will be routed to the following models: - [openai/gpt-5](/openai/gpt-5) - [openai/gpt-5-mini](/openai/gpt-5-mini) - [openai/gpt-5-nano](/openai/gpt-5-nano) - [openai/gpt-4.1-nano](/openai/gpt-4.1-nano) - [openai/gpt-4.1](/openai/gpt-4.1) - [openai/gpt-4.1-mini](/openai/gpt-4.1-mini) - [openai/gpt-4.1](/openai/gpt-4.1) - [openai/gpt-4o-mini](/openai/gpt-4o-mini) - [openai/chatgpt-4o-latest](/openai/chatgpt-4o-latest) - [anthropic/claude-3.5-haiku](/anthropic/claude-3.5-haiku) - [anthropic/claude-opus-4-1](/anthropic/claude-opus-4-1) - [anthropic/claude-sonnet-4-0](/anthropic/claude-sonnet-4-0) - [anthropic/claude-3-7-sonnet-latest](/anthropic/claude-3-7-sonnet-latest) - [google/gemini-2.5-pro](/google/gemini-2.5-pro) - [google/gemini-2.5-flash](/google/gemini-2.5-flash) - [mistral/mistral-large-latest](/mistral/mistral-large-latest) - [mistral/mistral-medium-latest](/mistral/mistral-medium-latest) - [mistral/mistral-small-latest](/mistral/mistral-small-latest) - [mistralai/mistral-nemo](/mistralai/mistral-nemo) - [x-ai/grok-3](/x-ai/grok-3) - [x-ai/grok-3-mini](/x-ai/grok-3-mini) - [x-ai/grok-4](/x-ai/grok-4) - [deepseek/deepseek-r1](/deepseek/deepseek-r1) - [meta-llama/llama-3.1-70b-instruct](/meta-llama/llama-3.1-70b-instruct) - [meta-llama/llama-3.1-405b-instruct](/meta-llama/llama-3.1-405b-instruct) - [mistralai/mixtral-8x22b-instruct](/mistralai/mixtral-8x22b-instruct) - [perplexity/sonar](/perplexity/sonar) - [cohere/command-r-plus](/cohere/command-r-plus) - [cohere/command-r](/cohere/command-r)","de":"Ihre Eingabeaufforderung wird von einem Metamodell verarbeitet und an eines von Dutzenden von Modellen (siehe unten) weitergeleitet, die für die bestmögliche Ausgabe optimiert sind. Um zu sehen, welches Modell verwendet wurde, besuchen Sie [Aktivität](/aktivität) oder lesen Sie das Attribut `Modell` der Antwort. Für Ihre Antwort wird der gleiche Preis wie für das weitergeleitete Modell berechnet. Das Metamodell wird von [Not Diamond](https://docs.notdiamond.ai/docs/how-not-diamond-works) unterstützt. Erfahren Sie mehr in unseren [docs](/docs/model-routing). Die Anfragen werden an die folgenden Modelle weitergeleitet: - [openai/gpt-5](/openai/gpt-5) - [openai/gpt-5-mini](/openai/gpt-5-mini) - [openai/gpt-5-nano](/openai/gpt-5-nano) - [openai/gpt-4.1-nano](/openai/gpt-4.1-nano) - [openai/gpt-4.1](/openai/gpt-4.1) - [openai/gpt-4.1-mini](/openai/gpt-4.1-mini) - [openai/gpt-4.1](/openai/gpt-4.1) - [openai/gpt-4o-mini](/openai/gpt-4o-mini) - [openai/chatgpt-4o-latest](/openai/chatgpt-4o-latest) - [anthropic/claude-3.5-haiku](/anthropic/claude-3.5-haiku) - [anthropic/claude-opus-4-1](/anthropic/claude-opus-4-1) - [anthropic/claude-sonnet-4-0](/anthropic/claude-sonnet-4-0) - [anthropic/claude-3-7-sonnet-latest](/anthropic/claude-3-7-sonnet-latest) - [google/gemini-2.5-pro](/google/gemini-2.5-pro) - [google/gemini-2.5-flash](/google/gemini-2.5-flash) - [mistral/mistral-groß-aktuell](/mistral/mistral-groß-aktuell) - [mistral/mistral-medium-latest](/mistral/mistral-medium-latest) - [mistral/mistral-klein-aktuell](/mistral/mistral-klein-aktuell) - [mistralai/mistral-nemo](/mistralai/mistral-nemo) - [x-ai/grok-3](/x-ai/grok-3) - [x-ai/grok-3-mini](/x-ai/grok-3-mini) - [x-ai/grok-4](/x-ai/grok-4) - [deepseek/deepseek-r1](/deepseek/deepseek-r1) - [meta-llama/llama-3.1-70b-instruct](/meta-llama/llama-3.1-70b-instruct) - [meta-llama/llama-3.1-405b-instruct](/meta-llama/llama-3.1-405b-instruct) - [mistralai/mixtral-8x22b-Anleitung](/mistralai/mixtral-8x22b-Anleitung) - [perplexity/sonar](/perplexity/sonar) - [cohere/command-r-plus](/cohere/command-r-plus) - [cohere/command-r](/cohere/command-r)"},"name":"Auto","knowledge":"2023-11-08","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"morph","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.85","output":"1.55"},"eur":{"currency":"eur","input":"0.73330469","output":"1.33720267"}}},{"providerId":"openrouter","contextLength":2000000,"price":{"usd":{"currency":"usd","input":"-1000000","output":"-1000000"},"eur":{"currency":"eur","input":"-862711.4","output":"-862711.4"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.755Z"},{"id":"baidu-ernie-4.5-300b-a47b","aliases":["baidu-ernie-4.5-300b-a47b"],"name":"baidu/ERNIE-4.5-300B-A47B","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.28","output":"1.10"},"eur":{"currency":"eur","input":"0.24","output":"0.96"}}}],"lastImportedAt":"2025-11-26T00:20:50.384Z","outputLimit":131000,"contextLength":131000},{"id":"bart-large-cnn","aliases":["workers-ai/bart-large-cnn","bart-large-cnn"],"name":"@cf/facebook/bart-large-cnn","openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.742Z","outputLimit":16384,"contextLength":128000},{"id":"bert-nebulon-alpha","aliases":["openrouter/bert-nebulon-alpha","bert-nebulon-alpha"],"name":"Bert-Nebulon Alpha","description":{"en":"This is a cloaked model provided to the community to gather feedback. A general-purpose multimodal model (text/image in, text out) designed for reliability, long-context comprehension, and adaptive logic. It is engineered for production-grade assistants, retrieval-augmented systems, science workloads, and complex agentic workflows. **Note:** All prompts and completions for this model are logged by the provider and may be used to improve the model.","de":"Dies ist ein getarntes Modell, das der Gemeinschaft zur Verfügung gestellt wird, um Feedback einzuholen. Ein universelles multimodales Modell (Text/Bild in, Text out), das auf Zuverlässigkeit, langes Kontextverständnis und adaptive Logik ausgelegt ist. Es wurde für produktionsreife Assistenten, Retrieval-erweiterte Systeme, wissenschaftliche Arbeitslasten und komplexe agentenbasierte Arbeitsabläufe entwickelt. **Hinweis:** Alle Eingabeaufforderungen und Vervollständigungen für dieses Modell werden vom Anbieter protokolliert und können zur Verbesserung des Modells verwendet werden."},"knowledge":"2025-11-24","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["max_tokens","response_format","structured_outputs","tool_choice","tools"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":256000,"price":{"usd":{"currency":"usd","input":"0","output":"0"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-29T00:20:01.311Z","contextLength":256000,"deprecated":true},{"id":"bge-base-en-v1.5","aliases":["workers-ai/bge-base-en-v1.5","bge-base-en-v1.5"],"name":"uge uase en v1.5","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.052Z","outputLimit":16384,"contextLength":128000},{"id":"bge-large-en-v1.5","aliases":["workers-ai/bge-large-en-v1.5","bge-large-en-v1.5"],"name":"uge large en v1.5","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.162Z","outputLimit":16384,"contextLength":128000},{"id":"bge-m3","aliases":["workers-ai/bge-m3","bge-m3"],"name":"uge m3","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.949Z","outputLimit":16384,"contextLength":128000},{"id":"bge-multilingual-gemma2","aliases":["bge-multilingual-gemma2"],"name":"BGE Multilingual Gemma2","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"scaleway","contextLength":8191,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.00"},"eur":{"currency":"eur","input":"0.112152482","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.756Z"},{"id":"bge-reranker-base","aliases":["workers-ai/bge-reranker-base","bge-reranker-base"],"name":"uge reranker uase","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.203Z","outputLimit":16384,"contextLength":128000},{"id":"bge-small-en-v1.5","aliases":["workers-ai/bge-small-en-v1.5","bge-small-en-v1.5"],"name":"uge small en v1.5","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.326Z","outputLimit":16384,"contextLength":128000},{"id":"big-pickle","aliases":["big-pickle"],"name":"Big Pickle","reasoning":true,"toolCalling":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"opencode","contextLength":200000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.729Z"},{"id":"bodybuilder","aliases":["openrouter/bodybuilder","bodybuilder"],"name":"Body Builder","description":{"en":"Transform your natural language requests into structured OpenRouter API request objects. Describe what you want to accomplish with AI models, and Body Builder will construct the appropriate API calls. Example: \"count to 10 using gemini and opus.\" This is useful for creating multi-model requests, custom model routers, or programmatic generation of API calls from human descriptions.","de":"Wandeln Sie Ihre natürlichsprachlichen Anfragen in strukturierte OpenRouter-API-Anfrageobjekte um. Beschreiben Sie, was Sie mit KI-Modellen erreichen wollen, und der Body Builder erstellt die entsprechenden API-Aufrufe. Beispiel: \"Zählen bis 10 mit Gemini und Opus\". Dies ist nützlich für die Erstellung von Multi-Modell-Anfragen, benutzerdefinierte Modell-Router oder die programmatische Generierung von API-Aufrufen aus menschlichen Beschreibungen."},"knowledge":"2025-12-05","input":["text"],"output":["text"],"parameters":[],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"-1000000","output":"-1000000"},"eur":{"currency":"eur","input":"-857391.77","output":"-857391.77"}}}],"lastImportedAt":"2025-12-05T12:09:21.139Z","contextLength":128000,"deprecated":true},{"id":"bytedance-seed-seed-oss-36b-instruct","aliases":["bytedance-seed-seed-oss-36b-instruct"],"name":"ByteDance-Seed/Seed-OSS-36B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.21","output":"0.57"},"eur":{"currency":"eur","input":"0.18","output":"0.5"}}}],"lastImportedAt":"2025-11-26T00:20:49.491Z","outputLimit":262000,"contextLength":262000},{"id":"cerebras-llama-4-maverick-17b-128e-instruct","aliases":["cerebras-llama-4-maverick-17b-128e-instruct"],"name":"Cerebras-Llama-4-Maverick-17B-128E-Instruct","toolCalling":true,"openWeights":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"llama","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.756Z"},{"id":"cerebras-llama-4-scout-17b-16e-instruct","aliases":["cerebras-llama-4-scout-17b-16e-instruct"],"name":"Cerebras-Llama-4-Scout-17B-16E-Instruct","toolCalling":true,"openWeights":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"llama","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.756Z"},{"id":"claude-3-haiku","aliases":["anthropic/claude-3-haiku","claude-3-haiku"],"description":{"en":"Claude 3 Haiku is Anthropic's fastest and most compact model for near-instant responsiveness. Quick and accurate targeted performance. See the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku) #multimodal","de":"Claude 3 Haiku ist das schnellste und kompakteste Modell von Anthropic für nahezu sofortige Reaktionsfähigkeit. Schnelle und präzise Zielführung. Siehe die Ankündigung der Markteinführung und die Benchmark-Ergebnisse [hier] (https://www.anthropic.com/news/claude-3-haiku) #multimodal"},"name":"Claude Haiku 3","toolCalling":true,"knowledge":"2023-08-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools","max_tokens","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"vercel","contextLength":200000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.25"},"eur":{"currency":"eur","input":"0.21","output":"1.07"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"1.25"},"eur":{"currency":"eur","input":"0","output":"1.07"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.25"},"eur":{"currency":"eur","input":"0.21","output":"1.07"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.960Z","outputLimit":4096,"contextLength":128000},{"id":"claude-3-opus","aliases":["anthropic/claude-3-opus","claude-3-opus"],"description":{"en":"Claude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding. See the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family) #multimodal","de":"Claude 3 Opus ist das leistungsstärkste Modell von Anthropic für hochkomplexe Aufgaben. Es zeichnet sich durch ein Höchstmaß an Leistung, Intelligenz, Sprachgewandtheit und Verständnis aus. Siehe die Ankündigung der Markteinführung und die Benchmark-Ergebnisse [hier] (https://www.anthropic.com/news/claude-3-family) #multimodal"},"name":"Claude Opus 3","toolCalling":true,"knowledge":"2023-08-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools","max_tokens","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"vercel","contextLength":200000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.88","output":"64.42"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.88","output":"64.42"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"15","output":"75"},"eur":{"currency":"eur","input":"12.88","output":"64.42"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.959Z","outputLimit":4096,"contextLength":128000},{"id":"claude-3-sonnet","aliases":["anthropic/claude-3-sonnet","claude-3-sonnet"],"name":"claude 3 sonnet","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}}],"lastImportedAt":"2025-12-08T00:21:42.419Z","outputLimit":16384,"contextLength":128000},{"id":"claude-3.5-haiku","aliases":["anthropic/claude-3.5-haiku","anthropic/claude-3-5-haiku","claude-3.5-haiku","claude-3-5-haiku"],"description":{"en":"Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions. This makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems. This model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).","de":"Claude 3.5 Haiku bietet verbesserte Funktionen in Bezug auf Geschwindigkeit, Codiergenauigkeit und Werkzeugnutzung. Es wurde für Echtzeitanwendungen entwickelt und liefert schnelle Reaktionszeiten, die für dynamische Aufgaben wie Chat-Interaktionen und sofortige Codierungsvorschläge unerlässlich sind. Dadurch eignet es sich hervorragend für Umgebungen, in denen sowohl Geschwindigkeit als auch Präzision gefragt sind, wie z. B. Softwareentwicklung, Kundendienst-Bots und Datenverwaltungssysteme. Dieses Modell verweist derzeit auf [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022)."},"name":"Claude Haiku 3.5","toolCalling":true,"knowledge":"2024-07-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools","max_tokens","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"vercel","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.80","output":"4.00"},"eur":{"currency":"eur","input":"0.69","output":"3.43"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.80","output":"4.00"},"eur":{"currency":"eur","input":"0.69","output":"3.43"}}},{"providerId":"opencode","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.80","output":"4.00"},"eur":{"currency":"eur","input":"0.69","output":"3.43"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"4.00"},"eur":{"currency":"eur","input":"0","output":"3.43"}}},{"providerId":"openrouter","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.80","output":"4.00"},"eur":{"currency":"eur","input":"0.69","output":"3.43"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.340Z","outputLimit":8192,"contextLength":128000},{"id":"claude-3.5-sonnet","aliases":["anthropic/claude-3.5-sonnet","claude-3.5-sonnet"],"description":{"en":"New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at: - Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding - Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights - Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone - Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems) #multimodal","de":"Der neue Claude 3.5 Sonnet bietet bessere Fähigkeiten als der Opus und höhere Geschwindigkeiten als der Sonnet, und das zu denselben Sonnet-Preisen. Sonnet ist besonders gut in: - Codierung: Erzielt ~49% im SWE-Bench Verified, höher als der letzte Bestwert, und das ohne ein ausgefallenes Prompt-Gerüst - Datenwissenschaft: Erweitert menschliches Fachwissen in der Datenwissenschaft; navigiert durch unstrukturierte Daten und nutzt mehrere Tools, um Erkenntnisse zu gewinnen - Visuelle Verarbeitung: Hervorragende Interpretation von Diagrammen, Grafiken und Bildern, genaue Transkription von Text, um Erkenntnisse zu gewinnen, die über den reinen Text hinausgehen - Agentische Aufgaben: Außergewöhnliche Nutzung von Tools, die sich hervorragend für agentische Aufgaben eignen (d. h. komplexe, mehrstufige Problemlösungsaufgaben, die die Zusammenarbeit mit anderen Systemen erfordern) #multimodal"},"name":"Claude Sonnet 3.5","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","max_tokens","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"github-copilot","contextLength":90000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"6","output":"30"},"eur":{"currency":"eur","input":"5.15","output":"25.77"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.945Z","outputLimit":8192,"contextLength":90000},{"id":"claude-3.5-sonnet-v2","aliases":["claude-3.5-sonnet-v2"],"name":"Anthropic: Claude 3.5 Sonnet v2","toolCalling":true,"knowledge":"2024-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"helicone","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}}],"lastImportedAt":"2025-12-09T00:20:58.705Z","outputLimit":8192,"contextLength":200000},{"id":"claude-3.7-sonnet","aliases":["anthropic/claude-3-7-sonnet-20250219","anthropic/claude-3.7-sonnet","anthropic/claude-3-7-sonnet","claude-3-7-sonnet-20250219","claude-3.7-sonnet","claude-3-7-sonnet"],"description":{"en":"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. Claude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks. Read more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)","de":"Claude 3.7 Sonnet ist ein fortschrittliches großes Sprachmodell mit verbesserten Argumentations-, Codierungs- und Problemlösungsfähigkeiten. Es führt einen hybriden Denkansatz ein, der es dem Benutzer ermöglicht, zwischen schnellen Antworten und erweiterter, schrittweiser Verarbeitung komplexer Aufgaben zu wählen. Das Modell zeigt bemerkenswerte Verbesserungen bei der Kodierung, insbesondere bei der Front-End-Entwicklung und Full-Stack-Updates, und zeichnet sich durch agentenbasierte Workflows aus, bei denen es autonom durch mehrstufige Prozesse navigieren kann. Claude 3.7 Sonnet behält im Standardmodus die gleiche Leistung wie sein Vorgänger bei, bietet aber einen erweiterten Argumentationsmodus für höhere Genauigkeit bei mathematischen, kodierenden und anweisungsgebundenen Aufgaben. Lesen Sie mehr im [Blogbeitrag hier](https://www.anthropic.com/news/claude-3-7-sonnet)"},"name":"Claude Sonnet 3.7","reasoning":true,"toolCalling":true,"knowledge":"2024-01-01","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"github-copilot","contextLength":200000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"openrouter","contextLength":200000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"requesty","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.332Z","outputLimit":16384,"contextLength":200000},{"id":"claude-3.7-sonnet-thought","aliases":["claude-3.7-sonnet-thought"],"name":"Claude Sonnet 3.7 Thinking","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-copilot","contextLength":200000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.702Z"},{"id":"claude-3.7-sonnet:thinking","aliases":["anthropic/claude-3.7-sonnet:thinking","anthropic/claude-3-7-sonnet-20250219","claude-3.7-sonnet:thinking","claude-3-7-sonnet-20250219"],"name":"Anthropic: Claude 3.7 Sonnet (thinking)","description":{"en":"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. Claude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks. Read more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)","de":"Claude 3.7 Sonnet ist ein fortschrittliches großes Sprachmodell mit verbesserten Argumentations-, Codierungs- und Problemlösungsfähigkeiten. Es führt einen hybriden Denkansatz ein, der es dem Benutzer ermöglicht, zwischen schnellen Antworten und erweiterter, schrittweiser Verarbeitung komplexer Aufgaben zu wählen. Das Modell zeigt bemerkenswerte Verbesserungen bei der Kodierung, insbesondere bei der Front-End-Entwicklung und Full-Stack-Updates, und zeichnet sich durch agentenbasierte Workflows aus, bei denen es autonom durch mehrstufige Prozesse navigieren kann. Claude 3.7 Sonnet behält im Standardmodus die gleiche Leistung wie sein Vorgänger bei, bietet aber einen erweiterten Argumentationsmodus für höhere Genauigkeit bei mathematischen, kodierenden und anweisungsgebundenen Aufgaben. Lesen Sie mehr im [Blogbeitrag hier](https://www.anthropic.com/news/claude-3-7-sonnet)"},"knowledge":"2025-02-24","reasoning":true,"toolCalling":true,"input":["text","image","file"],"output":["text"],"parameters":["include_reasoning","max_tokens","reasoning","stop","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"3","output":"15"},"eur":{"currency":"eur","input":"2.5881342","output":"12.940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"claude-4-1-opus","aliases":["anthropic/claude-4-1-opus","claude-4-1-opus"],"name":"Claude Opus 4","reasoning":true,"toolCalling":true,"knowledge":"2025-03-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.940671","output":"64.703355"}}}],"lastImportedAt":"2025-11-19T12:06:32.712Z"},{"id":"claude-4-opus","aliases":["anthropic/claude-4-opus","claude-4-opus"],"name":"Claude Opus 4","reasoning":true,"toolCalling":true,"knowledge":"2025-03-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.940671","output":"64.703355"}}}],"lastImportedAt":"2025-11-19T12:06:32.713Z"},{"id":"claude-4-sonnet","aliases":["anthropic/claude-4-sonnet","claude-4-sonnet"],"name":"Claude Sonnet 4","reasoning":true,"toolCalling":true,"knowledge":"2025-03-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.5881342","output":"12.940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.712Z"},{"id":"claude-4.5-haiku","aliases":["claude-4.5-haiku"],"name":"Anthropic: Claude 4.5 Haiku","toolCalling":true,"knowledge":"2025-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"helicone","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}}],"lastImportedAt":"2025-12-09T00:20:58.539Z","outputLimit":8192,"contextLength":200000},{"id":"claude-4.5-opus","aliases":["claude-4.5-opus"],"name":"Anthropic: Claude Opus 4.5","reasoning":true,"toolCalling":true,"knowledge":"2025-11-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"helicone","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.29","output":"21.46"}}}],"lastImportedAt":"2025-12-09T00:20:58.738Z","outputLimit":64000,"contextLength":200000},{"id":"claude-4.5-sonnet","aliases":["anthropic/claude-4.5-sonnet","claude-4.5-sonnet","claude-4-5-sonnet"],"name":"Claude Sonnet 4.5","reasoning":true,"toolCalling":true,"knowledge":"2025-07-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"cortecs","contextLength":200000,"outputLimit":200000,"price":{"usd":{"currency":"usd","input":"3.26","output":"16.30"},"eur":{"currency":"eur","input":"2.8","output":"13.99"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}}],"lastImportedAt":"2025-12-09T00:20:58.340Z","outputLimit":64000,"contextLength":200000},{"id":"claude-haiku-3","aliases":["anthropic/claude-haiku-3","claude-haiku-3"],"name":"Claude-Haiku-3","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":189096,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.21","output":"1.10"},"eur":{"currency":"eur","input":"0.18","output":"0.95"}}}],"lastImportedAt":"2025-11-21T07:05:45.616Z","outputLimit":8192,"contextLength":189096},{"id":"claude-haiku-3.5","aliases":["anthropic/claude-haiku-3.5","claude-haiku-3.5"],"name":"Claude-Haiku-3.5","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":189096,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.68","output":"3.40"},"eur":{"currency":"eur","input":"0.59","output":"2.95"}}}],"lastImportedAt":"2025-11-21T07:05:46.851Z","outputLimit":8192,"contextLength":189096},{"id":"claude-haiku-3.5-search","aliases":["anthropic/claude-haiku-3.5-search","claude-haiku-3.5-search"],"name":"Claude-Haiku-3.5-Search","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":189096,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.68","output":"3.40"},"eur":{"currency":"eur","input":"0.59","output":"2.95"}}}],"lastImportedAt":"2025-11-21T07:05:45.088Z","outputLimit":8192,"contextLength":189096},{"id":"claude-haiku-4.5","aliases":["anthropic/claude-4.5-haiku-20251001","anthropic/claude-haiku-4.5","anthropic/claude-haiku-4-5","claude-4.5-haiku-20251001","claude-haiku-4.5","claude-haiku-4-5"],"description":{"en":"Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intelligence at a fraction of the cost and latency of larger Claude models. Matching Claude Sonnet 4’s performance across reasoning, coding, and computer-use tasks, Haiku 4.5 brings frontier-level capability to real-time and high-volume applications. It introduces extended thinking to the Haiku line; enabling controllable reasoning depth, summarized or interleaved thought output, and tool-assisted workflows with full support for coding, bash, web search, and computer-use tools. Scoring >73% on SWE-bench Verified, Haiku 4.5 ranks among the world’s best coding models while maintaining exceptional responsiveness for sub-agents, parallelized execution, and scaled deployment.","de":"Claude Haiku 4.5 ist das schnellste und effizienteste Modell von Anthropic, das zu einem Bruchteil der Kosten und Latenzzeiten größerer Claude-Modelle Intelligenz auf Grenzniveau bietet. Haiku 4.5 bietet die gleiche Leistung wie Claude Sonnet 4 in den Bereichen Argumentation, Kodierung und Computernutzung und ermöglicht Echtzeit- und Massenanwendungen auf Grenzniveau. Es führt erweitertes Denken in die Haiku-Linie ein; es ermöglicht kontrollierbare Argumentationstiefe, zusammengefasste oder verschachtelte Gedankenausgabe und werkzeugunterstützte Workflows mit voller Unterstützung für Codierung, Bash, Websuche und Computernutzungstools. Mit einem Ergebnis von mehr als 73% im SWE-Bench Verified gehört Haiku 4.5 zu den besten Codierungsmodellen der Welt und bietet gleichzeitig eine außergewöhnliche Reaktionsfähigkeit für Subagenten, parallele Ausführung und skalierte Bereitstellung."},"name":"Claude Haiku 4.5","reasoning":true,"toolCalling":true,"knowledge":"2025-02-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.00","output":"1.25"},"eur":{"currency":"eur","input":"0.86","output":"1.07"}}},{"providerId":"azure","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"opencode","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"openrouter","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"zenmux","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"requesty","contextLength":200000,"outputLimit":62000,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"anthropic","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"aihubmix","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.10","output":"5.50"},"eur":{"currency":"eur","input":"0.94","output":"4.72"}}},{"providerId":"azure-cognitive-services","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"poe","contextLength":192000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.85","output":"4.30"},"eur":{"currency":"eur","input":"0.73","output":"3.69"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.089Z","outputLimit":16000,"contextLength":128000},{"id":"claude-opus-3","aliases":["anthropic/claude-opus-3","claude-opus-3"],"name":"Claude-Opus-3","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":189096,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"13.00","output":"64.00"},"eur":{"currency":"eur","input":"11.28","output":"55.53"}}}],"lastImportedAt":"2025-11-21T07:05:43.970Z","outputLimit":8192,"contextLength":189096},{"id":"claude-opus-4","aliases":["anthropic/claude-4-opus-20250522","anthropic/claude-opus-4","claude-4-opus-20250522","claude-opus-4"],"description":{"en":"Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. Read more at the [blog post here](https://www.anthropic.com/news/claude-4)","de":"Claude Opus 4 gilt zum Zeitpunkt der Veröffentlichung als das weltweit beste Codierungsmodell, das bei komplexen, langwierigen Aufgaben und Agenten-Workflows eine nachhaltige Leistung erbringt. Es setzt neue Maßstäbe im Software-Engineering und erzielt führende Ergebnisse im SWE-Bench (72,5 %) und Terminal-Bench (43,2 %). Opus 4 unterstützt erweiterte, agentenbasierte Workflows und verarbeitet Tausende von Aufgabenschritten über Stunden hinweg ohne Leistungseinbußen. Lesen Sie mehr im [Blogbeitrag hier](https://www.anthropic.com/news/claude-4)"},"name":"Claude Opus 4","reasoning":true,"toolCalling":true,"knowledge":"2025-03-31","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"github-copilot","contextLength":80000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"openrouter","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"requesty","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"poe","contextLength":192512,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"13.00","output":"64.00"},"eur":{"currency":"eur","input":"11.16","output":"54.93"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.327Z","outputLimit":16000,"contextLength":80000},{"id":"claude-opus-4-0","aliases":["claude-opus-4-0"],"name":"Claude Opus 4 (latest)","reasoning":true,"toolCalling":true,"knowledge":"2025-03-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"anthropic","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.940671","output":"64.703355"}}}],"lastImportedAt":"2025-11-19T12:06:32.755Z"},{"id":"claude-opus-4-reasoning","aliases":["anthropic/claude-opus-4-reasoning","claude-opus-4-reasoning"],"name":"Claude Opus 4 Reasoning","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":196608,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"13.00","output":"64.00"},"eur":{"currency":"eur","input":"11.28","output":"55.53"}}}],"lastImportedAt":"2025-11-21T07:05:46.073Z","outputLimit":32768,"contextLength":196608},{"id":"claude-opus-4-search","aliases":["anthropic/claude-opus-4-search","claude-opus-4-search"],"name":"Claude Opus 4 Search","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":196608,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"13.00","output":"64.00"},"eur":{"currency":"eur","input":"11.28","output":"55.53"}}}],"lastImportedAt":"2025-11-21T07:05:44.621Z","outputLimit":128000,"contextLength":196608},{"id":"claude-opus-4.5","aliases":["anthropic/claude-4.5-opus-20251124","anthropic/claude-opus-4.5","anthropic/claude-opus-4-5","claude-4.5-opus-20251124","claude-opus-4.5","claude-opus-4-5","claude-opus-45"],"description":{"en":"Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high. Opus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.","de":"Claude Opus 4.5 ist Anthropics Frontier-Reasoning-Modell, das für komplexes Software-Engineering, agentenbasierte Arbeitsabläufe und den Einsatz von Computern mit langen Laufzeiten optimiert wurde. Es bietet starke multimodale Fähigkeiten, eine konkurrenzfähige Leistung bei realen Codierungs- und Argumentationsbenchmarks und eine verbesserte Robustheit gegenüber Prompt Injection. Das Modell ist so konzipiert, dass es über verschiedene Aufwandsstufen hinweg effizient arbeitet und es Entwicklern ermöglicht, Geschwindigkeit, Tiefe und Token-Nutzung je nach Aufgabenanforderung abzuwägen. Es verfügt über einen neuen Parameter zur Steuerung der Token-Effizienz, der über den OpenRouter Verbosity-Parameter mit low, medium oder high aufgerufen werden kann. Opus 4.5 unterstützt den erweiterten Einsatz von Werkzeugen, ein erweitertes Kontextmanagement und koordinierte Multi-Agenten-Setups, wodurch es sich gut für autonome Forschung, Debugging, mehrstufige Planung und Tabellenkalkulation/Browser-Manipulation eignet. Im Vergleich zu früheren Opus-Generationen bietet Opus 4.5 erhebliche Verbesserungen bei der strukturierten Argumentation, der Ausführungszuverlässigkeit und dem Abgleich, während der Token-Overhead reduziert und die Leistung bei lang laufenden Aufgaben verbessert wird."},"name":"Claude Opus 4.5","reasoning":true,"toolCalling":true,"knowledge":"2025-03-01","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","stop","structured_outputs","tool_choice","top_k","verbosity"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.29","output":"21.46"}}},{"providerId":"venice","contextLength":202752,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"6.00","output":"30.00"},"eur":{"currency":"eur","input":"5.15","output":"25.76"}}},{"providerId":"azure","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.29","output":"21.46"}}},{"providerId":"opencode","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.29","output":"21.46"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.29","output":"21.46"}}},{"providerId":"openrouter","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.29","output":"21.46"}}},{"providerId":"requesty","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.29","output":"21.46"}}},{"providerId":"anthropic","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.29","output":"21.46"}}},{"providerId":"azure-cognitive-services","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.29","output":"21.46"}}},{"providerId":"poe","contextLength":196608,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"4.30","output":"21.00"},"eur":{"currency":"eur","input":"3.69","output":"18.03"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.091Z","outputLimit":8192,"contextLength":128000},{"id":"claude-opus-41","aliases":["anthropic/claude-4.1-opus-20250805","anthropic/claude-opus-4.1","anthropic/claude-opus-4-1","claude-4.1-opus-20250805","claude-opus-4-1","claude-opus-4.1","claude-opus-41"],"description":{"en":"Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.","de":"Claude Opus 4.1 ist eine aktualisierte Version des Vorzeigemodells von Anthropic und bietet eine verbesserte Leistung bei der Codierung, beim schlussfolgernden Denken und bei agentenbasierten Aufgaben. Es erreicht 74,5 % im SWE-Bench Verified und zeigt bemerkenswerte Verbesserungen beim Refactoring von Mehrdateien-Code, bei der Debugging-Präzision und beim detailorientierten Denken. Das Modell unterstützt erweitertes Denken mit bis zu 64K Token und ist für Aufgaben in den Bereichen Forschung, Datenanalyse und toolgestütztes Schließen optimiert."},"name":"Claude Opus 4.1","reasoning":true,"toolCalling":true,"knowledge":"2025-03-31","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"github-copilot","contextLength":80000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"opencode","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"fastrouter","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"openrouter","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"zenmux","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"requesty","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"anthropic","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"aihubmix","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"16.50","output":"82.50"},"eur":{"currency":"eur","input":"14.16","output":"70.8"}}},{"providerId":"azure-cognitive-services","contextLength":200000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.87","output":"64.37"}}},{"providerId":"poe","contextLength":196608,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"13.00","output":"64.00"},"eur":{"currency":"eur","input":"11.16","output":"54.93"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.331Z","outputLimit":16000,"contextLength":80000},{"id":"claude-sonnet-3.5","aliases":["anthropic/claude-sonnet-3.5","claude-sonnet-3.5"],"name":"Claude-Sonnet-3.5","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":189096,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.60","output":"13.00"},"eur":{"currency":"eur","input":"2.26","output":"11.28"}}}],"lastImportedAt":"2025-11-21T07:05:46.581Z","outputLimit":8192,"contextLength":189096},{"id":"claude-sonnet-3.5-june","aliases":["anthropic/claude-sonnet-3.5-june","claude-sonnet-3.5-june"],"name":"Claude-Sonnet-3.5-June","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":189096,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.60","output":"13.00"},"eur":{"currency":"eur","input":"2.26","output":"11.28"}}}],"lastImportedAt":"2025-11-21T07:05:47.077Z","outputLimit":8192,"contextLength":189096},{"id":"claude-sonnet-3.7","aliases":["anthropic/claude-sonnet-3.7","claude-sonnet-3.7"],"name":"Claude Sonnet 3.7","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":196608,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.60","output":"13.00"},"eur":{"currency":"eur","input":"2.26","output":"11.28"}}}],"lastImportedAt":"2025-11-21T07:05:44.852Z","outputLimit":32768,"contextLength":196608},{"id":"claude-sonnet-3.7-reasoning","aliases":["anthropic/claude-sonnet-3.7-reasoning","claude-sonnet-3.7-reasoning"],"name":"Claude Sonnet 3.7 Reasoning","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":196608,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"2.60","output":"13.00"},"eur":{"currency":"eur","input":"2.26","output":"11.28"}}}],"lastImportedAt":"2025-11-21T07:05:44.211Z","outputLimit":128000,"contextLength":196608},{"id":"claude-sonnet-3.7-search","aliases":["anthropic/claude-sonnet-3.7-search","claude-sonnet-3.7-search"],"name":"Claude Sonnet 3.7 Search","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":196608,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"2.60","output":"13.00"},"eur":{"currency":"eur","input":"2.26","output":"11.28"}}}],"lastImportedAt":"2025-11-21T07:05:45.845Z","outputLimit":128000,"contextLength":196608},{"id":"claude-sonnet-4","aliases":["anthropic/claude-4-sonnet-20250522","anthropic/claude-sonnet-4","claude-4-sonnet-20250522","claude-sonnet-4"],"description":{"en":"Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios. Read more at the [blog post here](https://www.anthropic.com/news/claude-4)","de":"Claude Sonnet 4 erweitert die Fähigkeiten seines Vorgängers Sonnet 3.7 erheblich und zeichnet sich sowohl bei Codierungs- als auch bei Argumentationsaufgaben durch verbesserte Präzision und Kontrollierbarkeit aus. Mit einer Spitzenleistung im SWE-Bench (72,7 %) bietet Sonnet 4 ein ausgewogenes Verhältnis zwischen Leistungsfähigkeit und Recheneffizienz und eignet sich damit für eine breite Palette von Anwendungen, von routinemäßigen Codierungsaufgaben bis hin zu komplexen Softwareentwicklungsprojekten. Zu den wichtigsten Verbesserungen gehören eine verbesserte autonome Navigation in der Codebasis, geringere Fehlerraten in agentengesteuerten Workflows und eine höhere Zuverlässigkeit bei der Befolgung komplizierter Anweisungen. Sonnet 4 ist für den praktischen Alltagseinsatz optimiert und bietet fortschrittliche Argumentationsfähigkeiten bei gleichzeitiger Effizienz und Reaktionsfähigkeit in verschiedenen internen und externen Szenarien. Lesen Sie mehr im [Blogbeitrag hier](https://www.anthropic.com/news/claude-4)"},"name":"Claude Sonnet 4","reasoning":true,"toolCalling":true,"knowledge":"2025-03-01","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"cortecs","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.31","output":"16.54"},"eur":{"currency":"eur","input":"2.84","output":"14.19"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"opencode","contextLength":1000000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"fastrouter","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"openrouter","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"requesty","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"poe","contextLength":983040,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.60","output":"13.00"},"eur":{"currency":"eur","input":"2.23","output":"11.16"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.333Z","outputLimit":16000,"contextLength":128000},{"id":"claude-sonnet-4-0","aliases":["claude-sonnet-4-0"],"name":"Claude Sonnet 4 (latest)","reasoning":true,"toolCalling":true,"knowledge":"2025-03-31","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"anthropic","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.5881342","output":"12.940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.755Z"},{"id":"claude-sonnet-4-reasoning","aliases":["anthropic/claude-sonnet-4-reasoning","claude-sonnet-4-reasoning"],"name":"Claude Sonnet 4 Reasoning","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":983040,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"2.60","output":"13.00"},"eur":{"currency":"eur","input":"2.26","output":"11.28"}}}],"lastImportedAt":"2025-11-21T07:05:45.342Z","outputLimit":64000,"contextLength":983040},{"id":"claude-sonnet-4-search","aliases":["anthropic/claude-sonnet-4-search","claude-sonnet-4-search"],"name":"Claude Sonnet 4 Search","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":983040,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"2.60","output":"13.00"},"eur":{"currency":"eur","input":"2.26","output":"11.28"}}}],"lastImportedAt":"2025-11-21T07:05:47.331Z","outputLimit":128000,"contextLength":983040},{"id":"claude-sonnet-4.5","aliases":["anthropic/claude-4.5-sonnet-20250929","anthropic/claude-sonnet-4-5","anthropic/claude-sonnet-4.5","claude-4.5-sonnet-20250929","claude-sonnet-4.5","claude-sonnet-4-5"],"description":{"en":"Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking. Sonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use.","de":"Claude Sonnet 4.5 ist das bisher fortschrittlichste Sonnet-Modell von Anthropic, das für reale Agenten und Coding-Workflows optimiert wurde. Es bietet modernste Leistung bei Coding-Benchmarks wie SWE-Bench Verified, mit Verbesserungen in den Bereichen Systemdesign, Codesicherheit und Einhaltung von Spezifikationen. Das Modell ist für einen erweiterten autonomen Betrieb ausgelegt, wobei die Aufgabenkontinuität über Sitzungen hinweg erhalten bleibt und eine faktenbasierte Fortschrittsverfolgung möglich ist. Sonnet 4.5 führt auch stärkere agentechnische Fähigkeiten ein, einschließlich verbesserter Tool-Orchestrierung, spekulativer paralleler Ausführung und effizienterer Kontext- und Speicherverwaltung. Dank der verbesserten Kontextverfolgung und der Kenntnis der Token-Verwendung über Tool-Aufrufe hinweg eignet sich Sonnet 4.5 besonders gut für Multikontext- und langlaufende Workflows. Die Anwendungsfälle erstrecken sich auf Software-Engineering, Cybersicherheit, Finanzanalyse, Forschungsagenten und andere Bereiche, in denen eine kontinuierliche Argumentation und Toolverwendung erforderlich ist."},"name":"Claude Sonnet 4.5","reasoning":true,"toolCalling":true,"knowledge":"2025-03-31","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"opencode","contextLength":1000000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"openrouter","contextLength":1000000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"zenmux","contextLength":1000000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"requesty","contextLength":1000000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"anthropic","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"aihubmix","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.30","output":"16.50"},"eur":{"currency":"eur","input":"2.83","output":"14.17"}}},{"providerId":"azure-cognitive-services","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}},{"providerId":"poe","contextLength":983040,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.60","output":"13.00"},"eur":{"currency":"eur","input":"2.23","output":"11.17"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.953Z","outputLimit":16000,"contextLength":128000},{"id":"codellama-7b-instruct-solidity","aliases":["alfredpros/codellama-7b-instruct-solidity","codellama-7b-instruct-solidity"],"name":"AlfredPros: CodeLLaMa 7B Instruct Solidity","description":{"en":"A finetuned 7 billion parameters Code LLaMA - Instruct model to generate Solidity smart contract using 4-bit QLoRA finetuning provided by PEFT library.","de":"Ein auf 7 Milliarden Parameter abgestimmter Code LLaMA - Instruct Modell zur Erzeugung von Solidity Smart Contracts unter Verwendung von 4-Bit QLoRA Finetuning, das von der PEFT-Bibliothek bereitgestellt wird."},"knowledge":"2025-04-14","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":4096,"price":{"usd":{"currency":"usd","input":"0.8","output":"1.2"},"eur":{"currency":"eur","input":"0.69016912","output":"1.03525368"}}}],"lastImportedAt":"2025-11-19T12:06:32.765Z"},{"id":"coder-large","aliases":["arcee-ai/coder-large","coder-large"],"name":"Arcee AI: Coder Large","description":{"en":"Coder‑Large is a 32 B‑parameter offspring of Qwen 2.5‑Instruct that has been further trained on permissively‑licensed GitHub, CodeSearchNet and synthetic bug‑fix corpora. It supports a 32k context window, enabling multi‑file refactoring or long diff review in a single call, and understands 30‑plus programming languages with special attention to TypeScript, Go and Terraform. Internal benchmarks show 5–8 pt gains over CodeLlama‑34 B‑Python on HumanEval and competitive BugFix scores thanks to a reinforcement pass that rewards compilable output. The model emits structured explanations alongside code blocks by default, making it suitable for educational tooling as well as production copilot scenarios. Cost‑wise, Together AI prices it well below proprietary incumbents, so teams can scale interactive coding without runaway spend.","de":"Coder-Large ist ein 32-B-Parameter-Abkömmling von Qwen 2.5-Instruct, der auf den frei lizenzierten Korpora GitHub, CodeSearchNet und synthetischen Bugfixes weiter trainiert wurde. Es unterstützt ein 32k-Kontextfenster, das Refactoring mehrerer Dateien oder lange Diff-Reviews in einem einzigen Aufruf ermöglicht, und versteht über 30 Programmiersprachen mit besonderem Augenmerk auf TypeScript, Go und Terraform. Interne Benchmarks zeigen 5-8 Punkte mehr als CodeLlama-34 B-Python auf HumanEval und konkurrenzfähige BugFix-Ergebnisse dank eines Verstärkungspasses, der kompilierbaren Output belohnt. Das Modell gibt standardmäßig neben den Codeblöcken auch strukturierte Erklärungen aus, wodurch es sich sowohl für Ausbildungszwecke als auch für Produktionsszenarien mit Kopiloten eignet. Die Kosten von Together AI liegen deutlich unter denen der etablierten Anbieter, so dass Teams die interaktive Codierung ohne übermäßige Ausgaben skalieren können."},"knowledge":"2025-05-05","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.5","output":"0.8"},"eur":{"currency":"eur","input":"0.4313557","output":"0.69016912"}}}],"lastImportedAt":"2025-11-19T12:06:32.765Z"},{"id":"codestral","aliases":["mistral/codestral","codestral"],"name":"Codestral","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":256000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.90"},"eur":{"currency":"eur","input":"0.25881342","output":"0.77644026"}}}],"lastImportedAt":"2025-11-19T12:06:32.704Z"},{"id":"codex-mini","aliases":["openai/codex-mini","codex-mini"],"description":{"en":"codex-mini-latest is a fine-tuned version of o4-mini specifically for use in Codex CLI. For direct use in the API, we recommend starting with gpt-4.1.","de":"codex-mini-latest ist eine fein abgestimmte Version von o4-mini speziell für die Verwendung in Codex CLI. Für die direkte Verwendung in der API empfehlen wir, mit gpt-4.1 zu beginnen."},"name":"Codex Mini","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text","image"],"output":["text"],"parameters":["tools","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"azure","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.50","output":"6.00"},"eur":{"currency":"eur","input":"1.2940671","output":"5.1762684"}}},{"providerId":"azure-cognitive-services","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.50","output":"6.00"},"eur":{"currency":"eur","input":"1.2940671","output":"5.1762684"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"1.5","output":"6"},"eur":{"currency":"eur","input":"1.2940671","output":"5.1762684"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.727Z"},{"id":"cogito-2.1-671b","aliases":["cogito-2.1-671b"],"name":"Cogito 2.1 671B","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":160000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-23T00:23:30.757Z","outputLimit":8192,"contextLength":160000,"deprecated":true},{"id":"cogito-2.1:671b-cloud","aliases":["cogito-2.1:671b-cloud"],"name":"Cogito 2.1 671B","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":160000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.637Z","outputLimit":8192,"contextLength":160000},{"id":"cogito-v2.1-671b","aliases":["deepcogito/cogito-v2.1-671b-20251118","deepcogito/cogito-v2.1-671b","cogito-v2.1-671b-20251118","cogito-v2.1-671b"],"name":"Deep Cogito: Cogito v2.1 671B","description":{"en":"Cogito v2.1 671B MoE represents one of the strongest open models globally, matching performance of frontier closed and open models. This model is trained using self play with reinforcement learning to reach state-of-the-art performance on multiple categories (instruction following, coding, longer queries and creative writing). This advanced system demonstrates significant progress toward scalable superintelligence through policy improvement.","de":"Cogito v2.1 671B MoE ist eines der stärksten offenen Modelle weltweit, das die Leistung von geschlossenen und offenen Spitzenmodellen erreicht. Dieses Modell wird durch Selbstspiel mit verstärktem Lernen trainiert, um in mehreren Kategorien (Befolgung von Anweisungen, Codierung, längere Abfragen und kreatives Schreiben) Spitzenleistungen zu erzielen. Dieses fortschrittliche System ist ein bedeutender Fortschritt auf dem Weg zu skalierbarer Superintelligenz durch Verbesserung von Strategien."},"knowledge":"2025-11-13","reasoning":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"1.25"},"eur":{"currency":"eur","input":"1.0797171","output":"1.0797171"}}}],"lastImportedAt":"2025-11-19T21:41:30.122Z","contextLength":128000},{"id":"cohere-command-a","aliases":["cohere/cohere-command-a","cohere-command-a"],"name":"Cohere Command A","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":256000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"2.50","output":"10.00"},"eur":{"currency":"eur","input":"2.16","output":"8.63"}}},{"providerId":"azure-cognitive-services","contextLength":256000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"2.50","output":"10.00"},"eur":{"currency":"eur","input":"2.16","output":"8.63"}}}],"lastImportedAt":"2025-11-30T00:23:20.386Z","outputLimit":4096,"contextLength":128000},{"id":"cohere-command-r","aliases":["cohere/cohere-command-r","cohere-command-r"],"name":"Cohere Command R","reasoning":true,"toolCalling":true,"knowledge":"2024-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.722Z"},{"id":"cohere-command-r-plus","aliases":["cohere/cohere-command-r-plus","cohere-command-r-plus"],"name":"Cohere Command R+","reasoning":true,"toolCalling":true,"knowledge":"2024-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.722Z"},{"id":"cohere-embed-v-4-0","aliases":["cohere-embed-v-4-0"],"name":"Embed v4","openWeights":true,"input":["text","image"],"output":["text"],"parameters":[],"providers":[{"providerId":"azure","contextLength":128000,"outputLimit":1536,"price":{"usd":{"currency":"usd","input":"0.12","output":"0.00"},"eur":{"currency":"eur","input":"0.1","output":"0"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":1536,"price":{"usd":{"currency":"usd","input":"0.12","output":"0.00"},"eur":{"currency":"eur","input":"0.1","output":"0"}}}],"lastImportedAt":"2025-11-30T00:23:20.662Z","outputLimit":1536,"contextLength":128000},{"id":"cohere-embed-v3-english","aliases":["cohere-embed-v3-english"],"name":"Embed v3 English","openWeights":true,"input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"azure","contextLength":512,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.00"},"eur":{"currency":"eur","input":"0.09","output":"0"}}},{"providerId":"azure-cognitive-services","contextLength":512,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.00"},"eur":{"currency":"eur","input":"0.09","output":"0"}}}],"lastImportedAt":"2025-11-30T00:23:20.733Z","outputLimit":1024,"contextLength":512},{"id":"cohere-embed-v3-multilingual","aliases":["cohere-embed-v3-multilingual"],"name":"Embed v3 Multilingual","openWeights":true,"input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"azure","contextLength":512,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.00"},"eur":{"currency":"eur","input":"0.09","output":"0"}}},{"providerId":"azure-cognitive-services","contextLength":512,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.00"},"eur":{"currency":"eur","input":"0.09","output":"0"}}}],"lastImportedAt":"2025-11-30T00:23:20.677Z","outputLimit":1024,"contextLength":512},{"id":"cohere.command-light-text-v14","aliases":["cohere.command-light-text-v14"],"name":"Command Light","openWeights":true,"knowledge":"2023-08-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"amazon-bedrock","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.60"},"eur":{"currency":"eur","input":"0.25881342","output":"0.51762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.757Z"},{"id":"cohere.command-r-plus-v1:0","aliases":["cohere.command-r-plus-v1:0"],"name":"Command R+","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.5881342","output":"12.940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.757Z"},{"id":"cohere.command-r-v1:0","aliases":["cohere.command-r-v1:0"],"name":"Command R","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.50"},"eur":{"currency":"eur","input":"0.4313557","output":"1.2940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.758Z"},{"id":"cohere.command-text-v14","aliases":["cohere.command-text-v14"],"name":"Command","openWeights":true,"knowledge":"2023-08-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"amazon-bedrock","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"1.50","output":"2.00"},"eur":{"currency":"eur","input":"1.2940671","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.759Z"},{"id":"command-a","aliases":["cohereforai/c4ai-command-a-03-2025","cohere/command-a-03-2025","c4ai-command-a-03-2025","command-a-03-2025","cohere/command-a","command-a"],"name":"Cohere: Command A","description":{"en":"Command A is an open-weights 111B parameter model with a 256k context window focused on delivering great performance across agentic, multilingual, and coding use cases. Compared to other leading proprietary and open-weights models Command A delivers maximum performance with minimum hardware costs, excelling on business-critical agentic and multilingual tasks.","de":"Command A ist ein 111B-Parameter-Modell mit offener Gewichtung und einem 256k-Kontextfenster, das darauf ausgerichtet ist, in agentenbasierten, mehrsprachigen und kodierenden Anwendungsfällen eine hervorragende Leistung zu erbringen. Im Vergleich zu anderen führenden proprietären und Open-Weights-Modellen bietet Command A maximale Leistung bei minimalen Hardwarekosten und zeichnet sich bei geschäftskritischen agentenbasierten und mehrsprachigen Aufgaben aus."},"knowledge":"2025-03-13","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":256000,"price":{"usd":{"currency":"usd","input":"2.5","output":"10"},"eur":{"currency":"eur","input":"2.1567785","output":"8.627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"cosmos-nemotron-34b","aliases":["nvidia/cosmos-nemotron-34b","cosmos-nemotron-34b"],"name":"Cosmos Nemotron 34B","reasoning":true,"knowledge":"2024-01-01","input":["text","image","video"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"nvidia","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.692Z"},{"id":"cydonia-24b-v4.1","aliases":["thedrummer/cydonia-24b-v4.1","cydonia-24b-v4.1"],"name":"TheDrummer: Cydonia 24B V4.1","description":{"en":"Uncensored and creative writing model based on Mistral Small 3.2 24B with good recall, prompt adherence, and intelligence.","de":"Unzensiertes und kreatives Schreibmodell auf der Grundlage von Mistral Small 3.2 24B mit gutem Erinnerungsvermögen, prompter Befolgung und Intelligenz."},"knowledge":"2025-09-27","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.3","output":"0.5"},"eur":{"currency":"eur","input":"0.25881342","output":"0.4313557"}}}],"lastImportedAt":"2025-11-19T12:06:32.763Z"},{"id":"cypher-alpha","aliases":["openrouter/cypher-alpha:free","cypher-alpha:free","cypher-alpha"],"name":"Cypher Alpha (free)","toolCalling":true,"knowledge":"2025-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"openrouter","contextLength":1000000,"outputLimit":1000000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.751Z","deprecated":true},{"id":"dall-e-3","aliases":["openai/dall-e-3","dall-e-3"],"name":"DALL-E-3","toolCalling":true,"input":["text"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":800,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:38.492Z","contextLength":800},{"id":"deepseek-ai-deepseek-r1","aliases":["deepseek-ai-deepseek-r1"],"name":"deepseek-ai/DeepSeek-R1","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":164000,"outputLimit":164000,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.18"},"eur":{"currency":"eur","input":"0.43","output":"1.89"}}}],"lastImportedAt":"2025-11-26T00:20:46.827Z","outputLimit":164000,"contextLength":164000},{"id":"deepseek-ai-deepseek-r1-0528","aliases":["deepseek-ai-deepseek-r1-0528"],"name":"DeepSeek R1","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"io-intelligence","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.75"},"eur":{"currency":"eur","input":"1.73","output":"7.56"}}}],"lastImportedAt":"2025-11-27T12:08:36.905Z","outputLimit":4096,"contextLength":128000,"deprecated":true},{"id":"deepseek-ai-deepseek-r1-distill-qwen-14b","aliases":["deepseek-ai-deepseek-r1-distill-qwen-14b"],"name":"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.10"},"eur":{"currency":"eur","input":"0.09","output":"0.09"}}}],"lastImportedAt":"2025-11-26T00:20:48.823Z","outputLimit":131000,"contextLength":131000},{"id":"deepseek-ai-deepseek-r1-distill-qwen-32b","aliases":["deepseek-ai-deepseek-r1-distill-qwen-32b"],"name":"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.18","output":"0.18"},"eur":{"currency":"eur","input":"0.16","output":"0.16"}}}],"lastImportedAt":"2025-11-26T00:20:50.810Z","outputLimit":131000,"contextLength":131000},{"id":"deepseek-ai-deepseek-r1-distill-qwen-7b","aliases":["deepseek-ai-deepseek-r1-distill-qwen-7b"],"name":"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.05"},"eur":{"currency":"eur","input":"0.04","output":"0.04"}}}],"lastImportedAt":"2025-11-26T00:20:45.092Z","outputLimit":16000,"contextLength":33000},{"id":"deepseek-ai-deepseek-v3","aliases":["deepseek-ai-deepseek-v3"],"name":"deepseek-ai/DeepSeek-V3","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":164000,"outputLimit":164000,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.00"},"eur":{"currency":"eur","input":"0.22","output":"0.87"}}}],"lastImportedAt":"2025-11-26T00:20:48.906Z","outputLimit":164000,"contextLength":164000},{"id":"deepseek-ai-deepseek-v3.1","aliases":["deepseek-ai-deepseek-v3.1"],"name":"deepseek-ai/DeepSeek-V3.1","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":164000,"outputLimit":164000,"price":{"usd":{"currency":"usd","input":"0.27","output":"1.00"},"eur":{"currency":"eur","input":"0.23","output":"0.87"}}}],"lastImportedAt":"2025-11-26T00:20:46.912Z","outputLimit":164000,"contextLength":164000},{"id":"deepseek-ai-deepseek-v3.1-terminus","aliases":["deepseek-ai-deepseek-v3.1-terminus"],"name":"deepseek-ai/DeepSeek-V3.1-Terminus","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":164000,"outputLimit":164000,"price":{"usd":{"currency":"usd","input":"0.27","output":"1.00"},"eur":{"currency":"eur","input":"0.23","output":"0.87"}}}],"lastImportedAt":"2025-11-26T00:20:48.001Z","outputLimit":164000,"contextLength":164000},{"id":"deepseek-ai-deepseek-v3.2-exp","aliases":["deepseek-ai-deepseek-v3.2-exp"],"name":"deepseek-ai/DeepSeek-V3.2-Exp","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":164000,"outputLimit":164000,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.41"},"eur":{"currency":"eur","input":"0.23","output":"0.36"}}}],"lastImportedAt":"2025-11-26T00:20:45.985Z","outputLimit":164000,"contextLength":164000},{"id":"deepseek-ai-deepseek-vl2","aliases":["deepseek-ai-deepseek-vl2"],"name":"deepseek-ai/deepseek-vl2","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":4000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.13","output":"0.13"}}}],"lastImportedAt":"2025-11-26T00:20:45.344Z","outputLimit":4000,"contextLength":4000},{"id":"deepseek-chat","aliases":["deepseek/deepseek-chat-v3","deepseek-ai/deepseek-v3","deepseek/deepseek-chat","deepseek-chat-v3","deepseek-chat","deepseek-v3"],"description":{"en":"DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models. For model details, please visit [the DeepSeek-V3 repo](https://github.com/deepseek-ai/DeepSeek-V3) for more information, or see the [launch announcement](https://api-docs.deepseek.com/news/news1226).","de":"DeepSeek-V3 ist das neueste Modell des DeepSeek-Teams, das auf den Anweisungen und Codierungsfähigkeiten der vorherigen Versionen aufbaut. Das Modell wurde mit fast 15 Billionen Token trainiert und zeigt, dass es andere Open-Source-Modelle übertrifft und mit führenden Closed-Source-Modellen konkurriert. Weitere Informationen zum Modell finden Sie im [DeepSeek-V3 Repo] (https://github.com/deepseek-ai/DeepSeek-V3) oder in der [Launch-Ankündigung] (https://api-docs.deepseek.com/news/news1226)."},"name":"DeepSeek Chat","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"deepseek","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.28","output":"0.42"},"eur":{"currency":"eur","input":"0.241559192","output":"0.362338788"}}},{"providerId":"zenmux","contextLength":128000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"0.56","output":"1.68"},"eur":{"currency":"eur","input":"0.483118384","output":"1.449355152"}}},{"providerId":"openrouter","contextLength":163840,"price":{"usd":{"currency":"usd","input":"0.3","output":"1.2"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.714Z"},{"id":"deepseek-chat-v3-0324","aliases":["deepseek/deepseek-chat-v3-0324","deepseek-ai/deepseek-v3-0324","deepseek-chat-v3-0324","deepseek-v3-0324"],"description":{"en":"DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team. It succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.","de":"DeepSeek V3, ein 685B-Parameter Mixture-of-Experts-Modell, ist die neueste Iteration der Flaggschiff-Chat-Modellfamilie des DeepSeek-Teams. Es ist der Nachfolger des Modells [DeepSeek V3](/deepseek/deepseek-chat-v3) und schneidet bei einer Vielzahl von Aufgaben sehr gut ab."},"name":"DeepSeek V3 0324","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","tools","top_k","top_logprobs","top_p"],"providers":[{"providerId":"openrouter","contextLength":16384,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.2","output":"0.88"},"eur":{"currency":"eur","input":"0.17","output":"0.76"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.456Z","outputLimit":8192,"contextLength":16384},{"id":"deepseek-chat-v3.1","aliases":["deepseek/deepseek-chat-v3.1","deepseek-ai/deepseek-v3.1","deepseek-chat-v3.1","deepseek-v3.1"],"description":{"en":"DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes via prompt templates. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) The model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. It succeeds the [DeepSeek V3-0324](/deepseek/deepseek-chat-v3-0324) model and performs well on a variety of tasks.","de":"DeepSeek-V3.1 ist ein großes hybrides Schlussfolgerungsmodell (671B Parameter, 37B aktiv), das sowohl denkende als auch nicht denkende Modi über Prompt-Vorlagen unterstützt. Es erweitert die DeepSeek-V3-Basis mit einem zweistufigen Langkontext-Trainingsprozess, der bis zu 128K Token erreicht, und verwendet FP8-Mikroskalierung für effiziente Inferenz. Benutzer können das Schlussfolgerungsverhalten mit dem Bool `reasoning` `enabled` steuern. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) Das Modell verbessert die Nutzung von Werkzeugen, die Codegenerierung und die Effizienz der Schlussfolgerungen und erreicht bei schwierigen Benchmarks eine mit DeepSeek-R1 vergleichbare Leistung, während es schneller reagiert. Es unterstützt strukturierte Werkzeugaufrufe, Code-Agenten und Suchagenten und eignet sich daher für Forschungs-, Codierungs- und agentenbasierte Arbeitsabläufe. Es ist das Nachfolgemodell von [DeepSeek V3-0324](/deepseek/deepseek-chat-v3-0324) und erbringt gute Leistungen bei einer Vielzahl von Aufgaben."},"name":"DeepSeek-V3.1","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"openrouter","contextLength":8192,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.80"},"eur":{"currency":"eur","input":"0.17","output":"0.69"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.456Z","outputLimit":163840,"contextLength":8192},{"id":"deepseek-coder-6.7b-base-awq","aliases":["deepseek-coder-6.7b-base-awq"],"name":"@hf/thebloke/deepseek-coder-6.7b-base-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.621Z","outputLimit":4096,"contextLength":4096},{"id":"deepseek-coder-6.7b-instruct-awq","aliases":["deepseek-coder-6.7b-instruct-awq"],"name":"@hf/thebloke/deepseek-coder-6.7b-instruct-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.869Z","outputLimit":4096,"contextLength":4096},{"id":"deepseek-coder-v2-lite","aliases":["deepseek-coder-v2-lite"],"name":"DeepSeek Coder V2 Lite","openWeights":true,"knowledge":"2021-09-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"venice","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.4313557","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.717Z","deprecated":true},{"id":"deepseek-math-7b-instruct","aliases":["deepseek-math-7b-instruct"],"name":"@cf/deepseek-ai/deepseek-math-7b-instruct","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.713Z","outputLimit":4096,"contextLength":4096},{"id":"deepseek-prover-v2","aliases":["deepseek-ai/deepseek-prover-v2-671b","deepseek/deepseek-prover-v2","deepseek-prover-v2-671b","deepseek-prover-v2"],"name":"DeepSeek: DeepSeek Prover V2","description":{"en":"DeepSeek Prover V2 is a 671B parameter model, speculated to be geared towards logic and mathematics. Likely an upgrade from [DeepSeek-Prover-V1.5](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL) Not much is known about the model yet, as DeepSeek released it on Hugging Face without an announcement or description.","de":"DeepSeek Prover V2 ist ein Modell mit 671B-Parametern, von dem man annimmt, dass es auf Logik und Mathematik ausgerichtet ist. Wahrscheinlich handelt es sich um ein Upgrade von [DeepSeek-Prover-V1.5] (https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL) Es ist noch nicht viel über das Modell bekannt, da DeepSeek es ohne Ankündigung oder Beschreibung auf Hugging Face veröffentlicht hat."},"knowledge":"2025-04-30","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":163840,"price":{"usd":{"currency":"usd","input":"0.5","output":"2.18"},"eur":{"currency":"eur","input":"0.4313557","output":"1.880710852"}}}],"lastImportedAt":"2025-11-19T12:06:32.765Z"},{"id":"deepseek-r1","aliases":["replicate/deepseek-ai/deepseek-r1","hf:deepseek-ai/deepseek-r1","deepseek/deepseek-r1:free","deepseek-ai/deepseek-r1","deepseek/deepseek-r1","deepseek-r1:free","deepseek-r1"],"description":{"en":"DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass. Fully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120). MIT licensed: Distill & commercialize freely!","de":"DeepSeek R1 ist da: Die Leistung entspricht der von [OpenAI o1](/openai/o1), aber mit offenen Quellen und vollständig offenen Schlussfolgerungstokens. Es ist 671B Parameter groß, mit 37B aktiv in einem Inferenzdurchgang. Vollständig quelloffenes Modell & [technischer Bericht](https://api-docs.deepseek.com/news/news250120). MIT lizenziert: Frei destillieren und kommerzialisieren!"},"name":"DeepSeek-R1","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"1.35","output":"5.40"},"eur":{"currency":"eur","input":"1.16","output":"4.64"}}},{"providerId":"alibaba-cn","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.57","output":"2.29"},"eur":{"currency":"eur","input":"0.49","output":"1.97"}}},{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}},{"providerId":"github-models","contextLength":65536,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"togetherai","contextLength":163839,"outputLimit":12288,"price":{"usd":{"currency":"usd","input":"3.00","output":"7.00"},"eur":{"currency":"eur","input":"2.58","output":"6.01"}}},{"providerId":"azure","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"1.35","output":"5.40"},"eur":{"currency":"eur","input":"1.16","output":"4.64"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"10.00","output":"10.00"},"eur":{"currency":"eur","input":"8.59","output":"8.59"}}},{"providerId":"iflowcn","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.19"},"eur":{"currency":"eur","input":"0.47","output":"1.88"}}},{"providerId":"azure-cognitive-services","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"1.35","output":"5.40"},"eur":{"currency":"eur","input":"1.16","output":"4.64"}}},{"providerId":"openrouter","contextLength":163840,"price":{"usd":{"currency":"usd","input":"0.3","output":"1.2"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":163840,"outputLimit":163840,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.955Z","outputLimit":8192,"contextLength":65536},{"id":"deepseek-r1-0528","aliases":["accounts/fireworks/models/deepseek-r1-0528","hf:deepseek-ai/deepseek-r1-0528","deepseek/deepseek-r1-0528:free","deepseek-ai/deepseek-r1-0528","deepseek/deepseek-r1-0528","deepseek-r1-0528:free","deepseek-r1-0528"],"description":{"en":"May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass. Fully open-source model.","de":"Update vom 28. Mai zum [originalen DeepSeek R1](/deepseek/deepseek-r1) Leistung auf dem Niveau von [OpenAI o1](/openai/o1), aber mit offenem Quellcode und vollständig offenen Schlussfolgerungstoken. Es ist 671B Parameter groß, mit 37B aktiv in einem Inferenzdurchgang. Vollständig quelloffenes Modell."},"name":"DeepSeek R1 0528","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"alibaba-cn","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.57","output":"2.29"},"eur":{"currency":"eur","input":"0.49","output":"1.97"}}},{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.75"},"eur":{"currency":"eur","input":"0.34","output":"1.5"}}},{"providerId":"github-models","contextLength":65536,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"1.35","output":"5.40"},"eur":{"currency":"eur","input":"1.16","output":"4.64"}}},{"providerId":"huggingface","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"3.00","output":"5.00"},"eur":{"currency":"eur","input":"2.58","output":"4.29"}}},{"providerId":"wandb","contextLength":161000,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"1.35","output":"5.40"},"eur":{"currency":"eur","input":"1.16","output":"4.64"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"3.00","output":"8.00"},"eur":{"currency":"eur","input":"2.58","output":"6.87"}}},{"providerId":"submodel","contextLength":75000,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.15"},"eur":{"currency":"eur","input":"0.43","output":"1.85"}}},{"providerId":"fireworks-ai","contextLength":160000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"3.00","output":"8.00"},"eur":{"currency":"eur","input":"2.58","output":"6.87"}}},{"providerId":"io-net","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.75"},"eur":{"currency":"eur","input":"1.72","output":"7.52"}}},{"providerId":"azure-cognitive-services","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"1.35","output":"5.40"},"eur":{"currency":"eur","input":"1.16","output":"4.64"}}},{"providerId":"openrouter","contextLength":163840,"price":{"usd":{"currency":"usd","input":"0.4","output":"1.75"},"eur":{"currency":"eur","input":"0.34","output":"1.5"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":163840,"outputLimit":163840,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.960Z","outputLimit":4096,"contextLength":65536},{"id":"deepseek-r1-0528-qwen3-8b","aliases":["deepseek/deepseek-r1-0528-qwen3-8b:free","deepseek-ai/deepseek-r1-0528-qwen3-8b","deepseek/deepseek-r1-0528-qwen3-8b","deepseek-r1-0528-qwen3-8b:free","deepseek-r1-0528-qwen3-8b"],"description":{"en":"DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro. It now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought. The distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B “thinking” giant on AIME 2024.","de":"DeepSeek-R1-0528 ist eine leicht aktualisierte Version von DeepSeek R1, die mehr Rechenleistung und intelligentere Post-Trainings-Tricks nutzt, wodurch seine Schlussfolgerungen und Inferenzen an die Grenzen von Flaggschiff-Modellen wie O3 und Gemini 2.5 Pro stoßen. Es führt jetzt die Bestenlisten in den Bereichen Mathematik, Programmierung und Logik an und zeigt damit, dass sich die Denktiefe deutlich verbessert hat. Die destillierte Variante, DeepSeek-R1-0528-Qwen3-8B, überträgt diese Denkkette in eine Form mit 8 B-Parametern und übertrifft damit den Standard-Qwen3 8B um +10 pp und zieht mit dem 235 B \"denkenden\" Riesen auf der AIME 2024 gleich."},"name":"DeepSeek R1 0528 Qwen3 8B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-05-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"chutes","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.10"},"eur":{"currency":"eur","input":"0.02","output":"0.09"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.1"},"eur":{"currency":"eur","input":"0.02","output":"0.09"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":131072,"outputLimit":131072,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.963Z","outputLimit":32768,"contextLength":32768},{"id":"deepseek-r1-671b","aliases":["deepseek-r1-671b"],"name":"DeepSeek R1 671B","reasoning":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"venice","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.50","output":"14.00"},"eur":{"currency":"eur","input":"3.0194899","output":"12.0779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.717Z","deprecated":true},{"id":"deepseek-r1-distill-llama-70b","aliases":["deepseek-ai/deepseek-r1-distill-llama-70b","deepseek/deepseek-r1-distill-llama-70b","deepseek-r1-distill-llama-70b"],"description":{"en":"DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including: - AIME 2024 pass@1: 70.0 - MATH-500 pass@1: 94.5 - CodeForces Rating: 1633 The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.","de":"DeepSeek R1 Distill Llama 70B ist ein destilliertes großes Sprachmodell, das auf [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct) basiert und Ausgaben von [DeepSeek R1](/deepseek/deepseek-r1) verwendet. Das Modell kombiniert fortschrittliche Destillationstechniken, um eine hohe Leistung bei mehreren Benchmarks zu erreichen, darunter: - AIME 2024 pass@1: 70,0 - MATH-500 bestanden@1: 94,5 - CodeForces Bewertung: 1633 Das Modell nutzt die Feinabstimmung der Ergebnisse von DeepSeek R1 und ermöglicht eine wettbewerbsfähige Leistung, die mit größeren Grenzmodellen vergleichbar ist."},"name":"DeepSeek R1 Distill Llama 70B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"vultr","contextLength":121808,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"groq","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.75","output":"0.99"},"eur":{"currency":"eur","input":"0.64","output":"0.85"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.75","output":"0.99"},"eur":{"currency":"eur","input":"0.64","output":"0.85"}}},{"providerId":"alibaba-cn","contextLength":32768,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.29","output":"0.86"},"eur":{"currency":"eur","input":"0.25","output":"0.74"}}},{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.13"},"eur":{"currency":"eur","input":"0.03","output":"0.11"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.13"},"eur":{"currency":"eur","input":"0.03","output":"0.11"}}},{"providerId":"fastrouter","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.14"},"eur":{"currency":"eur","input":"0.03","output":"0.12"}}},{"providerId":"openrouter","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.13"},"eur":{"currency":"eur","input":"0.03","output":"0.11"}}},{"providerId":"ovhcloud","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.74","output":"0.74"},"eur":{"currency":"eur","input":"0.64","output":"0.64"}}},{"providerId":"scaleway","contextLength":32000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.90","output":"0.90"},"eur":{"currency":"eur","input":"0.77","output":"0.77"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.277Z","outputLimit":4096,"contextLength":8192},{"id":"deepseek-r1-distill-llama-8b","aliases":["deepseek-r1-distill-llama-8b"],"name":"DeepSeek R1 Distill Llama 8B","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":32768,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.716Z"},{"id":"deepseek-r1-distill-qwen-1-5b","aliases":["deepseek-r1-distill-qwen-1-5b"],"name":"DeepSeek R1 Distill Qwen 1.5B","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":32768,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.716Z"},{"id":"deepseek-r1-distill-qwen-14b","aliases":["deepseek-ai/deepseek-r1-distill-qwen-14b","deepseek/deepseek-r1-distill-qwen-14b","deepseek-r1-distill-qwen-14b"],"description":{"en":"DeepSeek R1 Distill Qwen 14B is a distilled large language model based on [Qwen 2.5 14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models. Other benchmark results include: - AIME 2024 pass@1: 69.7 - MATH-500 pass@1: 93.9 - CodeForces Rating: 1481 The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.","de":"DeepSeek R1 Distill Qwen 14B ist ein destilliertes großes Sprachmodell, das auf [Qwen 2.5 14B] (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) basiert und die Ergebnisse von [DeepSeek R1] (/deepseek/deepseek-r1) verwendet. Es übertrifft das o1-mini von OpenAI in verschiedenen Benchmarks und erreicht neue Spitzenergebnisse für dichte Modelle. Andere Benchmark-Ergebnisse umfassen: - AIME 2024 pass@1: 69.7 - MATH-500 bestanden@1: 93,9 - CodeForces Bewertung: 1481 Das Modell nutzt die Feinabstimmung der Ergebnisse von DeepSeek R1 und ermöglicht eine wettbewerbsfähige Leistung, die mit größeren Grenzmodellen vergleichbar ist."},"name":"DeepSeek R1 Distill Qwen 14B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"alibaba-cn","contextLength":32768,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.43"},"eur":{"currency":"eur","input":"0.12","output":"0.37"}}},{"providerId":"openrouter","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.12","output":"0.12"},"eur":{"currency":"eur","input":"0.1","output":"0.1"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.961Z","outputLimit":8192,"contextLength":32768},{"id":"deepseek-r1-distill-qwen-32b","aliases":["deepseek-ai/deepseek-r1-distill-qwen-32b","workers-ai/deepseek-r1-distill-qwen-32b","deepseek/deepseek-r1-distill-qwen-32b","deepseek-r1-distill-qwen-32b"],"description":{"en":"DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\nOther benchmark results include:\\n\\n- AIME 2024 pass@1: 72.6\\n- MATH-500 pass@1: 94.3\\n- CodeForces Rating: 1691\\n\\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.","de":"DeepSeek R1 Distill Qwen 32B ist ein destilliertes großes Sprachmodell, das auf [Qwen 2.5 32B] (https://huggingface.co/Qwen/Qwen2.5-32B) basiert und die Ergebnisse von [DeepSeek R1] (/deepseek/deepseek-r1) verwendet. Es übertrifft das o1-mini von OpenAI in verschiedenen Benchmarks und erzielt neue Spitzenergebnisse für dichte Modelle.\\n\\nWeitere Benchmark-Ergebnisse sind:\\n\\n- AIME 2024 pass@1: 72.6\\n- MATH-500 pass@1: 94.3\\n- CodeForces Rating: 1691\\n\\nDas Modell nutzt die Feinabstimmung der Ergebnisse von DeepSeek R1 und ermöglicht eine wettbewerbsfähige Leistung, die mit größeren Grenzmodellen vergleichbar ist."},"name":"DeepSeek R1 Distill Qwen 32B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"vultr","contextLength":121808,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"alibaba-cn","contextLength":32768,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.29","output":"0.86"},"eur":{"currency":"eur","input":"0.25","output":"0.74"}}},{"providerId":"cloudflare-workers-ai","contextLength":80000,"outputLimit":80000,"price":{"usd":{"currency":"usd","input":"0.50","output":"4.88"},"eur":{"currency":"eur","input":"0.43","output":"4.19"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"4.88"},"eur":{"currency":"eur","input":"0","output":"4.19"}}},{"providerId":"openrouter","contextLength":64000,"price":{"usd":{"currency":"usd","input":"0.24","output":"0.24"},"eur":{"currency":"eur","input":"0.21","output":"0.21"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.628Z","outputLimit":8192,"contextLength":32768},{"id":"deepseek-r1-distill-qwen-7b","aliases":["deepseek-r1-distill-qwen-7b"],"name":"DeepSeek R1 Distill Qwen 7B","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":32768,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.14"},"eur":{"currency":"eur","input":"0.060389798","output":"0.120779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.714Z"},{"id":"deepseek-r1t-chimera","aliases":["tngtech/deepseek-r1t-chimera:free","tngtech/deepseek-r1t-chimera","deepseek-r1t-chimera:free","deepseek-r1t-chimera"],"description":{"en":"DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks. The model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.","de":"DeepSeek-R1T-Chimera wurde durch die Verschmelzung von DeepSeek-R1 und DeepSeek-V3 (0324) geschaffen und kombiniert die Argumentationsfähigkeiten von R1 mit den Verbesserungen der Token-Effizienz von V3. Er basiert auf einer DeepSeek-MoE-Transformer-Architektur und ist für allgemeine Textgenerierungsaufgaben optimiert. Das Modell kombiniert vortrainierte Gewichte aus beiden Ausgangsmodellen, um die Leistung in den Bereichen Schlussfolgerungen, Effizienz und Befolgung von Anweisungen auszugleichen. Es ist unter der MIT-Lizenz veröffentlicht und für Forschung und kommerzielle Nutzung bestimmt."},"name":"DeepSeek R1T Chimera","reasoning":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","seed","stop","top_k","top_p","response_format","structured_outputs"],"providers":[{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}},{"providerId":"openrouter","contextLength":163840,"price":{"usd":{"currency":"usd","input":"0.3","output":"1.2"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":163840,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.718Z"},{"id":"deepseek-r1t2-chimera","aliases":["tngtech/deepseek-r1t2-chimera:free","tngtech/deepseek-tng-r1t2-chimera","tngtech/deepseek-r1t2-chimera","deepseek-r1t2-chimera:free","deepseek-tng-r1t2-chimera","deepseek-r1t2-chimera"],"description":{"en":"DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2× faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.","de":"DeepSeek-TNG-R1T2-Chimera ist das Chimera-Modell der zweiten Generation von TNG Tech. Es ist ein 671 B-Parameter Mixture-of-Experts-Textgenerierungsmodell, das aus den Kontrollpunkten R1-0528, R1 und V3-0324 von DeepSeek-AI mit einem Assembly-of-Experts-Merge zusammengesetzt wurde. Das Tri-Eltern-Design liefert eine starke Argumentationsleistung und läuft dabei etwa 20 % schneller als das ursprüngliche R1 und mehr als 2× schneller als R1-0528 unter vLLM, was ein günstiges Kosten-Intelligenz-Verhältnis ergibt. Der Checkpoint unterstützt Kontexte bis zu 60 k Token im Standardgebrauch (getestet bis ~130 k) und behält ein konsistentes <think>-Token-Verhalten bei, wodurch er sich für lange Kontextanalysen, Dialoge und andere Generierungsaufgaben mit offenem Ende eignet."},"name":"DeepSeek R1T2 Chimera (free)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-07-01","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","seed","stop","top_k","top_p","response_format","structured_outputs","tool_choice","tools"],"providers":[{"providerId":"openrouter","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.3","output":"1.2"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":163840,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.750Z"},{"id":"deepseek-reasoner","aliases":["deepseek-reasoner"],"name":"DeepSeek Reasoner","reasoning":true,"toolCalling":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"deepseek","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.28","output":"0.42"},"eur":{"currency":"eur","input":"0.24","output":"0.36"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.56","output":"1.68"},"eur":{"currency":"eur","input":"0.48","output":"1.44"}}}],"lastImportedAt":"2025-12-09T00:20:58.388Z","outputLimit":64000,"contextLength":128000},{"id":"deepseek-tng-r1t2-chimera","aliases":["tngtech/deepseek-tng-r1t2-chimera","deepseek-tng-r1t2-chimera"],"description":{"en":"DeepSeek-TNG R1T2 Chimera is a 671B tri-mind language model optimized for text generation using three parent models (R1-0528, R1, V3-0324). It offers improved speed (20% faster than R1) and intelligence, with enhanced reasoning capabilities. Ideal for applications requiring high-quality responses and long-context handling.","de":"DeepSeek-TNG R1T2 Chimera ist ein 671B Tri-Mind-Sprachmodell, das für die Texterstellung unter Verwendung von drei übergeordneten Modellen (R1-0528, R1, V3-0324) optimiert wurde. Es bietet eine verbesserte Geschwindigkeit (20 % schneller als R1) und Intelligenz mit erweiterten Argumentationsfähigkeiten. Es ist ideal für Anwendungen, die qualitativ hochwertige Antworten und die Verarbeitung langer Kontexte erfordern."},"name":"DeepSeek TNG R1T2 Chimera","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}},{"providerId":"helicone","contextLength":130000,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}}],"lastImportedAt":"2025-12-09T00:20:58.407Z","outputLimit":163840,"contextLength":130000},{"id":"deepseek-v3","aliases":["hf:deepseek-ai/deepseek-v3","deepseek-ai/deepseek-v3","deepseek-v3"],"description":{"en":"DeepSeek-V3 is a Mixture-of-Experts language model with 671B parameters, utilizing Multi-head Latent Attention for efficient inference. It has been pre-trained on 14.8 trillion tokens and excels in performance benchmarks, particularly in math and code tasks. The model supports multi-token prediction and is deployable on various hardware platforms.","de":"DeepSeek-V3 ist ein Mixture-of-Experts-Sprachmodell mit 671B Parametern, das Multi-head Latent Attention für effiziente Inferenz nutzt. Es wurde mit 14,8 Billionen Token vortrainiert und schneidet in Leistungsbenchmarks hervorragend ab, insbesondere bei Mathematik- und Codeaufgaben. Das Modell unterstützt Multi-Token-Vorhersagen und kann auf verschiedenen Hardware-Plattformen eingesetzt werden."},"name":"DeepSeek V3","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.50"},"eur":{"currency":"eur","input":"0.43","output":"1.29"}}},{"providerId":"alibaba-cn","contextLength":65536,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.29","output":"1.15"},"eur":{"currency":"eur","input":"0.25","output":"0.99"}}},{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}},{"providerId":"togetherai","contextLength":131072,"outputLimit":12288,"price":{"usd":{"currency":"usd","input":"1.25","output":"1.25"},"eur":{"currency":"eur","input":"1.07","output":"1.07"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.56","output":"1.68"},"eur":{"currency":"eur","input":"0.48","output":"1.44"}}},{"providerId":"iflowcn","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"1.25"},"eur":{"currency":"eur","input":"1.07","output":"1.07"}}}],"lastImportedAt":"2025-12-09T00:20:58.388Z","outputLimit":8192,"contextLength":65536},{"id":"deepseek-v3-0324","aliases":["accounts/fireworks/models/deepseek-v3-0324","hf:deepseek-ai/deepseek-v3-0324","deepseek-ai/deepseek-v3-0324","deepseek/deepseek-v3-0324","deepseek-v3-0324"],"description":{"en":"DeepSeek-V3-0324 improves reasoning capabilities with enhanced benchmark performance and function calling accuracy. It excels in front-end web development, Chinese writing, and search capabilities. It supports multi-turn interactive rewriting, optimized translations, and is suitable for tasks requiring detailed outputs. Licensed under MIT.","de":"DeepSeek-V3-0324 verbessert die Denkfähigkeiten mit verbesserter Benchmark-Leistung und Funktionsaufrufgenauigkeit. Es zeichnet sich durch Front-End-Web-Entwicklung, chinesische Schreib- und Suchfunktionen aus. Es unterstützt interaktives Umschreiben mit mehreren Umdrehungen, optimierte Übersetzungen und ist für Aufgaben geeignet, die detaillierte Ausgaben erfordern. Lizenziert unter MIT."},"name":"DeepSeek V3 (0324)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.24","output":"0.84"},"eur":{"currency":"eur","input":"0.21","output":"0.72"}}},{"providerId":"cortecs","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.55","output":"1.65"},"eur":{"currency":"eur","input":"0.47","output":"1.42"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"1.14","output":"4.56"},"eur":{"currency":"eur","input":"0.98","output":"3.93"}}},{"providerId":"huggingface","contextLength":16384,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.25","output":"1.25"},"eur":{"currency":"eur","input":"1.08","output":"1.08"}}},{"providerId":"wandb","contextLength":161000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.14","output":"2.75"},"eur":{"currency":"eur","input":"0.98","output":"2.37"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.20","output":"1.20"},"eur":{"currency":"eur","input":"1.04","output":"1.04"}}},{"providerId":"submodel","contextLength":75000,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.80"},"eur":{"currency":"eur","input":"0.17","output":"0.69"}}},{"providerId":"fireworks-ai","contextLength":160000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.90","output":"0.90"},"eur":{"currency":"eur","input":"0.78","output":"0.78"}}},{"providerId":"azure-cognitive-services","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"1.14","output":"4.56"},"eur":{"currency":"eur","input":"0.98","output":"3.93"}}}],"lastImportedAt":"2025-11-30T00:23:18.845Z","outputLimit":8192,"contextLength":16384},{"id":"deepseek-v3-base","aliases":["deepseek/deepseek-v3-base:free","deepseek-v3-base:free","deepseek-v3-base"],"name":"DeepSeek V3 Base (free)","openWeights":true,"knowledge":"2025-03-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"openrouter","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.450Z","outputLimit":163840,"contextLength":163840},{"id":"deepseek-v3.1","aliases":["hf:deepseek-ai/deepseek-v3.1","deepseek-ai/deepseek-v3.1","deepseek-ai/deepseek-v3-1","deepseek-v3.1","deepseek-v3-1"],"description":{"en":"DeepSeek-V3.1 is a hybrid model that operates in both thinking and non-thinking modes, enhancing tool usage and efficiency. It features a 10-fold increase in training tokens for improved performance and supports various agent frameworks. Designed for tasks requiring context and tool interaction, it is suitable for complex queries and code-related tasks.","de":"DeepSeek-V3.1 ist ein hybrides Modell, das sowohl im denkenden als auch im nicht denkenden Modus arbeitet und so die Nutzung und Effizienz des Tools verbessert. Es bietet eine 10-fache Steigerung der Trainings-Tokens für eine verbesserte Leistung und unterstützt verschiedene Agenten-Frameworks. Es wurde für Aufgaben entwickelt, die Kontext und Tool-Interaktion erfordern, und eignet sich für komplexe Abfragen und codebezogene Aufgaben."},"name":"DeepSeek V3.1","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"nvidia","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"alibaba-cn","contextLength":131072,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.57","output":"1.72"},"eur":{"currency":"eur","input":"0.49","output":"1.47"}}},{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.80"},"eur":{"currency":"eur","input":"0.17","output":"0.68"}}},{"providerId":"togetherai","contextLength":131072,"outputLimit":12288,"price":{"usd":{"currency":"usd","input":"0.60","output":"1.70"},"eur":{"currency":"eur","input":"0.51","output":"1.45"}}},{"providerId":"azure","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.56","output":"1.68"},"eur":{"currency":"eur","input":"0.48","output":"1.44"}}},{"providerId":"iflowcn","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.56","output":"1.68"},"eur":{"currency":"eur","input":"0.48","output":"1.44"}}},{"providerId":"submodel","contextLength":75000,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.80"},"eur":{"currency":"eur","input":"0.17","output":"0.68"}}},{"providerId":"azure-cognitive-services","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.56","output":"1.68"},"eur":{"currency":"eur","input":"0.48","output":"1.44"}}}],"lastImportedAt":"2025-12-12T12:08:56.204Z","outputLimit":8192,"contextLength":75000},{"id":"deepseek-v3.1-671b","aliases":["deepseek-v3.1-671b"],"name":"DeepSeek-V3.1 671B","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":160000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-23T00:23:30.706Z","outputLimit":8192,"contextLength":160000,"deprecated":true},{"id":"deepseek-v3.1-nex-n1","aliases":["nex-agi/deepseek-v3.1-nex-n1:free","nex-agi/deepseek-v3.1-nex-n1","deepseek-v3.1-nex-n1:free","deepseek-v3.1-nex-n1","deepseek-v3.1-nex-n1"],"name":"Nex AGI: DeepSeek V3.1 Nex N1 (free)","description":{"en":"DeepSeek V3.1 Nex-N1 is the flagship release of the Nex-N1 series — a post-trained model designed to highlight agent autonomy, tool use, and real-world productivity. Nex-N1 demonstrates competitive performance across all evaluation scenarios, showing particularly strong results in practical coding and HTML generation tasks.","de":"DeepSeek V3.1 Nex-N1 ist das Flaggschiff der Nex-N1-Serie - ein nachtrainiertes Modell, das die Autonomie des Agenten, den Einsatz von Werkzeugen und die Produktivität in der realen Welt hervorhebt. Nex-N1 zeigt eine konkurrenzfähige Leistung in allen Bewertungsszenarien, mit besonders starken Ergebnissen bei praktischen Codierungs- und HTML-Generierungsaufgaben."},"knowledge":"2025-12-08","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","response_format","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0","output":"0"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-12T00:21:33.762Z","contextLength":131072},{"id":"deepseek-v3.1-terminus","aliases":["hf:deepseek-ai/deepseek-v3.1-terminus","deepseek-ai/deepseek-v3.1-terminus","deepseek/deepseek-v3.1-terminus","deepseek-v3.1-terminus"],"description":{"en":"DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) The model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows.","de":"DeepSeek-V3.1 Terminus ist eine Aktualisierung von [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1), die die ursprünglichen Fähigkeiten des Modells beibehält und gleichzeitig von Benutzern gemeldete Probleme angeht, einschließlich Sprachkonsistenz und Agentenfähigkeiten, und die Leistung des Modells bei der Codierung und bei Suchagenten weiter optimiert. Es ist ein großes hybrides Argumentationsmodell (671B Parameter, 37B aktiv), das sowohl denkende als auch nicht denkende Modi unterstützt. Es erweitert die DeepSeek-V3-Basis mit einem zweistufigen Langkontext-Trainingsprozess, der bis zu 128K Token erreicht, und verwendet FP8-Mikroskalierung für effiziente Inferenz. Benutzer können das Schlussfolgerungsverhalten mit dem Bool `reasoning` `enabled` steuern. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) Das Modell verbessert die Nutzung von Werkzeugen, die Codegenerierung und die Effizienz der Schlussfolgerungen und erreicht bei schwierigen Benchmarks eine mit DeepSeek-R1 vergleichbare Leistung, während es schneller reagiert. Es unterstützt strukturierte Werkzeugaufrufe, Code-Agenten und Suchagenten und eignet sich daher für Forschungs-, Codierungs- und agentenbasierte Arbeitsabläufe."},"name":"DeepSeek V3.1 Terminus","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"nvidia","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.27","output":"1.00"},"eur":{"currency":"eur","input":"0.23","output":"0.86"}}},{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.23","output":"0.90"},"eur":{"currency":"eur","input":"0.2","output":"0.77"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.27","output":"1.00"},"eur":{"currency":"eur","input":"0.23","output":"0.86"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.27","output":"1.00"},"eur":{"currency":"eur","input":"0.23","output":"0.86"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.20","output":"1.20"},"eur":{"currency":"eur","input":"1.03","output":"1.03"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.279Z","outputLimit":8192,"contextLength":128000},{"id":"deepseek-v3.1-terminus:exacto","aliases":["deepseek/deepseek-v3.1-terminus:exacto","deepseek-ai/deepseek-v3.1-terminus","deepseek/deepseek-v3.1-terminus","deepseek-v3.1-terminus:exacto","deepseek-v3.1-terminus"],"description":{"en":"DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) The model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows.","de":"DeepSeek-V3.1 Terminus ist eine Aktualisierung von [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1), die die ursprünglichen Fähigkeiten des Modells beibehält und gleichzeitig von Benutzern gemeldete Probleme angeht, einschließlich Sprachkonsistenz und Agentenfähigkeiten, und die Leistung des Modells bei der Codierung und bei Suchagenten weiter optimiert. Es ist ein großes hybrides Argumentationsmodell (671B Parameter, 37B aktiv), das sowohl denkende als auch nicht denkende Modi unterstützt. Es erweitert die DeepSeek-V3-Basis mit einem zweistufigen Langkontext-Trainingsprozess, der bis zu 128K Token erreicht, und verwendet FP8-Mikroskalierung für effiziente Inferenz. Benutzer können das Schlussfolgerungsverhalten mit dem Bool `reasoning` `enabled` steuern. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) Das Modell verbessert die Nutzung von Werkzeugen, die Codegenerierung und die Effizienz der Schlussfolgerungen und erreicht bei schwierigen Benchmarks eine mit DeepSeek-R1 vergleichbare Leistung, während es schneller reagiert. Es unterstützt strukturierte Werkzeugaufrufe, Code-Agenten und Suchagenten und eignet sich daher für Forschungs-, Codierungs- und agentenbasierte Arbeitsabläufe."},"name":"DeepSeek V3.1 Terminus (exacto)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.27","output":"1.00"},"eur":{"currency":"eur","input":"0.23","output":"0.86"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.502Z","outputLimit":65536,"contextLength":131072},{"id":"deepseek-v3.1:671b-cloud","aliases":["deepseek-v3.1:671b-cloud"],"name":"DeepSeek-V3.1 671B","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":160000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.555Z","outputLimit":8192,"contextLength":160000},{"id":"deepseek-v3.2","aliases":["deepseek/deepseek-v3.2-20251201","hf:deepseek-ai/deepseek-v3.2","deepseek-ai/deepseek-v3.2","deepseek/deepseek-v3.2","deepseek-v3.2-20251201","deepseek-v3.2"],"description":{"en":"DeepSeek-V3.2 is a large language model designed to harmonize high computational efficiency with strong reasoning and agentic tool-use performance. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism that reduces training and inference cost while preserving quality in long-context scenarios. A scalable reinforcement learning post-training framework further improves reasoning, with reported performance in the GPT-5 class, and the model has demonstrated gold-medal results on the 2025 IMO and IOI. V3.2 also uses a large-scale agentic task synthesis pipeline to better integrate reasoning into tool-use settings, boosting compliance and generalization in interactive environments. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)","de":"DeepSeek-V3.2 ist ein großes Sprachmodell, das entwickelt wurde, um hohe Berechnungseffizienz mit starker Argumentation und agentenbasierter Werkzeugnutzung zu vereinen. Es führt DeepSeek Sparse Attention (DSA) ein, einen feinkörnigen Sparse-Attention-Mechanismus, der die Trainings- und Inferenzkosten reduziert und gleichzeitig die Qualität in Szenarien mit langen Kontexten bewahrt. Ein skalierbares Reinforcement-Learning-Framework nach dem Training verbessert das Reasoning weiter, mit gemeldeten Leistungen in der GPT-5-Klasse, und das Modell hat Goldmedaillen-Ergebnisse auf dem IMO und IOI 2025 gezeigt. V3.2 verwendet außerdem eine groß angelegte agentenbasierte Aufgabensynthese-Pipeline, um die Argumentation besser in Tool-Use-Settings zu integrieren und so die Compliance und Generalisierung in interaktiven Umgebungen zu verbessern. Benutzer können das schlussfolgernde Verhalten mit dem Boolean `reasoning` `enabled` steuern. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)"},"name":"DeepSeek V3.2","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"venice","contextLength":163840,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.00"},"eur":{"currency":"eur","input":"0.34","output":"0.86"}}},{"providerId":"chutes","contextLength":163840,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.41"},"eur":{"currency":"eur","input":"0.23","output":"0.35"}}},{"providerId":"baseten","contextLength":163800,"outputLimit":131100,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.45"},"eur":{"currency":"eur","input":"0.26","output":"0.39"}}},{"providerId":"helicone","contextLength":163840,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.41"},"eur":{"currency":"eur","input":"0.23","output":"0.35"}}},{"providerId":"openrouter","contextLength":163840,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.28","output":"0.40"},"eur":{"currency":"eur","input":"0.24","output":"0.34"}}},{"providerId":"iflowcn","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"synthetic","contextLength":162816,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.40"},"eur":{"currency":"eur","input":"0.23","output":"0.34"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T00:21:31.037Z","outputLimit":8000,"contextLength":128000},{"id":"deepseek-v3.2-chat","aliases":["deepseek-v3.2-chat"],"name":"DeepSeek-V3.2","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-11-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"iflowcn","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-04T00:21:43.990Z","outputLimit":64000,"contextLength":128000},{"id":"deepseek-v3.2-exp","aliases":["deepseek-ai/deepseek-v3.2-exp","deepseek/deepseek-v3.2-exp","deepseek-v3.2-exp","deepseek-v3-2-exp"],"description":{"en":"DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate step between V3.1 and future architectures. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism designed to improve training and inference efficiency in long-context scenarios while maintaining output quality. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) The model was trained under conditions aligned with V3.1-Terminus to enable direct comparison. Benchmarking shows performance roughly on par with V3.1 across reasoning, coding, and agentic tool-use tasks, with minor tradeoffs and gains depending on the domain. This release focuses on validating architectural optimizations for extended context lengths rather than advancing raw task accuracy, making it primarily a research-oriented model for exploring efficient transformer designs.","de":"DeepSeek-V3.2-Exp ist ein experimentelles großes Sprachmodell, das von DeepSeek als Zwischenschritt zwischen V3.1 und zukünftigen Architekturen veröffentlicht wurde. Es führt DeepSeek Sparse Attention (DSA) ein, einen feinkörnigen Sparse-Attention-Mechanismus, der entwickelt wurde, um das Training und die Inferenz-Effizienz in Szenarien mit langem Kontext zu verbessern und gleichzeitig die Qualität der Ausgabe zu erhalten. Benutzer können das Reasoning-Verhalten mit dem Boolean `reasoning` `enabled` steuern. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) Das Modell wurde unter Bedingungen trainiert, die mit V3.1-Terminus abgestimmt sind, um einen direkten Vergleich zu ermöglichen. Das Benchmarking zeigt eine Leistung, die in etwa der von V3.1 entspricht, und zwar in den Bereichen Argumentation, Kodierung und Nutzung von Agentenwerkzeugen, mit geringfügigen Kompromissen und Gewinnen je nach Bereich. Diese Version konzentriert sich auf die Validierung architektonischer Optimierungen für erweiterte Kontextlängen und nicht auf die Verbesserung der reinen Aufgabengenauigkeit, was sie in erster Linie zu einem forschungsorientierten Modell für die Erforschung effizienter Transformator-Designs macht."},"name":"DeepSeek V3.2 Exp","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-09-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"vercel","contextLength":163840,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.28","output":"0.42"},"eur":{"currency":"eur","input":"0.24","output":"0.36"}}},{"providerId":"alibaba-cn","contextLength":131072,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.29","output":"0.43"},"eur":{"currency":"eur","input":"0.25","output":"0.37"}}},{"providerId":"aihubmix","contextLength":163000,"outputLimit":163000,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.41"},"eur":{"currency":"eur","input":"0.23","output":"0.35"}}},{"providerId":"openrouter","contextLength":163840,"price":{"usd":{"currency":"usd","input":"0.21","output":"0.32"},"eur":{"currency":"eur","input":"0.18","output":"0.28"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-03T12:09:06.560Z","outputLimit":8192,"contextLength":131072},{"id":"deepseek-v3.2-exp-think","aliases":["deepseek-v3.2-exp-think"],"name":"DeepSeek-V3.2-Exp-Think","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-09-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"aihubmix","contextLength":131000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.41"},"eur":{"currency":"eur","input":"0.232932078","output":"0.353711674"}}}],"lastImportedAt":"2025-11-19T12:06:32.755Z"},{"id":"deepseek-v3.2-exp-thinking","aliases":["deepseek/deepseek-v3.2-exp-thinking","deepseek-v3.2-exp-thinking"],"name":"DeepSeek V3.2 Exp Thinking","reasoning":true,"toolCalling":true,"knowledge":"2025-09-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":163840,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.28","output":"0.42"},"eur":{"currency":"eur","input":"0.241559192","output":"0.362338788"}}}],"lastImportedAt":"2025-11-19T12:06:32.707Z"},{"id":"deepseek-v3.2-speciale","aliases":["deepseek/deepseek-v3.2-speciale-20251201","deepseek-ai/deepseek-v3.2-speciale","deepseek/deepseek-v3.2-speciale","deepseek-v3.2-speciale-20251201","deepseek-v3.2-speciale"],"description":{"en":"DeepSeek-V3.2-Speciale is a high-compute variant of DeepSeek-V3.2 optimized for maximum reasoning and agentic performance. It builds on DeepSeek Sparse Attention (DSA) for efficient long-context processing, then scales post-training reinforcement learning to push capability beyond the base model. Reported evaluations place Speciale ahead of GPT-5 on difficult reasoning workloads, with proficiency comparable to Gemini-3.0-Pro, while retaining strong coding and tool-use reliability. Like V3.2, it benefits from a large-scale agentic task synthesis pipeline that improves compliance and generalization in interactive environments.","de":"DeepSeek-V3.2-Speciale ist eine rechenintensive Variante von DeepSeek-V3.2, die für maximale Argumentations- und Agentenleistung optimiert ist. Es baut auf DeepSeek Sparse Attention (DSA) für eine effiziente Verarbeitung langer Kontexte auf und skaliert dann das Verstärkungslernen nach dem Training, um die Leistungsfähigkeit über das Basismodell hinaus zu steigern. Berichten zufolge liegt Speciale bei schwierigen logischen Aufgaben vor GPT-5 und ist mit Gemini-3.0-Pro vergleichbar, während es gleichzeitig eine hohe Zuverlässigkeit bei der Kodierung und dem Einsatz von Tools aufweist. Wie V3.2 profitiert es von einer groß angelegten agentenbasierten Aufgabensynthese-Pipeline, die die Konformität und Generalisierung in interaktiven Umgebungen verbessert."},"name":"DeepSeek V3.2 Speciale","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_logprobs","top_p"],"providers":[{"providerId":"openrouter","contextLength":163840,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.41"},"eur":{"currency":"eur","input":"0.23","output":"0.35"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T00:21:33.755Z","outputLimit":65536,"contextLength":163840},{"id":"deepseek-v3.2-speciale-tee","aliases":["deepseek-ai/deepseek-v3.2-speciale-tee","deepseek-v3.2-speciale-tee"],"name":"DeepSeek V3.2 Speciale TEE","reasoning":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"chutes","contextLength":163840,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.41"},"eur":{"currency":"eur","input":"0.23","output":"0.35"}}}],"lastImportedAt":"2025-12-12T00:21:33.699Z","outputLimit":65536,"contextLength":163840},{"id":"deepseek-v3p1","aliases":["accounts/fireworks/models/deepseek-v3p1","deepseek-v3p1"],"name":"DeepSeek V3.1","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"fireworks-ai","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.56","output":"1.68"},"eur":{"currency":"eur","input":"0.483118384","output":"1.449355152"}}}],"lastImportedAt":"2025-11-19T12:06:32.755Z"},{"id":"deepseek.r1-v1:0","aliases":["deepseek.r1-v1:0"],"name":"DeepSeek-R1","reasoning":true,"toolCalling":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"1.35","output":"5.40"},"eur":{"currency":"eur","input":"1.16466039","output":"4.65864156"}}}],"lastImportedAt":"2025-11-19T12:06:32.759Z"},{"id":"deepseek.v3-v1:0","aliases":["deepseek.v3-v1:0"],"name":"DeepSeek-V3.1","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":163840,"outputLimit":81920,"price":{"usd":{"currency":"usd","input":"0.58","output":"1.68"},"eur":{"currency":"eur","input":"0.500372612","output":"1.449355152"}}}],"lastImportedAt":"2025-11-19T12:06:32.757Z"},{"id":"devstral-medium","aliases":["mistralai/devstral-medium-2507","mistralai/devstral-medium","devstral-medium-2507","devstral-medium"],"name":"Mistral: Devstral Medium","description":{"en":"Devstral Medium is a high-performance code generation and agentic reasoning model developed jointly by Mistral AI and All Hands AI. Positioned as a step up from Devstral Small, it achieves 61.6% on SWE-Bench Verified, placing it ahead of Gemini 2.5 Pro and GPT-4.1 in code-related tasks, at a fraction of the cost. It is designed for generalization across prompt styles and tool use in code agents and frameworks. Devstral Medium is available via API only (not open-weight), and supports enterprise deployment on private infrastructure, with optional fine-tuning capabilities.","de":"Devstral Medium ist ein hochleistungsfähiges Modell für Codegenerierung und agentenbasiertes Reasoning, das gemeinsam von Mistral AI und All Hands AI entwickelt wurde. Devstral Medium ist eine Weiterentwicklung von Devstral Small und erreicht im SWE-Bench Verified 61,6% und liegt damit bei codebezogenen Aufgaben vor Gemini 2.5 Pro und GPT-4.1 - zu einem Bruchteil der Kosten. Es ist für die Verallgemeinerung über Souffleur-Stile und Tool-Einsatz in Code-Agenten und Frameworks konzipiert. Devstral Medium ist nur über API verfügbar (nicht Open-Weight) und unterstützt den Einsatz in Unternehmen auf privater Infrastruktur mit optionalen Feinabstimmungsfunktionen."},"knowledge":"2025-07-10","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.4","output":"2"},"eur":{"currency":"eur","input":"0.34","output":"1.72"}}}],"lastImportedAt":"2025-12-02T00:21:07.421Z","contextLength":131072},{"id":"devstral-small","aliases":["mistralai/devstral-small-2507","mistralai/devstral-small","devstral-small-2507","devstral-small"],"name":"Mistral: Devstral Small 1.1","description":{"en":"Devstral Small 1.1 is a 24B parameter open-weight language model for software engineering agents, developed by Mistral AI in collaboration with All Hands AI. Finetuned from Mistral Small 3.1 and released under the Apache 2.0 license, it features a 128k token context window and supports both Mistral-style function calling and XML output formats. Designed for agentic coding workflows, Devstral Small 1.1 is optimized for tasks such as codebase exploration, multi-file edits, and integration into autonomous development agents like OpenHands and Cline. It achieves 53.6% on SWE-Bench Verified, surpassing all other open models on this benchmark, while remaining lightweight enough to run on a single 4090 GPU or Apple silicon machine. The model uses a Tekken tokenizer with a 131k vocabulary and is deployable via vLLM, Transformers, Ollama, LM Studio, and other OpenAI-compatible runtimes.","de":"Devstral Small 1.1 ist ein 24B-Parameter-Sprachmodell mit offenem Gewicht für Software-Engineering-Agenten, entwickelt von Mistral AI in Zusammenarbeit mit All Hands AI. Es wurde aus Mistral Small 3.1 weiterentwickelt und unter der Apache 2.0 Lizenz veröffentlicht. Es verfügt über ein 128k Token-Kontextfenster und unterstützt sowohl Funktionsaufrufe im Mistral-Stil als auch XML-Ausgabeformate. Devstral Small 1.1 wurde für agentenbasierte Coding-Workflows entwickelt und ist für Aufgaben wie Codebase-Exploration, Multi-File-Edits und die Integration in autonome Entwicklungsagenten wie OpenHands und Cline optimiert. Es erreicht 53,6 % im SWE-Bench Verified und übertrifft damit alle anderen offenen Modelle in diesem Benchmark. Gleichzeitig ist es leicht genug, um auf einer einzelnen 4090 GPU oder einem Apple-Silizium-Rechner zu laufen. Das Modell verwendet einen Tekken-Tokenizer mit einem 131k-Vokabular und kann über vLLM, Transformers, Ollama, LM Studio und andere OpenAI-kompatible Laufzeiten eingesetzt werden."},"knowledge":"2025-07-10","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.28"},"eur":{"currency":"eur","input":"0.060389798","output":"0.241559192"}}}],"lastImportedAt":"2025-11-19T12:06:32.764Z"},{"id":"discolm-german-7b-v1-awq","aliases":["discolm-german-7b-v1-awq"],"name":"@cf/thebloke/discolm-german-7b-v1-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.525Z","outputLimit":4096,"contextLength":4096},{"id":"distilbert-sst-2-int8","aliases":["workers-ai/distilbert-sst-2-int8","distilbert-sst-2-int8"],"name":"distiluert sst 2 int8","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.356Z","outputLimit":16384,"contextLength":128000},{"id":"dolphin-2.9.2-qwen2-72b","aliases":["dolphin-2.9.2-qwen2-72b"],"name":"Dolphin 72B","openWeights":true,"knowledge":"2021-09-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"venice","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.70","output":"2.80"},"eur":{"currency":"eur","input":"0.60389798","output":"2.41559192"}}}],"lastImportedAt":"2025-11-19T12:06:32.716Z","deprecated":true},{"id":"dolphin-mistral-24b-venice-edition","aliases":["cognitivecomputations/dolphin-mistral-24b-venice-edition:free","cognitivecomputations/dolphin-mistral-24b-venice-edition","dolphin-mistral-24b-venice-edition:free","dolphin-mistral-24b-venice-edition","venice/uncensored","uncensored","dolphin-mistral-24b-venice-edition"],"name":"Venice: Uncensored (free)","description":{"en":"Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-Instruct-2501, developed by dphn.ai in collaboration with Venice.ai. This model is designed as an “uncensored” instruct-tuned LLM, preserving user control over alignment, system prompts, and behavior. Intended for advanced and unrestricted use cases, Venice Uncensored emphasizes steerability and transparent behavior, removing default safety and alignment layers typically found in mainstream assistant models.","de":"Venice Uncensored Dolphin Mistral 24B Venice Edition ist eine fein abgestimmte Variante von Mistral-Small-24B-Instruct-2501, entwickelt von dphn.ai in Zusammenarbeit mit Venice.ai. Dieses Modell ist als \"unzensiertes\" instruct-tuned LLM konzipiert, das dem Benutzer die Kontrolle über die Ausrichtung, die Systemaufforderungen und das Verhalten belässt. Venice Uncensored ist für fortgeschrittene und uneingeschränkte Anwendungsfälle gedacht und betont die Steuerbarkeit und das transparente Verhalten, indem es die standardmäßigen Sicherheits- und Ausrichtungsebenen entfernt, die typischerweise in Mainstream-Assistenzmodellen zu finden sind."},"knowledge":"2025-07-09","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","response_format","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0","output":"0"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.764Z"},{"id":"dolphin3.0-mistral-24b","aliases":["cognitivecomputations/dolphin3.0-mistral-24b","dolphin3.0-mistral-24b"],"description":{"en":"Dolphin 3.0 Mistral 24B is an instruct-tuned model designed for general-purpose applications, including coding and math. It allows users to control the system prompt and alignment, ensuring data privacy. Compatible with various libraries, it operates efficiently at low temperatures (0.05-0.1) for optimal results.","de":"Dolphin 3.0 Mistral 24B ist ein auf Anweisungen abgestimmtes Modell, das für allgemeine Anwendungen, einschließlich Codierung und Mathematik, entwickelt wurde. Es ermöglicht dem Benutzer die Kontrolle über den Systemprompt und die Ausrichtung, wodurch der Datenschutz gewährleistet wird. Er ist mit verschiedenen Bibliotheken kompatibel und arbeitet effizient bei niedrigen Temperaturen (0,05-0,1) für optimale Ergebnisse."},"name":"Dolphin3.0 Mistral 24B","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"openrouter","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.749Z"},{"id":"dolphin3.0-r1-mistral-24b","aliases":["cognitivecomputations/dolphin3.0-r1-mistral-24b","dolphin3.0-r1-mistral-24b"],"description":{"en":"Dolphin 3.0 R1 is a general-purpose instruct-tuned model based on Mistral 24B, designed for coding, math, and function calling tasks. It offers user control over system prompts and alignment, ensuring data privacy. Key capabilities include structured responses and adaptability for various applications in the Hugging Face ecosystem.","de":"Dolphin 3.0 R1 ist ein auf Mistral 24B basierendes Allzweckmodell, das für Codierungs-, Mathematik- und Funktionsaufrufe konzipiert ist. Es bietet dem Benutzer die Kontrolle über die Systemaufforderungen und die Ausrichtung und gewährleistet den Datenschutz. Zu den wichtigsten Funktionen gehören strukturierte Antworten und Anpassungsfähigkeit für verschiedene Anwendungen im Hugging Face Ökosystem."},"name":"Dolphin3.0 R1 Mistral 24B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"openrouter","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.749Z"},{"id":"dots.ocr","aliases":["rednote-hilab/dots.ocr","dots.ocr"],"description":{"en":"dots.ocr is a multilingual document layout parsing model that combines layout detection and content recognition in a single vision-language framework. It supports efficient text, table, and formula extraction, achieving state-of-the-art performance on various benchmarks. Designed for documents in multiple languages, it offers fast inference and a streamlined architecture. Use cases include document parsing, OCR tasks, and layout visualization.","de":"dots.ocr ist ein mehrsprachiges Modell zum Parsen von Dokumentenlayouts, das Layout-Erkennung und Inhaltserkennung in einem einzigen Vision-Language Framework kombiniert. Es unterstützt eine effiziente Text-, Tabellen- und Formelextraktion und erreicht bei verschiedenen Benchmarks die beste Leistung. Es wurde für Dokumente in mehreren Sprachen entwickelt und bietet eine schnelle Inferenz und eine schlanke Architektur. Zu den Anwendungsfällen gehören das Parsen von Dokumenten, OCR-Aufgaben und die Visualisierung von Layouts."},"name":"Dots.Ocr","openWeights":true,"input":["text","image"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.01","output":"0.01"},"eur":{"currency":"eur","input":"0.008627114","output":"0.008627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.718Z"},{"id":"dream-machine","aliases":["lumalabs/dream-machine","dream-machine"],"name":"Dream-Machine","toolCalling":true,"input":["text","image"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":5000,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:43.741Z","contextLength":5000},{"id":"dreamshaper-8-lcm","aliases":["dreamshaper-8-lcm"],"name":"@cf/lykon/dreamshaper-8-lcm","openWeights":true,"input":["text"],"output":["image"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.508Z"},{"id":"elevenlabs-music","aliases":["elevenlabs/elevenlabs-music","elevenlabs-music"],"name":"ElevenLabs-Music","toolCalling":true,"input":["text"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":2000,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:39.403Z","contextLength":2000},{"id":"elevenlabs-v2.5-turbo","aliases":["elevenlabs/elevenlabs-v2.5-turbo","elevenlabs-v2.5-turbo"],"name":"ElevenLabs-v2.5-Turbo","toolCalling":true,"input":["text"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":128000,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:39.634Z","contextLength":128000},{"id":"elevenlabs-v3","aliases":["elevenlabs/elevenlabs-v3","elevenlabs-v3"],"name":"ElevenLabs-v3","toolCalling":true,"input":["text"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":128000,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:39.180Z","contextLength":128000},{"id":"ernie-4.5-21b-a3b","aliases":["baidu/ernie-4.5-21b-a3b-pt","baidu/ernie-4.5-21b-a3b","ernie-4.5-21b-a3b-pt","ernie-4.5-21b-a3b"],"name":"Baidu: ERNIE 4.5 21B A3B","description":{"en":"A sophisticated text-based Mixture-of-Experts (MoE) model featuring 21B total parameters with 3B activated per token, delivering exceptional multimodal understanding and generation through heterogeneous MoE structures and modality-isolated routing. Supporting an extensive 131K token context length, the model achieves efficient inference via multi-expert parallel collaboration and quantization, while advanced post-training techniques including SFT, DPO, and UPO ensure optimized performance across diverse applications with specialized routing and balancing losses for superior task handling.","de":"Ein hochentwickeltes textbasiertes Mixture-of-Experts (MoE)-Modell mit 21B Gesamtparametern und 3B Aktivierungen pro Token, das durch heterogene MoE-Strukturen und modalitätsisoliertes Routing ein außergewöhnliches multimodales Verstehen und Generieren ermöglicht. Das Modell unterstützt eine umfangreiche Kontextlänge von 131K Token und erreicht eine effiziente Inferenz durch parallele Zusammenarbeit mehrerer Experten und Quantisierung, während fortschrittliche Post-Trainingstechniken wie SFT, DPO und UPO eine optimierte Leistung bei verschiedenen Anwendungen mit speziellem Routing und Ausgleichsverlusten für eine überlegene Aufgabenbewältigung gewährleisten."},"knowledge":"2025-08-12","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","repetition_penalty","seed","stop","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":120000,"price":{"usd":{"currency":"usd","input":"0.056","output":"0.224"},"eur":{"currency":"eur","input":"0.05","output":"0.19"}}}],"lastImportedAt":"2025-11-25T00:19:20.579Z","contextLength":120000},{"id":"ernie-4.5-21b-a3b-thinking","aliases":["baidu/ernie-4.5-21b-a3b-thinking","ernie-4.5-21b-a3b-thinking"],"description":{"en":"ERNIE-4.5-21B-A3B-Thinking is Baidu's upgraded lightweight MoE model, refined to boost reasoning depth and quality for top-tier performance in logical puzzles, math, science, coding, text generation, and expert-level academic benchmarks.","de":"ERNIE-4.5-21B-A3B-Thinking ist Baidus verbessertes, leichtgewichtiges MoE-Modell, das die Tiefe und Qualität des Denkens für Spitzenleistungen bei logischen Rätseln, Mathematik, Wissenschaft, Codierung, Texterstellung und akademischen Benchmarks auf Expertenniveau verbessert."},"name":"Baidu Ernie 4.5 21B A3B Thinking","reasoning":true,"knowledge":"2025-03-01","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","seed","stop","top_k","top_p"],"providers":[{"providerId":"helicone","contextLength":128000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.28"},"eur":{"currency":"eur","input":"0.06","output":"0.24"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.056","output":"0.224"},"eur":{"currency":"eur","input":"0.05","output":"0.19"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.784Z","outputLimit":8000,"contextLength":128000},{"id":"ernie-4.5-300b-a47b","aliases":["baidu/ernie-4.5-300b-a47b-pt","baidu/ernie-4.5-300b-a47b","ernie-4.5-300b-a47b-pt","ernie-4.5-300b-a47b"],"name":"Baidu: ERNIE 4.5 300B A47B ","description":{"en":"ERNIE-4.5-300B-A47B is a 300B parameter Mixture-of-Experts (MoE) language model developed by Baidu as part of the ERNIE 4.5 series. It activates 47B parameters per token and supports text generation in both English and Chinese. Optimized for high-throughput inference and efficient scaling, it uses a heterogeneous MoE structure with advanced routing and quantization strategies, including FP8 and 2-bit formats. This version is fine-tuned for language-only tasks and supports reasoning, tool parameters, and extended context lengths up to 131k tokens. Suitable for general-purpose LLM applications with high reasoning and throughput demands.","de":"ERNIE-4.5-300B-A47B ist ein 300B-Parameter Mixture-of-Experts (MoE) Sprachmodell, das von Baidu als Teil der ERNIE 4.5 Serie entwickelt wurde. Es aktiviert 47B Parameter pro Token und unterstützt die Textgenerierung in Englisch und Chinesisch. Optimiert für Inferenz mit hohem Durchsatz und effizienter Skalierung, verwendet es eine heterogene MoE-Struktur mit fortschrittlichen Routing- und Quantisierungsstrategien, einschließlich FP8- und 2-Bit-Formaten. Diese Version ist auf reine Sprachaufgaben abgestimmt und unterstützt Reasoning, Toolparameter und erweiterte Kontextlängen bis zu 131k Token. Sie eignet sich für allgemeine LLM-Anwendungen mit hohen Anforderungen an die Argumentation und den Durchsatz."},"knowledge":"2025-06-30","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":123000,"price":{"usd":{"currency":"usd","input":"0.224","output":"0.88"},"eur":{"currency":"eur","input":"0.19","output":"0.76"}}}],"lastImportedAt":"2025-11-25T00:19:20.580Z","contextLength":123000},{"id":"ernie-4.5-vl-28b-a3b","aliases":["baidu/ernie-4.5-vl-28b-a3b-pt","baidu/ernie-4.5-vl-28b-a3b","ernie-4.5-vl-28b-a3b-pt","ernie-4.5-vl-28b-a3b"],"name":"Baidu: ERNIE 4.5 VL 28B A3B","description":{"en":"A powerful multimodal Mixture-of-Experts chat model featuring 28B total parameters with 3B activated per token, delivering exceptional text and vision understanding through its innovative heterogeneous MoE structure with modality-isolated routing. Built with scaling-efficient infrastructure for high-throughput training and inference, the model leverages advanced post-training techniques including SFT, DPO, and UPO for optimized performance, while supporting an impressive 131K context length and RLVR alignment for superior cross-modal reasoning and generation capabilities.","de":"Ein leistungsstarkes multimodales Mixture-of-Experts-Chatmodell mit 28B Gesamtparametern und 3B aktivierten Token, das durch seine innovative heterogene MoE-Struktur mit modalitätsisoliertem Routing ein außergewöhnliches Text- und Bildverständnis ermöglicht. Das Modell wurde mit einer skalierbaren, effizienten Infrastruktur für Training und Inferenz mit hohem Durchsatz entwickelt und nutzt fortschrittliche Post-Trainingstechniken wie SFT, DPO und UPO für eine optimierte Leistung, während es eine beeindruckende Kontextlänge von 131K und RLVR-Ausrichtung für überragende cross-modale Schlussfolgerungen und Generierungsfähigkeiten unterstützt."},"knowledge":"2025-08-12","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","seed","stop","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":30000,"price":{"usd":{"currency":"usd","input":"0.112","output":"0.448"},"eur":{"currency":"eur","input":"0.1","output":"0.39"}}}],"lastImportedAt":"2025-11-25T00:19:20.579Z","contextLength":30000},{"id":"ernie-4.5-vl-424b-a47b","aliases":["baidu/ernie-4.5-vl-424b-a47b-pt","baidu/ernie-4.5-vl-424b-a47b","ernie-4.5-vl-424b-a47b-pt","ernie-4.5-vl-424b-a47b"],"name":"Baidu: ERNIE 4.5 VL 424B A47B ","description":{"en":"ERNIE-4.5-VL-424B-A47B is a multimodal Mixture-of-Experts (MoE) model from Baidu’s ERNIE 4.5 series, featuring 424B total parameters with 47B active per token. It is trained jointly on text and image data using a heterogeneous MoE architecture and modality-isolated routing to enable high-fidelity cross-modal reasoning, image understanding, and long-context generation (up to 131k tokens). Fine-tuned with techniques like SFT, DPO, UPO, and RLVR, this model supports both “thinking” and non-thinking inference modes. Designed for vision-language tasks in English and Chinese, it is optimized for efficient scaling and can operate under 4-bit/8-bit quantization.","de":"ERNIE-4.5-VL-424B-A47B ist ein multimodales Mixture-of-Experts (MoE)-Modell aus der ERNIE 4.5-Serie von Baidu mit insgesamt 424B Parametern und 47B aktiven Parametern pro Token. Es wird gemeinsam auf Text- und Bilddaten trainiert, wobei eine heterogene MoE-Architektur und modalitätsgetrenntes Routing zum Einsatz kommen, um ein hochgradig glaubwürdiges cross-modales Reasoning, Bildverständnis und die Generierung langer Kontexte (bis zu 131k Token) zu ermöglichen. Mit Techniken wie SFT, DPO, UPO und RLVR fein abgestimmt, unterstützt dieses Modell sowohl \"denkende\" als auch nicht denkende Inferenzmodi. Es wurde für visuell-sprachliche Aufgaben in Englisch und Chinesisch entwickelt, ist für eine effiziente Skalierung optimiert und kann mit einer 4-Bit/8-Bit-Quantisierung arbeiten."},"knowledge":"2025-06-30","reasoning":true,"input":["image","text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":123000,"price":{"usd":{"currency":"usd","input":"0.336","output":"1"},"eur":{"currency":"eur","input":"0.29","output":"0.87"}}}],"lastImportedAt":"2025-11-25T00:19:20.580Z","contextLength":123000},{"id":"falcon-7b-instruct","aliases":["falcon-7b-instruct"],"name":"@cf/tiiuae/falcon-7b-instruct","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.360Z","outputLimit":4096,"contextLength":4096},{"id":"flux-1-schnell","aliases":["flux-1-schnell"],"name":"@cf/black-forest-labs/flux-1-schnell","openWeights":true,"input":["text"],"output":["image"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":2048,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.888Z","contextLength":2048},{"id":"flux.1-dev","aliases":["black-forest-labs/flux.1-dev","flux.1-dev"],"name":"FLUX.1-dev","knowledge":"2024-08-01","input":["text"],"output":["image"],"parameters":["temperature"],"providers":[{"providerId":"nvidia","contextLength":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.694Z"},{"id":"gemini-1.5-flash","aliases":["gemini-1.5-flash"],"name":"Gemini 1.5 Flash","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"google","contextLength":1000000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.060389798","output":"0.25881342"}}}],"lastImportedAt":"2025-11-19T12:06:32.729Z"},{"id":"gemini-1.5-flash-8b","aliases":["gemini-1.5-flash-8b"],"name":"Gemini 1.5 Flash-8B","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"google","contextLength":1000000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.15"},"eur":{"currency":"eur","input":"0.034508456","output":"0.12940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.730Z"},{"id":"gemini-1.5-pro","aliases":["gemini-1.5-pro"],"name":"Gemini 1.5 Pro","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"google","contextLength":1000000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.25","output":"5.00"},"eur":{"currency":"eur","input":"1.07","output":"4.29"}}}],"lastImportedAt":"2025-12-05T12:09:20.193Z","outputLimit":8192,"contextLength":1000000},{"id":"gemini-2.0-flash","aliases":["google/gemini-2.0-flash","gemini-2.0-flash"],"name":"Gemini 2.0 Flash","toolCalling":true,"knowledge":"2024-06-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":1048576,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.35"}}},{"providerId":"google","contextLength":1048576,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.35"}}},{"providerId":"google-vertex","contextLength":1048576,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.35"}}},{"providerId":"poe","contextLength":990000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.42"},"eur":{"currency":"eur","input":"0.09","output":"0.36"}}}],"lastImportedAt":"2025-11-21T07:05:33.883Z","outputLimit":8192,"contextLength":990000},{"id":"gemini-2.0-flash-001","aliases":["google/gemini-2.0-flash-001","gemini-2.0-flash-001"],"description":{"en":"Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.","de":"Gemini Flash 2.0 bietet im Vergleich zu [Gemini Flash 1.5](/google/gemini-flash-1.5) eine deutlich schnellere Zeit bis zum ersten Token (TTFT), während die Qualität auf dem Niveau größerer Modelle wie [Gemini Pro 1.5](/google/gemini-pro-1.5) bleibt. Es bietet bemerkenswerte Verbesserungen im Bereich des multimodalen Verständnisses, der Codierfähigkeiten, der Befolgung komplexer Anweisungen und des Funktionsaufrufs. Diese Verbesserungen sorgen für ein nahtloses und stabiles Agentenerlebnis."},"name":"Gemini 2.0 Flash","toolCalling":true,"knowledge":"2024-06-01","input":["text","image","audio","video","file"],"output":["text"],"parameters":["temperature","tools","max_tokens","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"github-copilot","contextLength":1000000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":1048576,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.944Z","outputLimit":8192,"contextLength":1000000},{"id":"gemini-2.0-flash-exp","aliases":["google/gemini-2.0-flash-exp:free","google/gemini-2.0-flash-exp","gemini-2.0-flash-exp:free","gemini-2.0-flash-exp"],"description":{"en":"Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.","de":"Gemini Flash 2.0 bietet im Vergleich zu [Gemini Flash 1.5](/google/gemini-flash-1.5) eine deutlich schnellere Zeit bis zum ersten Token (TTFT), während die Qualität auf dem Niveau größerer Modelle wie [Gemini Pro 1.5](/google/gemini-pro-1.5) bleibt. Es bietet bemerkenswerte Verbesserungen im Bereich des multimodalen Verständnisses, der Codierfähigkeiten, der Befolgung komplexer Anweisungen und des Funktionsaufrufs. Diese Verbesserungen sorgen für ein nahtloses und stabiles Agentenerlebnis."},"name":"Gemini 2.0 Flash Experimental (free)","toolCalling":true,"knowledge":"2024-12-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","max_tokens","response_format","seed","stop","tool_choice","top_p"],"providers":[{"providerId":"openrouter","contextLength":1048576,"outputLimit":1048576,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":1048576,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.502Z","outputLimit":1048576,"contextLength":1048576},{"id":"gemini-2.0-flash-lite","aliases":["google/gemini-2.0-flash-lite","gemini-2.0-flash-lite"],"name":"Gemini 2.0 Flash Lite","toolCalling":true,"knowledge":"2024-06-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":1048576,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}},{"providerId":"google","contextLength":1048576,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}},{"providerId":"google-vertex","contextLength":1048576,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}},{"providerId":"poe","contextLength":990000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.21"},"eur":{"currency":"eur","input":"0.04","output":"0.18"}}}],"lastImportedAt":"2025-11-21T07:05:34.111Z","outputLimit":8192,"contextLength":990000},{"id":"gemini-2.0-flash-lite-001","aliases":["google/gemini-2.0-flash-lite-001","gemini-2.0-flash-lite-001"],"name":"Google: Gemini 2.0 Flash Lite","description":{"en":"Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5), all at extremely economical token prices.","de":"Gemini 2.0 Flash Lite bietet im Vergleich zu [Gemini Flash 1.5](/google/gemini-flash-1.5) eine deutlich schnellere Zeit bis zum ersten Token (TTFT), während die Qualität auf dem Niveau größerer Modelle wie [Gemini Pro 1.5](/google/gemini-pro-1.5) bleibt, und das alles zu extrem günstigen Token-Preisen."},"knowledge":"2025-02-25","toolCalling":true,"input":["text","image","file","audio","video"],"output":["text"],"parameters":["max_tokens","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":1048576,"price":{"usd":{"currency":"usd","input":"0.075","output":"0.3"},"eur":{"currency":"eur","input":"0.064703355","output":"0.25881342"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"gemini-2.5-flash","aliases":["google/gemini-2.5-flash","gemini-2.5-flash"],"description":{"en":"Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. Additionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).","de":"Gemini 2.5 Flash ist das hochmoderne Arbeitsmodell von Google, das speziell für fortgeschrittenes logisches Denken, Codierung, Mathematik und wissenschaftliche Aufgaben entwickelt wurde. Es verfügt über integrierte \"Denk\"-Fähigkeiten, die es ermöglichen, Antworten mit größerer Genauigkeit und differenzierter Kontextbehandlung zu geben. Zusätzlich ist Gemini 2.5 Flash über den Parameter \"max tokens for reasoning\" konfigurierbar, wie in der Dokumentation beschrieben (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."},"name":"Gemini 2.5 Flash","reasoning":true,"toolCalling":true,"knowledge":"2025-01-01","input":["text","image","audio","video","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"vercel","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.50"},"eur":{"currency":"eur","input":"0.26","output":"2.15"}}},{"providerId":"helicone","contextLength":1048576,"outputLimit":65535,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.50"},"eur":{"currency":"eur","input":"0.26","output":"2.15"}}},{"providerId":"fastrouter","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.50"},"eur":{"currency":"eur","input":"0.26","output":"2.15"}}},{"providerId":"google","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.50"},"eur":{"currency":"eur","input":"0.26","output":"2.15"}}},{"providerId":"google-vertex","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.50"},"eur":{"currency":"eur","input":"0.26","output":"2.15"}}},{"providerId":"openrouter","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.50"},"eur":{"currency":"eur","input":"0.26","output":"2.15"}}},{"providerId":"requesty","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.50"},"eur":{"currency":"eur","input":"0.26","output":"2.15"}}},{"providerId":"sap-ai-core","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.50"},"eur":{"currency":"eur","input":"0.26","output":"2.15"}}},{"providerId":"aihubmix","contextLength":1000000,"outputLimit":65000,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}},{"providerId":"poe","contextLength":1065535,"outputLimit":65535,"price":{"usd":{"currency":"usd","input":"0.21","output":"1.80"},"eur":{"currency":"eur","input":"0.18","output":"1.55"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.093Z","outputLimit":65000,"contextLength":1000000},{"id":"gemini-2.5-flash-image","aliases":["google/gemini-2.5-flash-image","gemini-2.5-flash-image"],"description":{"en":"Gemini 2.5 Flash Image, a.k.a. \"Nano Banana,\" is now generally available. It is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations. Aspect ratios can be controlled with the [image_config API Parameter](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration)","de":"Gemini 2.5 Flash Image, auch bekannt als \"Nano Banana\", ist jetzt allgemein verfügbar. Es handelt sich um ein hochmodernes Bilderzeugungsmodell mit kontextbezogenem Verständnis. Es ist in der Lage, Bilder zu erzeugen, zu bearbeiten und Konversationen mit mehreren Umdrehungen durchzuführen. Das Seitenverhältnis kann mit dem [image_config API Parameter](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration) gesteuert werden."},"name":"Gemini 2.5 Flash Image","reasoning":true,"knowledge":"2025-06-01","input":["text","image"],"output":["text","image"],"parameters":["temperature","max_tokens","response_format","seed","structured_outputs","top_p"],"providers":[{"providerId":"google","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.30","output":"30.00"},"eur":{"currency":"eur","input":"0.25881342","output":"25.881342"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.3","output":"2.5"},"eur":{"currency":"eur","input":"0.25881342","output":"2.1567785"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.729Z"},{"id":"gemini-2.5-flash-lite","aliases":["google/gemini-2.5-flash-lite","gemini-2.5-flash-lite"],"description":{"en":"Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence.","de":"Gemini 2.5 Flash-Lite ist ein leichtgewichtiges Argumentationsmodell in der Gemini 2.5-Familie, das für extrem niedrige Latenzzeiten und Kosteneffizienz optimiert ist. Es bietet einen verbesserten Durchsatz, eine schnellere Token-Generierung und eine bessere Leistung bei gängigen Benchmarks im Vergleich zu früheren Flash-Modellen. Standardmäßig ist das \"Denken\" (d.h. Multi-Pass-Reasoning) deaktiviert, um der Geschwindigkeit den Vorrang zu geben, aber Entwickler können es über den [Reasoning-API-Parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) aktivieren, um selektiv Kosten gegen Intelligenz einzutauschen."},"name":"Gemini 2.5 Flash Lite","reasoning":true,"toolCalling":true,"knowledge":"2025-01-01","input":["text","image","audio","video","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"vercel","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"helicone","contextLength":1048576,"outputLimit":65535,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"google","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"google-vertex","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"openrouter","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"poe","contextLength":1024000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.28"},"eur":{"currency":"eur","input":"0.06","output":"0.24"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.336Z","outputLimit":64000,"contextLength":1024000},{"id":"gemini-2.5-pro","aliases":["google/gemini-2.5-pro","gemini-2.5-pro"],"description":{"en":"Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.","de":"Gemini 2.5 Pro ist das hochmoderne KI-Modell von Google, das für fortgeschrittenes logisches Denken, Codierung, Mathematik und wissenschaftliche Aufgaben entwickelt wurde. Es verfügt über \"denkende\" Fähigkeiten, die es ihm ermöglichen, Antworten mit verbesserter Genauigkeit und nuancierter Kontextbehandlung zu durchdenken. Gemini 2.5 Pro erreicht Spitzenleistungen in mehreren Benchmarks, einschließlich des ersten Platzes in der LMArena-Rangliste, was eine überlegene Anpassung an menschliche Präferenzen und komplexe Problemlösungsfähigkeiten widerspiegelt."},"name":"Gemini 2.5 Pro","reasoning":true,"toolCalling":true,"knowledge":"2025-01-01","input":["text","image","audio","video","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"cortecs","contextLength":1048576,"outputLimit":65535,"price":{"usd":{"currency":"usd","input":"1.65","output":"11.02"},"eur":{"currency":"eur","input":"1.42","output":"9.46"}}},{"providerId":"helicone","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"fastrouter","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"google","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"google-vertex","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"openrouter","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"zenmux","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"requesty","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"sap-ai-core","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"aihubmix","contextLength":2000000,"outputLimit":65000,"price":{"usd":{"currency":"usd","input":"1.25","output":"5.00"},"eur":{"currency":"eur","input":"1.07","output":"4.29"}}},{"providerId":"poe","contextLength":1065535,"outputLimit":65535,"price":{"usd":{"currency":"usd","input":"0.87","output":"7.00"},"eur":{"currency":"eur","input":"0.75","output":"6.01"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.333Z","outputLimit":64000,"contextLength":128000},{"id":"gemini-3-pro","aliases":["google/gemini-3-pro","gemini-3-pro"],"name":"Gemini 3 Pro","reasoning":true,"toolCalling":true,"knowledge":"2025-01-01","input":["text","image","video","audio"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"opencode","contextLength":1000000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"2.00","output":"12.00"},"eur":{"currency":"eur","input":"1.72","output":"10.3"}}},{"providerId":"poe","contextLength":1048576,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.60","output":"9.60"},"eur":{"currency":"eur","input":"1.37","output":"8.24"}}}],"lastImportedAt":"2025-12-10T00:21:34.312Z","outputLimit":64000,"contextLength":1000000},{"id":"gemini-3.0-pro","aliases":["google/gemini-3.0-pro","gemini-3.0-pro"],"name":"Gemini-3.0-Pro","reasoning":true,"toolCalling":true,"input":["text","image","video","audio"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":1048576,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.60","output":"9.60"},"eur":{"currency":"eur","input":"1.39","output":"8.33"}}}],"lastImportedAt":"2025-11-21T07:05:40.769Z","outputLimit":64000,"contextLength":1048576,"deprecated":true},{"id":"gemini-embedding-001","aliases":["gemini-embedding-001"],"name":"Gemini Embedding 001","knowledge":"2025-05-01","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"google","contextLength":2048,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.00"},"eur":{"currency":"eur","input":"0.12940671","output":"0"}}},{"providerId":"google-vertex","contextLength":2048,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.00"},"eur":{"currency":"eur","input":"0.12940671","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.729Z"},{"id":"gemini-live-2.5-flash","aliases":["gemini-live-2.5-flash"],"name":"Gemini Live 2.5 Flash","reasoning":true,"toolCalling":true,"knowledge":"2025-01-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"google","contextLength":128000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.4313557","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.729Z"},{"id":"gemma-2-27b-it","aliases":["google/gemma-2-27b-it","gemma-2-27b-it"],"name":"Google: Gemma 2 27B","description":{"en":"Gemma 2 27B by Google is an open model built from the same research and technology used to create the [Gemini models](/models?q=gemini). Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. See the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).","de":"Gemma 2 27B von Google ist ein offenes Modell, das auf der gleichen Forschung und Technologie basiert wie die [Gemini-Modelle](/models?q=gemini). Gemma-Modelle eignen sich für eine Vielzahl von Textgenerierungsaufgaben, einschließlich der Beantwortung von Fragen, Zusammenfassungen und Schlussfolgerungen. Weitere Einzelheiten finden Sie in der [launch announcement](https://blog.google/technology/developers/google-gemma-2/). Die Nutzung von Gemma unterliegt den [Gemma-Nutzungsbedingungen](https://ai.google.dev/gemma/terms) von Google."},"knowledge":"2024-07-13","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","response_format","stop","structured_outputs","temperature","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0.65","output":"0.65"},"eur":{"currency":"eur","input":"0.56076241","output":"0.56076241"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"gemma-2b-it-lora","aliases":["gemma-2b-it-lora"],"name":"@cf/google/gemma-2b-it-lora","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.907Z","outputLimit":8192,"contextLength":8192},{"id":"gemma-3","aliases":["google/gemma-3","gemma-3"],"name":"Google Gemma 3","toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"inference","contextLength":125000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.30"},"eur":{"currency":"eur","input":"0.12940671","output":"0.25881342"}}}],"lastImportedAt":"2025-11-19T12:06:32.754Z"},{"id":"gemma-3-12b-it","aliases":["google/gemma-3-12b-it:free","workers-ai/gemma-3-12b-it","unsloth/gemma-3-12b-it","google/gemma-3-12b-it","gemma-3-12b-it:free","gemma-3-12b-it"],"description":{"en":"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 12B is the second largest in the family of Gemma 3 models after [Gemma 3 27B](google/gemma-3-27b-it)","de":"Gemma 3 führt Multimodalität ein und unterstützt Eingaben in Bildsprache und Textausgaben. Es verarbeitet Kontextfenster mit bis zu 128k Token, versteht über 140 Sprachen und bietet verbesserte Mathematik-, Argumentations- und Chat-Funktionen, einschließlich strukturierter Ausgaben und Funktionsaufrufe. Gemma 3 12B ist das zweitgrößte Modell der Gemma 3-Familie nach [Gemma 3 27B](google/gemma-3-27b-it)"},"name":"Gemma 3 12b It","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","max_tokens","seed","top_p","frequency_penalty","logit_bias","min_p","presence_penalty","repetition_penalty","response_format","stop","structured_outputs","top_k"],"providers":[{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.10"},"eur":{"currency":"eur","input":"0.03","output":"0.09"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.10"},"eur":{"currency":"eur","input":"0.04","output":"0.09"}}},{"providerId":"cloudflare-workers-ai","contextLength":80000,"outputLimit":80000,"price":{"usd":{"currency":"usd","input":"0.35","output":"0.56"},"eur":{"currency":"eur","input":"0.3","output":"0.48"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":96000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.1"},"eur":{"currency":"eur","input":"0.03","output":"0.09"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":32768,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.408Z","outputLimit":8192,"contextLength":80000},{"id":"gemma-3-27b-it","aliases":["google/gemma-3-27b-it:free","unsloth/gemma-3-27b-it","google/gemma-3-27b-it","gemma-3-27b-it:free","gemma-3-27b-it"],"description":{"en":"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 27B is Google's latest open source model, successor to [Gemma 2](google/gemma-2-27b-it)","de":"Gemma 3 führt Multimodalität ein und unterstützt Eingaben in Bildsprache und Textausgaben. Es verarbeitet Kontextfenster mit bis zu 128k Token, versteht über 140 Sprachen und bietet verbesserte Mathematik-, Argumentations- und Chat-Funktionen, einschließlich strukturierter Ausgaben und Funktionsaufrufe. Gemma 3 27B ist Googles neuestes Open-Source-Modell, Nachfolger von [Gemma 2](google/gemma-2-27b-it)"},"name":"Gemma-3-27B-IT","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","top_p","logit_bias","min_p","tool_choice","top_k"],"providers":[{"providerId":"nvidia","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"chutes","contextLength":96000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.15"},"eur":{"currency":"eur","input":"0.03","output":"0.13"}}},{"providerId":"openrouter","contextLength":96000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.15"},"eur":{"currency":"eur","input":"0.03","output":"0.13"}}},{"providerId":"scaleway","contextLength":40000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.25","output":"0.50"},"eur":{"currency":"eur","input":"0.21","output":"0.43"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":131072,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T00:21:30.947Z","outputLimit":8192,"contextLength":40000},{"id":"gemma-3-4b-it","aliases":["google/gemma-3-4b-it:free","unsloth/gemma-3-4b-it","google/gemma-3-4b-it","gemma-3-4b-it:free","gemma-3-4b-it"],"description":{"en":"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.","de":"Gemma 3 führt Multimodalität ein und unterstützt Eingaben in Bildsprache und Textausgaben. Es verarbeitet Kontextfenster mit bis zu 128k Token, versteht über 140 Sprachen und bietet verbesserte Mathematik-, Argumentations- und Chatfunktionen, einschließlich strukturierter Ausgaben und Funktionsaufrufe."},"name":"Gemma 3 4b It","openWeights":true,"knowledge":"2025-03-13","input":["text","image"],"output":["text"],"parameters":["temperature","max_tokens","response_format","seed","structured_outputs","top_p","frequency_penalty","min_p","presence_penalty","repetition_penalty","stop","top_k"],"providers":[{"providerId":"chutes","contextLength":96000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":96000,"price":{"usd":{"currency":"usd","input":"0.01703012","output":"0.0681536"},"eur":{"currency":"eur","input":"0.014692078667368","output":"0.05879688767104"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":32768,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.719Z"},{"id":"gemma-3n-e2b-it","aliases":["google/gemma-3n-e2b-it:free","google/gemma-3n-e2b-it","gemma-3n-e2b-it:free","gemma-3n-e2b-it","gemma-3n-e2b-it"],"name":"Google: Gemma 3n 2B (free)","description":{"en":"Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to operate efficiently at an effective parameter size of 2B while leveraging a 6B architecture. Based on the MatFormer architecture, it supports nested submodels and modular composition via the Mix-and-Match framework. Gemma 3n models are optimized for low-resource deployment, offering 32K context length and strong multilingual and reasoning performance across common benchmarks. This variant is trained on a diverse corpus including code, math, web, and multimodal data.","de":"Gemma 3n E2B IT ist ein von Google DeepMind entwickeltes multimodales, anweisungsabgestimmtes Modell, das für einen effizienten Betrieb bei einer effektiven Parametergröße von 2B ausgelegt ist und eine 6B-Architektur nutzt. Es basiert auf der MatFormer-Architektur und unterstützt verschachtelte Submodelle und modulare Komposition über das Mix-and-Match-Framework. Gemma 3n-Modelle sind für den Einsatz mit geringen Ressourcen optimiert und bieten eine Kontextlänge von 32K sowie eine starke mehrsprachige und schlussfolgernde Leistung bei gängigen Benchmarks. Diese Variante wurde auf einem vielfältigen Korpus trainiert, darunter Code-, Mathematik-, Web- und multimodale Daten."},"knowledge":"2025-07-09","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","temperature","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0","output":"0"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.764Z"},{"id":"gemma-3n-e4b-it","aliases":["google/gemma-3n-e4b-it:free","google/gemma-3n-e4b-it","gemma-3n-e4b-it:free","gemma-3n-e4b-it"],"description":{"en":"Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as phones, laptops, and tablets. It supports multimodal inputs—including text, visual data, and audio—enabling diverse tasks such as text generation, speech recognition, translation, and image analysis. Leveraging innovations like Per-Layer Embedding (PLE) caching and the MatFormer architecture, Gemma 3n dynamically manages memory usage and computational load by selectively activating model parameters, significantly reducing runtime resource requirements. This model supports a wide linguistic range (trained in over 140 languages) and features a flexible 32K token context window. Gemma 3n can selectively load parameters, optimizing memory and computational efficiency based on the task or device capabilities, making it well-suited for privacy-focused, offline-capable applications and on-device AI solutions. [Read more in the blog post](https://developers.googleblog.com/en/introducing-gemma-3n/)","de":"Gemma 3n E4B-it ist für die effiziente Ausführung auf mobilen und ressourcenarmen Geräten wie Telefonen, Laptops und Tablets optimiert. Es unterstützt multimodale Eingaben - einschließlich Text, visuelle Daten und Audio - und ermöglicht so vielfältige Aufgaben wie Texterstellung, Spracherkennung, Übersetzung und Bildanalyse. Durch die Nutzung von Innovationen wie Per-Layer Embedding (PLE) Caching und der MatFormer-Architektur verwaltet Gemma 3n dynamisch die Speichernutzung und die Rechenlast durch selektive Aktivierung von Modellparametern, wodurch die Anforderungen an die Laufzeitressourcen erheblich reduziert werden. Dieses Modell unterstützt ein breites linguistisches Spektrum (trainiert in über 140 Sprachen) und verfügt über ein flexibles 32K Token-Kontextfenster. Gemma 3n ist in der Lage, Parameter selektiv zu laden und so die Speicher- und Recheneffizienz je nach Aufgabe oder Gerätefähigkeiten zu optimieren, wodurch es sich gut für datenschutzorientierte, offline-fähige Anwendungen und On-Device-KI-Lösungen eignet. [Lesen Sie mehr im Blogbeitrag](https://developers.googleblog.com/en/introducing-gemma-3n/)"},"name":"Gemma 3n E4B IT","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text","image","audio"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","top_p","logit_bias","min_p","repetition_penalty","top_k"],"providers":[{"providerId":"openrouter","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.04"},"eur":{"currency":"eur","input":"0.02","output":"0.03"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":8192,"outputLimit":8192,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.502Z","outputLimit":8192,"contextLength":8192},{"id":"gemma-7b-it","aliases":["gemma-7b-it"],"name":"@hf/google/gemma-7b-it","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.207Z","outputLimit":8192,"contextLength":8192},{"id":"gemma-7b-it-lora","aliases":["gemma-7b-it-lora"],"name":"@cf/google/gemma-7b-it-lora","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":3500,"outputLimit":3500,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.054Z","outputLimit":3500,"contextLength":3500},{"id":"gemma-sea-lion-v4-27b-it","aliases":["workers-ai/gemma-sea-lion-v4-27b-it","gemma-sea-lion-v4-27b-it"],"name":"@cf/aisingapore/gemma-sea-lion-v4-27b-it","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.35","output":"0.56"},"eur":{"currency":"eur","input":"0.3","output":"0.48"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.946Z","outputLimit":16384,"contextLength":128000},{"id":"gemma2-9b-it","aliases":["google/gemma-2-9b-it:free","google/gemma-2-9b-it","gemma-2-9b-it:free","gemma-2-9b-it","gemma2-9b-it"],"description":{"en":"Gemma 2 9B by Google is an advanced, open-source language model that sets a new standard for efficiency and performance in its size class. Designed for a wide variety of tasks, it empowers developers and researchers to build innovative applications, while maintaining accessibility, safety, and cost-effectiveness. See the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).","de":"Gemma 2 9B von Google ist ein fortschrittliches Open-Source-Sprachmodell, das in seiner Größenklasse einen neuen Standard für Effizienz und Leistung setzt. Es wurde für eine Vielzahl von Aufgaben entwickelt und ermöglicht es Entwicklern und Forschern, innovative Anwendungen zu erstellen und dabei Zugänglichkeit, Sicherheit und Kosteneffizienz zu gewährleisten. Weitere Einzelheiten finden Sie in der [Launch-Ankündigung] (https://blog.google/technology/developers/google-gemma-2/). Die Nutzung von Gemma unterliegt den [Gemma-Nutzungsbedingungen] von Google (https://ai.google.dev/gemma/terms)."},"name":"Gemma 2 9B","toolCalling":true,"openWeights":true,"knowledge":"2024-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","top_k","top_p"],"providers":[{"providerId":"groq","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"helicone","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.01","output":"0.03"},"eur":{"currency":"eur","input":"0.01","output":"0.03"}}},{"providerId":"openrouter","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.09"},"eur":{"currency":"eur","input":"0.03","output":"0.08"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":8192,"outputLimit":8192,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.306Z","outputLimit":8192,"contextLength":8192},{"id":"glm-4-32b","aliases":["z-ai/glm-4-32b-0414","z-ai/glm-4-32b","glm-4-32b-0414","glm-4-32b"],"name":"Z.AI: GLM 4 32B ","description":{"en":"GLM 4 32B is a cost-effective foundation language model. It can efficiently perform complex tasks and has significantly enhanced capabilities in tool use, online search, and code-related intelligent tasks. It is made by the same lab behind the thudm models.","de":"GLM 4 32B ist ein kosteneffizientes Grundsprachenmodell. Es ist in der Lage, komplexe Aufgaben effizient auszuführen und verfügt über deutlich verbesserte Fähigkeiten bei der Nutzung von Werkzeugen, der Online-Suche und bei codebezogenen intelligenten Aufgaben. Es stammt aus demselben Labor, das auch die thudm-Modelle entwickelt hat."},"knowledge":"2025-07-24","toolCalling":true,"input":["text"],"output":["text"],"parameters":["max_tokens","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.1"},"eur":{"currency":"eur","input":"0.08627114","output":"0.08627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.764Z"},{"id":"glm-4.1v-9b-thinking","aliases":["thudm/glm-4.1v-9b-thinking","glm-4.1v-9b-thinking"],"name":"THUDM: GLM 4.1V 9B Thinking","description":{"en":"GLM-4.1V-9B-Thinking is a 9B parameter vision-language model developed by THUDM, based on the GLM-4-9B foundation. It introduces a reasoning-centric \"thinking paradigm\" enhanced with reinforcement learning to improve multimodal reasoning, long-context understanding (up to 64K tokens), and complex problem solving. It achieves state-of-the-art performance among models in its class, outperforming even larger models like Qwen-2.5-VL-72B on a majority of benchmark tasks.","de":"GLM-4.1V-9B-Thinking ist ein von THUDM entwickeltes 9B-Parameter-Vision-Language-Modell, das auf der Grundlage von GLM-4-9B basiert. Es führt ein schlussfolgerungszentriertes \"Denkparadigma\" ein, das durch Verstärkungslernen verbessert wird, um multimodales Schlussfolgern, das Verstehen langer Kontexte (bis zu 64K Token) und das Lösen komplexer Probleme zu verbessern. Es erreicht die beste Leistung unter den Modellen seiner Klasse und übertrifft sogar größere Modelle wie Qwen-2.5-VL-72B bei einer Mehrzahl von Benchmark-Aufgaben."},"knowledge":"2025-07-11","reasoning":true,"input":["image","text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":65536,"price":{"usd":{"currency":"usd","input":"0.028","output":"0.1104"},"eur":{"currency":"eur","input":"0.02","output":"0.1"}}}],"lastImportedAt":"2025-11-25T00:19:20.580Z","contextLength":65536},{"id":"glm-4.5","aliases":["hf:zai-org/glm-4.5","zai-org/glm-4.5","zhipuai/glm-4.5","z-ai/glm-4.5","zai/glm-4.5","glm-4.5"],"description":{"en":"GLM-4.5 is our latest flagship foundation model, purpose-built for agent-based applications. It leverages a Mixture-of-Experts (MoE) architecture and supports a context length of up to 128k tokens. GLM-4.5 delivers significantly enhanced capabilities in reasoning, code generation, and agent alignment. It supports a hybrid inference mode with two options, a \"thinking mode\" designed for complex reasoning and tool use, and a \"non-thinking mode\" optimized for instant responses. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)","de":"GLM-4.5 ist unser neuestes Flaggschiff unter den Basismodellen, das speziell für agentenbasierte Anwendungen entwickelt wurde. Es nutzt eine Mixture-of-Experts (MoE) Architektur und unterstützt eine Kontextlänge von bis zu 128k Token. GLM-4.5 bietet deutlich verbesserte Fähigkeiten in den Bereichen Reasoning, Codegenerierung und Agentenausrichtung. Er unterstützt einen hybriden Inferenzmodus mit zwei Optionen: einen \"Denkmodus\", der für komplexe Schlussfolgerungen und den Einsatz von Werkzeugen konzipiert ist, und einen \"Nicht-Denkmodus\", der für sofortige Antworten optimiert ist. Benutzer können das Schlussfolgerungsverhalten mit dem Bool `reasoning` `enabled` steuern. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)"},"name":"GLM-4.5","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-05-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_a","top_k","top_logprobs","top_p"],"providers":[{"providerId":"zai-coding-plan","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":128000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.89"}}},{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.89"}}},{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.35","output":"1.55"},"eur":{"currency":"eur","input":"0.3","output":"1.33"}}},{"providerId":"huggingface","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.89"}}},{"providerId":"zhipuai","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.89"}}},{"providerId":"openrouter","contextLength":128000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.89"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.19"},"eur":{"currency":"eur","input":"0.47","output":"1.88"}}},{"providerId":"deepinfra","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.89"}}},{"providerId":"zai","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.89"}}},{"providerId":"modelscope","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-05T00:21:32.516Z","outputLimit":8192,"contextLength":128000},{"id":"glm-4.5-air","aliases":["z-ai/glm-4.5-air:free","zai-org/glm-4.5-air","z-ai/glm-4.5-air","glm-4.5-air:free","zai/glm-4.5-air","glm-4.5-air"],"description":{"en":"GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a \"thinking mode\" for advanced reasoning and tool use, and a \"non-thinking mode\" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)","de":"GLM-4.5-Air ist die leichtgewichtige Variante unserer neuesten Flaggschiff-Modellfamilie, die ebenfalls speziell für agentenorientierte Anwendungen entwickelt wurde. Wie GLM-4.5 verwendet es die Mixture-of-Experts (MoE)-Architektur, jedoch mit einer kompakteren Parametergröße. GLM-4.5-Air unterstützt auch hybride Inferenzmodi und bietet einen \"Denkmodus\" für fortgeschrittene Schlussfolgerungen und die Verwendung von Werkzeugen sowie einen \"Nicht-Denkmodus\" für Echtzeit-Interaktion. Benutzer können das Schlussfolgerungsverhalten mit dem Bool `reasoning` `enabled` steuern. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)"},"name":"GLM-4.5-Air","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-05-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"zai-coding-plan","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":128000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.10"},"eur":{"currency":"eur","input":"0.17254228","output":"0.94898254"}}},{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.20"},"eur":{"currency":"eur","input":"0.17254228","output":"1.03525368"}}},{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"huggingface","contextLength":128000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.10"},"eur":{"currency":"eur","input":"0.17254228","output":"0.94898254"}}},{"providerId":"zhipuai","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.10"},"eur":{"currency":"eur","input":"0.17254228","output":"0.94898254"}}},{"providerId":"openrouter","contextLength":128000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.10"},"eur":{"currency":"eur","input":"0.17254228","output":"0.94898254"}}},{"providerId":"zenmux","contextLength":128000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.11","output":"0.56"},"eur":{"currency":"eur","input":"0.094898254","output":"0.483118384"}}},{"providerId":"submodel","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.50"},"eur":{"currency":"eur","input":"0.08627114","output":"0.4313557"}}},{"providerId":"zai","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.10"},"eur":{"currency":"eur","input":"0.17254228","output":"0.94898254"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":128000,"outputLimit":96000,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.683Z"},{"id":"glm-4.5-flash","aliases":["glm-4.5-flash"],"name":"GLM-4.5-Flash","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"zai-coding-plan","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"zhipuai","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"zai","contextLength":131072,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.682Z"},{"id":"glm-4.5-fp8","aliases":["zai-org/glm-4.5-fp8","glm-4.5-fp8"],"description":{"en":"GLM-4.5 is an open-source Mixture-of-Experts large language model with 355B parameters, designed for intelligent agents. It supports hybrid reasoning with thinking and non-thinking modes, excelling in reasoning, coding, and agentic tasks. Available in standard and compact versions, it achieves strong performance across multiple benchmarks.","de":"GLM-4.5 ist ein quelloffenes Mixture-of-Experts-Sprachmodell mit 355B Parametern, das für intelligente Agenten entwickelt wurde. Es unterstützt hybrides Denken mit denkenden und nicht denkenden Modi und zeichnet sich durch logisches Denken, Codierung und agenturische Aufgaben aus. Es ist in einer Standard- und einer kompakten Version erhältlich und erreicht in mehreren Benchmarks eine starke Leistung."},"name":"GLM 4.5 FP8","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"submodel","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.80"},"eur":{"currency":"eur","input":"0.17254228","output":"0.69016912"}}}],"lastImportedAt":"2025-11-19T12:06:32.754Z"},{"id":"glm-4.5v","aliases":["zai-org/glm-4.5v","z-ai/glm-4.5v","zai/glm-4.5v","glm-4.5v"],"description":{"en":"GLM-4.5V is a vision-language foundation model for multimodal agent applications. Built on a Mixture-of-Experts (MoE) architecture with 106B parameters and 12B activated parameters, it achieves state-of-the-art results in video understanding, image Q&A, OCR, and document parsing, with strong gains in front-end web coding, grounding, and spatial reasoning. It offers a hybrid inference mode: a \"thinking mode\" for deep reasoning and a \"non-thinking mode\" for fast responses. Reasoning behavior can be toggled via the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)","de":"GLM-4.5V ist ein Vision-Language-Grundmodell für multimodale Agentenanwendungen. Es basiert auf einer Mixture-of-Experts-Architektur (MoE) mit 106B Parametern und 12B aktivierten Parametern und erzielt modernste Ergebnisse in den Bereichen Videoverstehen, Bild-Q&A, OCR und Dokumenten-Parsing, mit starken Zuwächsen bei Front-End-Webcoding, Grounding und räumlichem Denken. Es bietet einen hybriden Inferenzmodus: einen \"Denkmodus\" für tiefgehende Schlussfolgerungen und einen \"Nicht-Denkmodus\" für schnelle Antworten. Das Reasoning-Verhalten kann über das Bool `reasoning` `enabled` umgeschaltet werden. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)"},"name":"GLM-4.5V","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"zai-coding-plan","contextLength":64000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":66000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.60","output":"1.80"},"eur":{"currency":"eur","input":"0.51","output":"1.54"}}},{"providerId":"zhipuai","contextLength":64000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.60","output":"1.80"},"eur":{"currency":"eur","input":"0.51","output":"1.54"}}},{"providerId":"openrouter","contextLength":64000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.60","output":"1.80"},"eur":{"currency":"eur","input":"0.51","output":"1.54"}}},{"providerId":"zai","contextLength":64000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.60","output":"1.80"},"eur":{"currency":"eur","input":"0.51","output":"1.54"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T12:08:56.197Z","outputLimit":16000,"contextLength":64000},{"id":"glm-4.6","aliases":["hf:zai-org/glm-4.6","zai-org/glm-4.6","zhipuai/glm-4.6","novita/glm-4.6","z-ai/glm-4.6","zai/glm-4.6","glm-4.6"],"description":{"en":"Compared with GLM-4.5, this generation brings several key improvements: Longer context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks. Superior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages. Advanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability. More capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks. Refined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.","de":"Im Vergleich zu GLM-4.5 bringt diese Generation mehrere wichtige Verbesserungen: Größeres Kontextfenster: Das Kontextfenster wurde von 128K auf 200K Token erweitert, wodurch das Modell komplexere agenturische Aufgaben bewältigen kann. Bessere Codierleistung: Das Modell erreicht höhere Punktzahlen bei Code-Benchmarks und zeigt in Anwendungen wie Claude Code、Cline、Roo Code und Kilo Code eine bessere Leistung in der realen Welt, einschließlich Verbesserungen bei der Erstellung visuell ausgefeilter Frontend-Seiten. Erweiterte Argumentation: GLM-4.6 zeigt eine deutliche Verbesserung der Argumentationsleistung und unterstützt die Verwendung von Werkzeugen während der Inferenz, was zu einer stärkeren Gesamtfähigkeit führt. Leistungsfähigere Agenten: GLM-4.6 zeigt eine bessere Leistung bei der Verwendung von Werkzeugen und suchbasierten Agenten und lässt sich besser in Agenten-Frameworks integrieren. Verfeinertes Schreiben: Bessere Anpassung an menschliche Präferenzen in Bezug auf Stil und Lesbarkeit und natürlichere Leistung in Rollenspielszenarien."},"name":"GLM-4.6","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_a","top_k","top_logprobs","top_p"],"providers":[{"providerId":"zai-coding-plan","contextLength":204800,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":200000,"outputLimit":96000,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.88"}}},{"providerId":"chutes","contextLength":202752,"outputLimit":202752,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.75"},"eur":{"currency":"eur","input":"0.34","output":"1.5"}}},{"providerId":"togetherai","contextLength":200000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.88"}}},{"providerId":"baseten","contextLength":200000,"outputLimit":200000,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.88"}}},{"providerId":"helicone","contextLength":204800,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.45","output":"1.50"},"eur":{"currency":"eur","input":"0.39","output":"1.28"}}},{"providerId":"huggingface","contextLength":200000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.88"}}},{"providerId":"opencode","contextLength":204800,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.88"}}},{"providerId":"zhipuai","contextLength":204800,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.88"}}},{"providerId":"openrouter","contextLength":200000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.88"}}},{"providerId":"zenmux","contextLength":200000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.35","output":"1.54"},"eur":{"currency":"eur","input":"0.3","output":"1.32"}}},{"providerId":"iflowcn","contextLength":200000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"synthetic","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.19"},"eur":{"currency":"eur","input":"0.47","output":"1.87"}}},{"providerId":"zai","contextLength":204800,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.20"},"eur":{"currency":"eur","input":"0.51","output":"1.88"}}},{"providerId":"aihubmix","contextLength":204800,"outputLimit":204800,"price":{"usd":{"currency":"usd","input":"0.27","output":"1.10"},"eur":{"currency":"eur","input":"0.23","output":"0.94"}}},{"providerId":"io-net","contextLength":200000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.75"},"eur":{"currency":"eur","input":"0.34","output":"1.5"}}},{"providerId":"modelscope","contextLength":202752,"outputLimit":98304,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"poe","price":{"usd":{"currency":"usd"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T12:08:56.198Z","outputLimit":4096,"contextLength":200000},{"id":"glm-4.6-tee","aliases":["zai-org/glm-4.6-tee","glm-4.6-tee"],"name":"GLM 4.6 TEE","openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"chutes","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}}],"lastImportedAt":"2025-12-12T00:21:33.658Z","outputLimit":8192,"contextLength":32768},{"id":"glm-4.6:cloud","aliases":["glm-4.6:cloud"],"name":"GLM-4.6","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.596Z","outputLimit":8192,"contextLength":200000},{"id":"glm-4.6:exacto","aliases":["z-ai/glm-4.6:exacto","glm-4.6:exacto","z-ai/glm-4.6","glm-4.6"],"description":{"en":"Compared with GLM-4.5, this generation brings several key improvements: Longer context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks. Superior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages. Advanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability. More capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks. Refined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.","de":"Im Vergleich zu GLM-4.5 bringt diese Generation mehrere wichtige Verbesserungen: Größeres Kontextfenster: Das Kontextfenster wurde von 128K auf 200K Token erweitert, wodurch das Modell komplexere agenturische Aufgaben bewältigen kann. Bessere Codierleistung: Das Modell erreicht höhere Punktzahlen bei Code-Benchmarks und zeigt in Anwendungen wie Claude Code、Cline、Roo Code und Kilo Code eine bessere Leistung in der realen Welt, einschließlich Verbesserungen bei der Erstellung visuell ausgefeilter Frontend-Seiten. Erweiterte Argumentation: GLM-4.6 zeigt eine deutliche Verbesserung der Argumentationsleistung und unterstützt die Verwendung von Werkzeugen während der Inferenz, was zu einer stärkeren Gesamtfähigkeit führt. Leistungsfähigere Agenten: GLM-4.6 zeigt eine bessere Leistung bei der Verwendung von Werkzeugen und suchbasierten Agenten und lässt sich besser in Agenten-Frameworks integrieren. Verfeinertes Schreiben: Bessere Anpassung an menschliche Präferenzen in Bezug auf Stil und Lesbarkeit und natürlichere Leistung in Rollenspielszenarien."},"name":"GLM 4.6 (exacto)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-09-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"openrouter","contextLength":200000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.60","output":"1.90"},"eur":{"currency":"eur","input":"0.51","output":"1.63"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T00:21:33.756Z","outputLimit":128000,"contextLength":200000},{"id":"glm-4.6v","aliases":["z-ai/glm-4.6-20251208","zai-org/glm-4.6v","glm-4.6-20251208","z-ai/glm-4.6v","glm-4.6v"],"description":{"en":"GLM-4.6V is a large multimodal model designed for high-fidelity visual understanding and long-context reasoning across images, documents, and mixed media. It supports up to 128K tokens, processes complex page layouts and charts directly as visual inputs, and integrates native multimodal function calling to connect perception with downstream tool execution. The model also enables interleaved image-text generation and UI reconstruction workflows, including screenshot-to-HTML synthesis and iterative visual editing.","de":"GLM-4.6V ist ein umfangreiches multimodales Modell, das für visuelles Verständnis und Schlussfolgerungen mit langem Kontext über Bilder, Dokumente und gemischte Medien hinweg entwickelt wurde. Es unterstützt bis zu 128K Token, verarbeitet komplexe Seitenlayouts und Diagramme direkt als visuelle Eingaben und integriert native multimodale Funktionsaufrufe, um die Wahrnehmung mit der nachgelagerten Tool-Ausführung zu verbinden. Das Modell ermöglicht auch verschachtelte Bild-Text-Generierung und UI-Rekonstruktions-Workflows, einschließlich Screenshot-zu-HTML-Synthese und iterativer visueller Bearbeitung."},"name":"GLM-4.6V","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"zai-coding-plan","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"chutes","contextLength":131072,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.90"},"eur":{"currency":"eur","input":"0.26","output":"0.77"}}},{"providerId":"zhipuai","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.90"},"eur":{"currency":"eur","input":"0.26","output":"0.77"}}},{"providerId":"zai","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.90"},"eur":{"currency":"eur","input":"0.26","output":"0.77"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.3","output":"0.9"},"eur":{"currency":"eur","input":"0.26","output":"0.77"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T00:21:30.899Z","outputLimit":32768,"contextLength":128000},{"id":"glm-4p5","aliases":["accounts/fireworks/models/glm-4p5","glm-4p5"],"name":"GLM 4.5","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"fireworks-ai","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.19"},"eur":{"currency":"eur","input":"0.47449127","output":"1.889337966"}}}],"lastImportedAt":"2025-11-19T12:06:32.756Z"},{"id":"glm-4p5-air","aliases":["accounts/fireworks/models/glm-4p5-air","glm-4p5-air"],"name":"GLM 4.5 Air","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"fireworks-ai","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.22","output":"0.88"},"eur":{"currency":"eur","input":"0.189796508","output":"0.759186032"}}}],"lastImportedAt":"2025-11-19T12:06:32.756Z"},{"id":"glm-z1-32b","aliases":["thudm/glm-z1-32b:free","glm-z1-32b:free","glm-z1-32b"],"name":"GLM Z1 32B (free)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"openrouter","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.749Z"},{"id":"goliath-120b","aliases":["alpindale/goliath-120b","goliath-120b"],"name":"Goliath 120B","description":{"en":"A large LLM created by combining two fine-tuned Llama 70B models into one 120B model. Combines Xwin and Euryale. Credits to - [@chargoddard](https://huggingface.co/chargoddard) for developing the framework used to merge the model - [mergekit](https://github.com/cg123/mergekit). - [@Undi95](https://huggingface.co/Undi95) for helping with the merge ratios. #merge","de":"Ein großes LLM, das durch die Kombination von zwei fein abgestimmten Llama 70B-Modellen zu einem 120B-Modell entstanden ist. Kombiniert Xwin und Euryale. Dank an - [@chargoddard](https://huggingface.co/chargoddard) für die Entwicklung des Rahmens, der für die Zusammenführung des Modells verwendet wurde - [mergekit](https://github.com/cg123/mergekit). - [@Undi95](https://huggingface.co/Undi95) für die Hilfe bei den Zusammenführungskennzahlen. #Zusammenführen"},"knowledge":"2023-11-10","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","temperature","top_a","top_k","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":6144,"price":{"usd":{"currency":"usd","input":"6","output":"8"},"eur":{"currency":"eur","input":"5.1762684","output":"6.9016912"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"google-gemma-3-27b-it","aliases":["google-gemma-3-27b-it"],"name":"Google Gemma 3 27B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2025-07-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"venice","contextLength":202752,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.12","output":"0.20"},"eur":{"currency":"eur","input":"0.1","output":"0.17"}}}],"lastImportedAt":"2025-12-10T00:21:34.161Z","outputLimit":8192,"contextLength":202752},{"id":"gpt-3.5-turbo","aliases":["openai/gpt-3.5-turbo","gpt-3.5-turbo"],"description":{"en":"GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, and is optimized for chat and traditional completion tasks. Training data up to Sep 2021.","de":"GPT-3.5 Turbo ist das schnellste Modell von OpenAI. Es kann natürliche Sprache oder Code verstehen und generieren und ist für Chats und herkömmliche Erledigungsaufgaben optimiert. Trainingsdaten bis September 2021."},"name":"gpt 3.5 turuo","toolCalling":true,"knowledge":"2021-09-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"1.50"},"eur":{"currency":"eur","input":"0","output":"1.29"}}},{"providerId":"openai","contextLength":16385,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.50"},"eur":{"currency":"eur","input":"0.43","output":"1.29"}}},{"providerId":"poe","contextLength":16384,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.45","output":"1.40"},"eur":{"currency":"eur","input":"0.39","output":"1.2"}}},{"providerId":"openrouter","contextLength":16385,"price":{"usd":{"currency":"usd","input":"0.5","output":"1.5"},"eur":{"currency":"eur","input":"0.43","output":"1.29"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.455Z","outputLimit":2048,"contextLength":16384},{"id":"gpt-3.5-turbo-0125","aliases":["gpt-3.5-turbo-0125"],"name":"GPT-3.5 Turbo 0125","knowledge":"2021-08-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"azure","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.50"},"eur":{"currency":"eur","input":"0.4313557","output":"1.2940671"}}},{"providerId":"azure-cognitive-services","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.50"},"eur":{"currency":"eur","input":"0.4313557","output":"1.2940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.727Z"},{"id":"gpt-3.5-turbo-0301","aliases":["gpt-3.5-turbo-0301"],"name":"GPT-3.5 Turbo 0301","knowledge":"2021-08-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"azure","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"1.50","output":"2.00"},"eur":{"currency":"eur","input":"1.2940671","output":"1.7254228"}}},{"providerId":"azure-cognitive-services","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"1.50","output":"2.00"},"eur":{"currency":"eur","input":"1.2940671","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.727Z"},{"id":"gpt-3.5-turbo-0613","aliases":["openai/gpt-3.5-turbo-0613","gpt-3.5-turbo-0613"],"description":{"en":"GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, and is optimized for chat and traditional completion tasks. Training data up to Sep 2021.","de":"GPT-3.5 Turbo ist das schnellste Modell von OpenAI. Es kann natürliche Sprache oder Code verstehen und generieren und ist für Chats und herkömmliche Erledigungsaufgaben optimiert. Trainingsdaten bis September 2021."},"name":"GPT-3.5 Turbo 0613","toolCalling":true,"knowledge":"2021-08-01","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","tools","top_logprobs","top_p"],"providers":[{"providerId":"azure","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"3.00","output":"4.00"},"eur":{"currency":"eur","input":"2.5881342","output":"3.4508456"}}},{"providerId":"azure-cognitive-services","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"3.00","output":"4.00"},"eur":{"currency":"eur","input":"2.5881342","output":"3.4508456"}}},{"providerId":"openrouter","contextLength":4095,"price":{"usd":{"currency":"usd","input":"1","output":"2"},"eur":{"currency":"eur","input":"0.8627114","output":"1.7254228"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.727Z"},{"id":"gpt-3.5-turbo-1106","aliases":["gpt-3.5-turbo-1106"],"name":"GPT-3.5 Turbo 1106","knowledge":"2021-08-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"azure","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.00","output":"2.00"},"eur":{"currency":"eur","input":"0.8627114","output":"1.7254228"}}},{"providerId":"azure-cognitive-services","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.00","output":"2.00"},"eur":{"currency":"eur","input":"0.8627114","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.728Z"},{"id":"gpt-3.5-turbo-16k","aliases":["openai/gpt-3.5-turbo-16k","gpt-3.5-turbo-16k"],"name":"OpenAI: GPT-3.5 Turbo 16k","description":{"en":"This model offers four times the context length of gpt-3.5-turbo, allowing it to support approximately 20 pages of text in a single request at a higher cost. Training data: up to Sep 2021.","de":"Dieses Modell bietet die vierfache Kontextlänge von gpt-3.5-turbo, so dass es bei höheren Kosten etwa 20 Seiten Text in einer einzigen Anfrage unterstützen kann. Trainingsdaten: bis zu September 2021."},"knowledge":"2023-08-28","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":16385,"price":{"usd":{"currency":"usd","input":"3","output":"4"},"eur":{"currency":"eur","input":"2.5881342","output":"3.4508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"gpt-3.5-turbo-instruct","aliases":["openai/gpt-3.5-turbo-instruct","gpt-3.5-turbo-instruct"],"description":{"en":"This model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.","de":"Bei diesem Modell handelt es sich um eine Variante von GPT-3.5 Turbo, die auf instruktive Aufforderungen abgestimmt ist und chatbezogene Optimierungen auslässt. Trainingsdaten: bis September 2021."},"name":"GPT-3.5 Turbo Instruct","toolCalling":true,"knowledge":"2021-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","top_logprobs","top_p"],"providers":[{"providerId":"azure","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"1.50","output":"2.00"},"eur":{"currency":"eur","input":"1.29","output":"1.72"}}},{"providerId":"azure-cognitive-services","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"1.50","output":"2.00"},"eur":{"currency":"eur","input":"1.29","output":"1.72"}}},{"providerId":"poe","contextLength":3500,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"1.40","output":"1.80"},"eur":{"currency":"eur","input":"1.2","output":"1.55"}}},{"providerId":"openrouter","contextLength":4095,"price":{"usd":{"currency":"usd","input":"1.5","output":"2"},"eur":{"currency":"eur","input":"1.29","output":"1.72"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.250Z","outputLimit":1024,"contextLength":3500},{"id":"gpt-3.5-turbo-raw","aliases":["openai/gpt-3.5-turbo-raw","gpt-3.5-turbo-raw"],"name":"GPT-3.5-Turbo-Raw","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":4524,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.45","output":"1.40"},"eur":{"currency":"eur","input":"0.39","output":"1.2"}}}],"lastImportedAt":"2025-12-10T00:21:34.550Z","outputLimit":2048,"contextLength":4524},{"id":"gpt-4","aliases":["openai/gpt-4","gpt-4"],"description":{"en":"OpenAI's flagship model, GPT-4 is a large-scale multimodal language model capable of solving difficult problems with greater accuracy than previous models due to its broader general knowledge and advanced reasoning capabilities. Training data: up to Sep 2021.","de":"GPT-4, das Vorzeigemodell von OpenAI, ist ein groß angelegtes multimodales Sprachmodell, das aufgrund seines breiteren Allgemeinwissens und seiner fortgeschrittenen Schlussfolgerungsfähigkeiten schwierige Probleme mit größerer Genauigkeit als frühere Modelle lösen kann. Trainingsdaten: bis September 2021."},"name":"GPT-4","toolCalling":true,"knowledge":"2023-05-28","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"azure","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"60.00","output":"120.00"},"eur":{"currency":"eur","input":"51.53","output":"103.07"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"30.00","output":"60.00"},"eur":{"currency":"eur","input":"25.77","output":"51.53"}}},{"providerId":"openai","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"30.00","output":"60.00"},"eur":{"currency":"eur","input":"25.77","output":"51.53"}}},{"providerId":"azure-cognitive-services","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"60.00","output":"120.00"},"eur":{"currency":"eur","input":"51.53","output":"103.07"}}},{"providerId":"openrouter","contextLength":8191,"price":{"usd":{"currency":"usd","input":"30","output":"60"},"eur":{"currency":"eur","input":"25.77","output":"51.53"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:40.366Z","outputLimit":8192,"contextLength":8191},{"id":"gpt-4-0314","aliases":["openai/gpt-4-0314","gpt-4-0314"],"name":"OpenAI: GPT-4 (older v0314)","description":{"en":"GPT-4-0314 is the first version of GPT-4 released, with a context length of 8,192 tokens, and was supported until June 14. Training data: up to Sep 2021.","de":"GPT-4-0314 ist die erste veröffentlichte Version von GPT-4 mit einer Kontextlänge von 8.192 Token und wurde bis Juni 14 unterstützt. Trainingsdaten: bis September 2021."},"knowledge":"2023-05-28","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8191,"price":{"usd":{"currency":"usd","input":"30","output":"60"},"eur":{"currency":"eur","input":"25.881342","output":"51.762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.768Z"},{"id":"gpt-4-32k","aliases":["gpt-4-32k"],"name":"GPT-4 32K","toolCalling":true,"knowledge":"2023-11-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"azure","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"60.00","output":"120.00"},"eur":{"currency":"eur","input":"51.762684","output":"103.525368"}}},{"providerId":"azure-cognitive-services","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"60.00","output":"120.00"},"eur":{"currency":"eur","input":"51.762684","output":"103.525368"}}}],"lastImportedAt":"2025-11-19T12:06:32.726Z"},{"id":"gpt-4-classic","aliases":["openai/gpt-4-classic","gpt-4-classic"],"name":"GPT-4-Classic","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":8192,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"27.00","output":"54.00"},"eur":{"currency":"eur","input":"23.43","output":"46.86"}}}],"lastImportedAt":"2025-11-21T07:05:37.083Z","outputLimit":4096,"contextLength":8192},{"id":"gpt-4-classic-0314","aliases":["openai/gpt-4-classic-0314","gpt-4-classic-0314"],"name":"GPT-4-Classic-0314","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":8192,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"27.00","output":"54.00"},"eur":{"currency":"eur","input":"23.43","output":"46.86"}}}],"lastImportedAt":"2025-11-21T07:05:38.262Z","outputLimit":4096,"contextLength":8192},{"id":"gpt-4-turbo","aliases":["openai/gpt-4-turbo","gpt-4-turbo"],"description":{"en":"The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling. Training data: up to December 2023.","de":"Das neueste GPT-4 Turbo-Modell mit Bildverarbeitungsfunktionen. Vision-Anfragen können jetzt JSON-Modus und Funktionsaufrufe verwenden. Trainingsdaten: bis Dezember 2023."},"name":"GPT-4 Turbo","toolCalling":true,"knowledge":"2023-11-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"10.00","output":"30.00"},"eur":{"currency":"eur","input":"8.59","output":"25.77"}}},{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"10.00","output":"30.00"},"eur":{"currency":"eur","input":"8.59","output":"25.77"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"10.00","output":"30.00"},"eur":{"currency":"eur","input":"8.59","output":"25.77"}}},{"providerId":"openai","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"10.00","output":"30.00"},"eur":{"currency":"eur","input":"8.59","output":"25.77"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"10.00","output":"30.00"},"eur":{"currency":"eur","input":"8.59","output":"25.77"}}},{"providerId":"poe","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"9.00","output":"27.00"},"eur":{"currency":"eur","input":"7.73","output":"23.19"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"10","output":"30"},"eur":{"currency":"eur","input":"8.59","output":"25.77"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.957Z","outputLimit":4096,"contextLength":128000},{"id":"gpt-4-turbo-vision","aliases":["gpt-4-turbo-vision"],"name":"GPT-4 Turbo Vision","toolCalling":true,"knowledge":"2023-11-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"10.00","output":"30.00"},"eur":{"currency":"eur","input":"8.627114","output":"25.881342"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"10.00","output":"30.00"},"eur":{"currency":"eur","input":"8.627114","output":"25.881342"}}}],"lastImportedAt":"2025-11-19T12:06:32.727Z"},{"id":"gpt-4.1","aliases":["openai/gpt-4.1-2025-04-14","gpt-4.1-2025-04-14","openai/gpt-4.1","gpt-4.1"],"description":{"en":"GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.","de":"GPT-4.1 ist ein Flaggschiff unter den großen Sprachmodellen, das für fortgeschrittene Befehlsverfolgung, reales Software-Engineering und Long-Context-Reasoning optimiert ist. Es unterstützt ein Kontextfenster von 1 Million Token und übertrifft GPT-4o und GPT-4.5 in den Bereichen Codierung (54,6 % SWE-Bench Verified), Anweisungskonformität (87,4 % IFEval) und multimodales Verstehen von Benchmarks. Es ist auf präzise Code-Diffs, Agenten-Zuverlässigkeit und hohe Auffindbarkeit in großen Dokumentenkontexten abgestimmt, was es ideal für Agenten, IDE-Tools und Wissensabfragen in Unternehmen macht."},"name":"GPT-4.1","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","max_tokens","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"cortecs","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.35","output":"9.42"},"eur":{"currency":"eur","input":"2.02","output":"8.08"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"helicone","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"fastrouter","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"openai","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"openrouter","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"requesty","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"aihubmix","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"azure-cognitive-services","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"poe","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"1.80","output":"7.20"},"eur":{"currency":"eur","input":"1.54","output":"6.18"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.330Z","outputLimit":16384,"contextLength":128000},{"id":"gpt-4.1-mini","aliases":["openai/gpt-4.1-mini-2025-04-14","gpt-4.1-mini-2025-04-14","openai/gpt-4.1-mini","gpt-4.1-mini"],"description":{"en":"GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider’s polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints.","de":"GPT-4.1 Mini ist ein mittelgroßes Modell, das bei wesentlich geringerer Latenz und geringeren Kosten eine mit GPT-4o vergleichbare Leistung bietet. Es behält ein Kontextfenster von 1 Million Token bei und erzielt 45,1 % bei Hard Instruction Evals, 35,8 % bei MultiChallenge und 84,1 % bei IFEval. Mini zeigt auch starke Kodierfähigkeiten (z.B. 31,6 % bei Aiders polyglot diff benchmark) und ein gutes Verständnis für Visionen, was es für interaktive Anwendungen mit engen Leistungsbeschränkungen geeignet macht."},"name":"GPT-4.1 mini","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","max_tokens","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"vercel","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.60"},"eur":{"currency":"eur","input":"0.34","output":"1.37"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.60"},"eur":{"currency":"eur","input":"0.34","output":"1.37"}}},{"providerId":"helicone","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.60"},"eur":{"currency":"eur","input":"0.34","output":"1.37"}}},{"providerId":"openai","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.60"},"eur":{"currency":"eur","input":"0.34","output":"1.37"}}},{"providerId":"openrouter","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.60"},"eur":{"currency":"eur","input":"0.34","output":"1.37"}}},{"providerId":"requesty","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.60"},"eur":{"currency":"eur","input":"0.34","output":"1.37"}}},{"providerId":"aihubmix","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.60"},"eur":{"currency":"eur","input":"0.34","output":"1.37"}}},{"providerId":"azure-cognitive-services","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.60"},"eur":{"currency":"eur","input":"0.34","output":"1.37"}}},{"providerId":"poe","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.36","output":"1.40"},"eur":{"currency":"eur","input":"0.31","output":"1.2"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.338Z","outputLimit":16384,"contextLength":128000},{"id":"gpt-4.1-nano","aliases":["openai/gpt-4.1-nano-2025-04-14","gpt-4.1-nano-2025-04-14","openai/gpt-4.1-nano","gpt-4.1-nano"],"description":{"en":"For tasks that demand low latency, GPT‑4.1 nano is the fastest and cheapest model in the GPT-4.1 series. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding – even higher than GPT‑4o mini. It’s ideal for tasks like classification or autocompletion.","de":"Für Aufgaben, die eine niedrige Latenz erfordern, ist der GPT-4.1 nano das schnellste und günstigste Modell der GPT-4.1-Serie. Mit seinem 1-Millionen-Token-Kontextfenster liefert er außergewöhnliche Leistung bei geringer Größe und erzielt 80,1 % bei MMLU, 50,3 % bei GPQA und 9,8 % bei Aider Polyglot Coding - sogar mehr als der GPT-4o mini. Er ist ideal für Aufgaben wie Klassifizierung oder Autovervollständigung."},"name":"GPT-4.1 nano","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","max_tokens","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"vercel","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"helicone","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"openai","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"aihubmix","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"azure-cognitive-services","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}},{"providerId":"poe","contextLength":1047576,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.36"},"eur":{"currency":"eur","input":"0.08","output":"0.31"}}},{"providerId":"openrouter","contextLength":1047576,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.4"},"eur":{"currency":"eur","input":"0.09","output":"0.34"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.338Z","outputLimit":16384,"contextLength":128000},{"id":"gpt-4o","aliases":["openai/gpt-4o","gpt-4o"],"description":{"en":"GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities. For benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209) #multimodal","de":"GPT-4o (\"o\" für \"omni\") ist das neueste KI-Modell von OpenAI, das sowohl Text- als auch Bildeingaben mit Textausgaben unterstützt. Es behält das Intelligenzniveau von [GPT-4 Turbo](/models/openai/gpt-4-turbo) bei, ist aber doppelt so schnell und 50% kostengünstiger. GPT-4o bietet außerdem eine verbesserte Leistung bei der Verarbeitung nicht-englischer Sprachen und verbesserte visuelle Fähigkeiten. Für das Benchmarking mit anderen Modellen wurde es kurz [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209) #multimodal"},"name":"GPT-4o","toolCalling":true,"knowledge":"2023-09-01","input":["text","image","audio","file"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_logprobs","top_p","web_search_options"],"providers":[{"providerId":"github-copilot","contextLength":64000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.50","output":"10.00"},"eur":{"currency":"eur","input":"2.15","output":"8.58"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.50","output":"10.00"},"eur":{"currency":"eur","input":"2.15","output":"8.58"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.50","output":"10.00"},"eur":{"currency":"eur","input":"2.15","output":"8.58"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.50","output":"10.00"},"eur":{"currency":"eur","input":"2.15","output":"8.58"}}},{"providerId":"openai","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.50","output":"10.00"},"eur":{"currency":"eur","input":"2.15","output":"8.58"}}},{"providerId":"aihubmix","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.50","output":"10.00"},"eur":{"currency":"eur","input":"2.15","output":"8.58"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.50","output":"10.00"},"eur":{"currency":"eur","input":"2.15","output":"8.58"}}},{"providerId":"poe","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"2.5","output":"10"},"eur":{"currency":"eur","input":"2.15","output":"8.58"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.330Z","outputLimit":8192,"contextLength":64000},{"id":"gpt-4o-aug","aliases":["openai/gpt-4o-aug","gpt-4o-aug"],"name":"GPT-4o-Aug","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.20","output":"9.00"},"eur":{"currency":"eur","input":"1.89","output":"7.73"}}}],"lastImportedAt":"2025-12-10T00:21:34.632Z","outputLimit":8192,"contextLength":128000},{"id":"gpt-4o-mini","aliases":["openai/gpt-4o-mini","gpt-4o-mini"],"description":{"en":"GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs. As their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective. GPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/). Check out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more. #multimodal","de":"GPT-4o mini ist das neueste Modell von OpenAI nach [GPT-4 Omni](/models/openai/gpt-4o) und unterstützt sowohl Text- als auch Bildeingaben mit Textausgaben. Als ihr fortschrittlichstes kleines Modell ist es um ein Vielfaches günstiger als andere aktuelle Frontier-Modelle und mehr als 60 % billiger als [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). Es behält die SOTA-Intelligenz bei, ist aber deutlich kostengünstiger. GPT-4o mini erreicht eine 82%ige Punktzahl bei MMLU und rangiert derzeit höher als GPT-4 bei den Chat-Einstellungen [common leaderboards](https://arena.lmsys.org/). Lesen Sie die [Launch-Ankündigung](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/), um mehr zu erfahren. #multimodal"},"name":"GPT-4o mini","toolCalling":true,"knowledge":"2023-09-01","input":["text","image","audio","file"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_logprobs","top_p","web_search_options"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openai","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"openrouter","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"requesty","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"poe","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.54"},"eur":{"currency":"eur","input":"0.12","output":"0.46"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.336Z","outputLimit":4096,"contextLength":128000},{"id":"gpt-4o-mini-search","aliases":["openai/gpt-4o-mini-search","gpt-4o-mini-search"],"name":"GPT-4o-mini-Search","toolCalling":true,"input":["text"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.54"},"eur":{"currency":"eur","input":"0.12","output":"0.47"}}}],"lastImportedAt":"2025-11-21T07:05:38.954Z","outputLimit":8192,"contextLength":128000},{"id":"gpt-4o-search","aliases":["openai/gpt-4o-search","gpt-4o-search"],"name":"GPT-4o-Search","toolCalling":true,"input":["text"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.20","output":"9.00"},"eur":{"currency":"eur","input":"1.89","output":"7.73"}}}],"lastImportedAt":"2025-12-10T00:21:34.591Z","outputLimit":8192,"contextLength":128000},{"id":"gpt-4o:extended","aliases":["openai/gpt-4o:extended","gpt-4o:extended","openai/gpt-4o","gpt-4o"],"name":"OpenAI: GPT-4o (extended)","description":{"en":"GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities. For benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209) #multimodal","de":"GPT-4o (\"o\" für \"omni\") ist das neueste KI-Modell von OpenAI, das sowohl Text- als auch Bildeingaben mit Textausgaben unterstützt. Es behält das Intelligenzniveau von [GPT-4 Turbo](/models/openai/gpt-4-turbo) bei, ist aber doppelt so schnell und 50% kostengünstiger. GPT-4o bietet außerdem eine verbesserte Leistung bei der Verarbeitung nicht-englischer Sprachen und verbesserte visuelle Fähigkeiten. Für das Benchmarking mit anderen Modellen wurde es kurz [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209) #multimodal"},"knowledge":"2024-05-13","toolCalling":true,"input":["text","image","file"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_logprobs","top_p","web_search_options"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"6","output":"18"},"eur":{"currency":"eur","input":"5.1762684","output":"15.5288052"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"gpt-5","aliases":["openai/gpt-5-2025-08-07","gpt-5-2025-08-07","openai/gpt-5","gpt-5"],"description":{"en":"GPT-5 is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.","de":"GPT-5 ist das fortschrittlichste Modell von OpenAI und bietet erhebliche Verbesserungen in Bezug auf Denkfähigkeit, Codequalität und Benutzerfreundlichkeit. Es ist für komplexe Aufgaben optimiert, die schrittweises Denken, das Befolgen von Anweisungen und Genauigkeit in anspruchsvollen Anwendungsfällen erfordern. Sie unterstützt Funktionen für das Routing zur Testzeit und ein erweitertes Verständnis von Eingabeaufforderungen, einschließlich benutzerspezifischer Absichten wie z. B. \"Denken Sie gründlich darüber nach\". Zu den Verbesserungen gehören die Verringerung von Halluzinationen und Schleimerei sowie eine bessere Leistung beim Programmieren, Schreiben und bei gesundheitsbezogenen Aufgaben."},"name":"GPT-5","reasoning":true,"toolCalling":true,"knowledge":"2024-09-30","input":["text","image","audio","video","file"],"output":["text","image"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"azure","contextLength":272000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"helicone","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"opencode","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.07","output":"8.50"},"eur":{"currency":"eur","input":"0.92","output":"7.29"}}},{"providerId":"fastrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"zenmux","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"requesty","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"sap-ai-core","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"aihubmix","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"5.00","output":"20.00"},"eur":{"currency":"eur","input":"4.29","output":"17.16"}}},{"providerId":"azure-cognitive-services","contextLength":272000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"poe","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.10","output":"9.00"},"eur":{"currency":"eur","input":"0.94","output":"7.72"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.334Z","outputLimit":128000,"contextLength":128000},{"id":"gpt-5-chat","aliases":["openai/gpt-5-chat-2025-08-07","gpt-5-chat-2025-08-07","openai/gpt-5-chat","gpt-5-chat"],"description":{"en":"GPT-5 Chat is designed for advanced, natural, multimodal, and context-aware conversations for enterprise applications.","de":"GPT-5 Chat wurde für fortschrittliche, natürliche, multimodale und kontextabhängige Unterhaltungen für Unternehmensanwendungen entwickelt."},"name":"GPT-5 Chat","reasoning":true,"toolCalling":true,"knowledge":"2024-09-30","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","max_tokens","response_format","seed","structured_outputs"],"providers":[{"providerId":"azure","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"openrouter","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"poe","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.10","output":"9.00"},"eur":{"currency":"eur","input":"0.94","output":"7.73"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:40.366Z","outputLimit":16384,"contextLength":128000},{"id":"gpt-5-codex","aliases":["openai/gpt-5-codex","gpt-5-codex"],"description":{"en":"GPT-5-Codex is a specialized version of GPT-5 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level) Codex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.","de":"GPT-5-Codex ist eine spezialisierte Version von GPT-5, die für die Software-Entwicklung und Codierungs-Workflows optimiert ist. Es ist sowohl für interaktive Entwicklungssitzungen als auch für die lange, unabhängige Ausführung komplexer technischer Aufgaben konzipiert. Das Modell unterstützt die Erstellung von Projekten von Grund auf, die Entwicklung von Funktionen, das Debugging, das Refactoring in großem Maßstab und die Codeüberprüfung. Im Vergleich zu GPT-5 ist Codex besser steuerbar, hält sich genau an die Anweisungen des Entwicklers und erzeugt sauberere und qualitativ hochwertigere Codeausgaben. Der Reasoning-Aufwand kann mit dem Parameter `reasoning.effort` angepasst werden. Lesen Sie die [docs hier](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level) Codex lässt sich in Entwicklerumgebungen integrieren, darunter die CLI, IDE-Erweiterungen, GitHub und Cloud-Tasks. Es passt den Reasoning-Aufwand dynamisch an und liefert schnelle Antworten für kleine Aufgaben, während es für große Projekte mehrere Stunden lang läuft. Das Modell ist darauf trainiert, strukturierte Code-Reviews durchzuführen und kritische Fehler zu erkennen, indem es Abhängigkeiten analysiert und das Verhalten anhand von Tests validiert. Es unterstützt auch multimodale Eingaben wie Bilder oder Screenshots für die UI-Entwicklung und integriert die Verwendung von Tools für die Suche, die Installation von Abhängigkeiten und die Einrichtung der Umgebung. Codex ist speziell für agentenbasierte Kodierungsanwendungen gedacht."},"name":"GPT-5-Codex","reasoning":true,"toolCalling":true,"knowledge":"2024-09-30","input":["text","image"],"output":["text"],"parameters":["tools","temperature","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"azure","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"helicone","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"opencode","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.07","output":"8.50"},"eur":{"currency":"eur","input":"0.92","output":"7.29"}}},{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"zenmux","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"aihubmix","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"azure-cognitive-services","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.58"}}},{"providerId":"poe","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.10","output":"9.00"},"eur":{"currency":"eur","input":"0.94","output":"7.72"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.329Z","outputLimit":128000,"contextLength":128000},{"id":"gpt-5-image","aliases":["openai/gpt-5-image","gpt-5-image"],"description":{"en":"[GPT-5](https://openrouter.ai/openai/gpt-5) Image combines OpenAI's most advanced language model with state-of-the-art image generation capabilities. It offers major improvements in reasoning, code quality, and user experience while incorporating GPT Image 1's superior instruction following, text rendering, and detailed image editing.","de":"[GPT-5](https://openrouter.ai/openai/gpt-5) Image kombiniert das fortschrittlichste Sprachmodell von OpenAI mit modernsten Bildgenerierungsfunktionen. Es bietet erhebliche Verbesserungen bei der Argumentation, der Codequalität und der Benutzerfreundlichkeit, während es die überlegene Befehlsverfolgung, das Textrendering und die detaillierte Bildbearbeitung von GPT Image 1 übernimmt."},"name":"GPT-5 Image","reasoning":true,"toolCalling":true,"knowledge":"2024-10-01","input":["text","image","file"],"output":["text","image"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","presence_penalty","reasoning","response_format","seed","stop","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"openrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"10","output":"10.00"},"eur":{"currency":"eur","input":"8.59","output":"8.59"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.503Z","outputLimit":128000,"contextLength":400000},{"id":"gpt-5-image-mini","aliases":["openai/gpt-5-image-mini","gpt-5-image-mini"],"name":"OpenAI: GPT-5 Image Mini","description":{"en":"GPT-5 Image Mini combines OpenAI's advanced language capabilities, powered by [GPT-5 Mini](https://openrouter.ai/openai/gpt-5-mini), with GPT Image 1 Mini for efficient image generation. This natively multimodal model features superior instruction following, text rendering, and detailed image editing with reduced latency and cost. It excels at high-quality visual creation while maintaining strong text understanding, making it ideal for applications that require both efficient image generation and text processing at scale.","de":"GPT-5 Image Mini kombiniert die fortschrittlichen Sprachfähigkeiten von OpenAI, die von [GPT-5 Mini] (https://openrouter.ai/openai/gpt-5-mini) unterstützt werden, mit GPT Image 1 Mini für effiziente Bilderzeugung. Dieses nativ multimodale Modell bietet überlegene Befehlsverfolgung, Textwiedergabe und detaillierte Bildbearbeitung mit reduzierten Latenzzeiten und Kosten. Es zeichnet sich durch qualitativ hochwertige Bilderzeugung bei gleichzeitiger Beibehaltung eines guten Textverständnisses aus und ist damit ideal für Anwendungen, die sowohl eine effiziente Bilderzeugung als auch eine Textverarbeitung in großem Umfang erfordern."},"knowledge":"2025-10-16","reasoning":true,"toolCalling":true,"input":["file","image","text"],"output":["image","text"],"parameters":["frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","presence_penalty","reasoning","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":400000,"price":{"usd":{"currency":"usd","input":"2.5","output":"2"},"eur":{"currency":"eur","input":"2.1567785","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.762Z"},{"id":"gpt-5-mini","aliases":["openai/gpt-5-mini-2025-08-07","gpt-5-mini-2025-08-07","openai/gpt-5-mini","gpt-5-mini"],"description":{"en":"GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It provides the same instruction-following and safety-tuning benefits as GPT-5, but with reduced latency and cost. GPT-5 Mini is the successor to OpenAI's o4-mini model.","de":"GPT-5 Mini ist eine kompakte Version von GPT-5, die für die Bewältigung leichterer Rechenaufgaben entwickelt wurde. Es bietet dieselben Vorteile wie GPT-5 in Bezug auf die Befolgung von Anweisungen und die Sicherheitsabstimmung, jedoch mit geringerer Latenzzeit und geringeren Kosten. GPT-5 Mini ist der Nachfolger von OpenAIs o4-mini-Modell."},"name":"GPT-5-mini","reasoning":true,"toolCalling":true,"knowledge":"2024-05-30","input":["text","image","file"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"azure","contextLength":272000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"helicone","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"fastrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"requesty","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"sap-ai-core","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"aihubmix","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"1.50","output":"6.00"},"eur":{"currency":"eur","input":"1.29","output":"5.15"}}},{"providerId":"azure-cognitive-services","contextLength":272000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"poe","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.22","output":"1.80"},"eur":{"currency":"eur","input":"0.19","output":"1.54"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.332Z","outputLimit":32000,"contextLength":128000},{"id":"gpt-5-nano","aliases":["openai/gpt-5-nano-2025-08-07","gpt-5-nano-2025-08-07","openai/gpt-5-nano","gpt-5-nano"],"description":{"en":"GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.","de":"GPT-5-Nano ist die kleinste und schnellste Variante des GPT-5-Systems, die für Entwicklerwerkzeuge, schnelle Interaktionen und Umgebungen mit extrem niedriger Latenz optimiert ist. Obwohl die Argumentationstiefe im Vergleich zu seinen größeren Pendants begrenzt ist, verfügt er über wichtige Befehlsfolge- und Sicherheitsfunktionen. Er ist der Nachfolger von GPT-4.1-nano und bietet eine leichtgewichtige Option für kostensensitive oder Echtzeitanwendungen."},"name":"GPT-5 Nano","reasoning":true,"toolCalling":true,"knowledge":"2024-05-30","input":["text","image","file"],"output":["text"],"parameters":["tools","temperature","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"vercel","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04","output":"0.34"}}},{"providerId":"azure","contextLength":272000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04","output":"0.34"}}},{"providerId":"helicone","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04","output":"0.34"}}},{"providerId":"opencode","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"fastrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04","output":"0.34"}}},{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04","output":"0.34"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04","output":"0.34"}}},{"providerId":"requesty","contextLength":16000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04","output":"0.34"}}},{"providerId":"sap-ai-core","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04","output":"0.34"}}},{"providerId":"aihubmix","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.43","output":"1.72"}}},{"providerId":"azure-cognitive-services","contextLength":272000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04","output":"0.34"}}},{"providerId":"poe","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.36"},"eur":{"currency":"eur","input":"0.03","output":"0.31"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.337Z","outputLimit":4000,"contextLength":16000},{"id":"gpt-5-pro","aliases":["openai/gpt-5-pro-2025-10-06","gpt-5-pro-2025-10-06","openai/gpt-5-pro","gpt-5-pro"],"description":{"en":"GPT-5 Pro is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.","de":"GPT-5 Pro ist das fortschrittlichste Modell von OpenAI und bietet erhebliche Verbesserungen in Bezug auf Denkfähigkeit, Codequalität und Benutzerfreundlichkeit. Es ist für komplexe Aufgaben optimiert, die schrittweises Denken, das Befolgen von Anweisungen und Genauigkeit in anspruchsvollen Anwendungsfällen erfordern. Sie unterstützt Funktionen für das Routing zur Testzeit und ein erweitertes Verständnis von Eingabeaufforderungen, einschließlich benutzerspezifischer Absichten wie z. B. \"Denken Sie gründlich darüber nach\". Zu den Verbesserungen gehören die Verringerung von Halluzinationen und Schleimerei sowie eine bessere Leistung beim Programmieren, Schreiben und bei gesundheitsbezogenen Aufgaben."},"name":"GPT-5 Pro","reasoning":true,"toolCalling":true,"knowledge":"2024-09-30","input":["text","image","file"],"output":["text"],"parameters":["tools","temperature","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"azure","contextLength":400000,"outputLimit":272000,"price":{"usd":{"currency":"usd","input":"15.00","output":"120.00"},"eur":{"currency":"eur","input":"12.88","output":"103.03"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"15.00","output":"120.00"},"eur":{"currency":"eur","input":"12.88","output":"103.03"}}},{"providerId":"openai","contextLength":400000,"outputLimit":272000,"price":{"usd":{"currency":"usd","input":"15.00","output":"120.00"},"eur":{"currency":"eur","input":"12.88","output":"103.03"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":272000,"price":{"usd":{"currency":"usd","input":"15.00","output":"120.00"},"eur":{"currency":"eur","input":"12.88","output":"103.03"}}},{"providerId":"aihubmix","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"7.00","output":"28.00"},"eur":{"currency":"eur","input":"6.01","output":"24.04"}}},{"providerId":"azure-cognitive-services","contextLength":400000,"outputLimit":272000,"price":{"usd":{"currency":"usd","input":"15.00","output":"120.00"},"eur":{"currency":"eur","input":"12.88","output":"103.03"}}},{"providerId":"poe","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"14.00","output":"110.00"},"eur":{"currency":"eur","input":"12.02","output":"94.44"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.250Z","outputLimit":32768,"contextLength":128000},{"id":"gpt-5.1","aliases":["openai/gpt-5.1-20251113","gpt-5.1-20251113","openai/gpt-5.1","gpt-5.1"],"description":{"en":"GPT-5.1 is the latest frontier-grade model in the GPT-5 series, offering stronger general-purpose reasoning, improved instruction adherence, and a more natural conversational style compared to GPT-5. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks. The model produces clearer, more grounded explanations with reduced jargon, making it easier to follow even on technical or multi-step problems. Built for broad task coverage, GPT-5.1 delivers consistent gains across math, coding, and structured analysis workloads, with more coherent long-form answers and improved tool-use reliability. It also features refined conversational alignment, enabling warmer, more intuitive responses without compromising precision. GPT-5.1 serves as the primary full-capability successor to GPT-5","de":"GPT-5.1 ist das neueste Spitzenmodell der GPT-5-Reihe und bietet im Vergleich zu GPT-5 ein stärkeres Allzweckdenken, eine verbesserte Befolgung von Anweisungen und einen natürlicheren Konversationsstil. Es nutzt adaptives Denken, um Berechnungen dynamisch zuzuweisen, und reagiert schnell auf einfache Anfragen, während es sich komplexen Aufgaben widmet. Das Modell liefert klarere, fundiertere Erklärungen mit weniger Fachjargon, so dass es selbst bei technischen oder mehrstufigen Problemen leichter zu verstehen ist. GPT-5.1 wurde für eine breite Aufgabenabdeckung entwickelt und bietet konsistente Verbesserungen bei Mathematik-, Codierungs- und strukturierten Analyseaufgaben, mit kohärenteren Langform-Antworten und verbesserter Zuverlässigkeit bei der Nutzung des Tools. Außerdem bietet es eine verfeinerte Konversationsanpassung, die wärmere, intuitivere Antworten ermöglicht, ohne die Präzision zu beeinträchtigen. GPT-5.1 ist der primäre, voll funktionsfähige Nachfolger von GPT-5"},"name":"GPT-5.1","reasoning":true,"toolCalling":true,"knowledge":"2024-09-30","input":["text","image","audio","file"],"output":["text","image"],"parameters":["tools","temperature","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":272000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"helicone","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"opencode","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.07","output":"8.50"},"eur":{"currency":"eur","input":"0.92","output":"7.3"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"aihubmix","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"azure-cognitive-services","contextLength":272000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"poe","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.10","output":"9.00"},"eur":{"currency":"eur","input":"0.94","output":"7.73"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-11T00:21:45.980Z","outputLimit":16384,"contextLength":128000},{"id":"gpt-5.1-chat","aliases":["openai/gpt-5.1-chat-20251113","gpt-5.1-chat-20251113","openai/gpt-5.1-chat","gpt-5.1-chat"],"description":{"en":"GPT-5.1 Chat (AKA Instant is the fast, lightweight member of the 5.1 family, optimized for low-latency chat while retaining strong general intelligence. It uses adaptive reasoning to selectively “think” on harder queries, improving accuracy on math, coding, and multi-step tasks without slowing down typical conversations. The model is warmer and more conversational by default, with better instruction following and more stable short-form reasoning. GPT-5.1 Chat is designed for high-throughput, interactive workloads where responsiveness and consistency matter more than deep deliberation.","de":"GPT-5.1 Chat (AKA Instant) ist das schnelle, leichtgewichtige Mitglied der 5.1-Familie, das für Chats mit geringer Latenzzeit optimiert ist und gleichzeitig eine starke allgemeine Intelligenz aufweist. Es nutzt adaptives Denken, um bei schwierigeren Anfragen selektiv \"mitzudenken\" und verbessert die Genauigkeit bei Mathematik, Codierung und mehrstufigen Aufgaben, ohne typische Unterhaltungen zu verlangsamen. Das Modell ist standardmäßig wärmer und konversationsfreudiger, mit besserer Befolgung von Anweisungen und stabilerem Kurzschlussdenken. GPT-5.1 Chat wurde für interaktive Arbeitslasten mit hohem Durchsatz entwickelt, bei denen Reaktionsfähigkeit und Konsistenz wichtiger sind als tiefgreifende Überlegungen."},"name":"GPT-5.1 Chat","reasoning":true,"toolCalling":true,"knowledge":"2024-09-30","input":["text","image","audio","file"],"output":["text","image"],"parameters":["tools","temperature","max_tokens","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"azure","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"openrouter","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-11T00:21:45.993Z","outputLimit":16384,"contextLength":128000},{"id":"gpt-5.1-codex","aliases":["openai/gpt-5.1-codex-20251113","gpt-5.1-codex-20251113","openai/gpt-5.1-codex","gpt-5.1-codex"],"description":{"en":"GPT-5.1-Codex is a specialized version of GPT-5.1 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5.1, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level) Codex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.","de":"GPT-5.1-Codex ist eine spezialisierte Version von GPT-5.1, die für die Software-Entwicklung und Codierungs-Workflows optimiert ist. Es ist sowohl für interaktive Entwicklungssitzungen als auch für die lange, unabhängige Ausführung komplexer technischer Aufgaben konzipiert. Das Modell unterstützt die Erstellung von Projekten von Grund auf, die Entwicklung von Funktionen, das Debugging, das Refactoring in großem Maßstab und die Codeüberprüfung. Im Vergleich zu GPT-5.1 ist Codex besser steuerbar, hält sich genau an die Anweisungen des Entwicklers und erzeugt sauberere und qualitativ hochwertigere Codeausgaben. Der Reasoning-Aufwand kann mit dem Parameter `reasoning.effort` angepasst werden. Lesen Sie die [docs hier](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level) Codex lässt sich in Entwicklerumgebungen integrieren, darunter die CLI, IDE-Erweiterungen, GitHub und Cloud-Tasks. Es passt den Reasoning-Aufwand dynamisch an und liefert schnelle Antworten für kleine Aufgaben, während es für große Projekte mehrere Stunden lang läuft. Das Modell ist darauf trainiert, strukturierte Code-Reviews durchzuführen und kritische Fehler zu erkennen, indem es Abhängigkeiten analysiert und das Verhalten anhand von Tests validiert. Es unterstützt auch multimodale Eingaben wie Bilder oder Screenshots für die UI-Entwicklung und integriert die Verwendung von Tools für die Suche, die Installation von Abhängigkeiten und die Einrichtung der Umgebung. Codex ist speziell für agentenbasierte Kodierungsanwendungen gedacht."},"name":"GPT-5.1-Codex","reasoning":true,"toolCalling":true,"knowledge":"2024-09-30","input":["text","image","audio"],"output":["text","image"],"parameters":["tools","temperature","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"helicone","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"opencode","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.07","output":"8.50"},"eur":{"currency":"eur","input":"0.92","output":"7.3"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"aihubmix","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"azure-cognitive-services","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"poe","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.10","output":"9.00"},"eur":{"currency":"eur","input":"0.94","output":"7.73"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-11T00:21:45.979Z","outputLimit":16384,"contextLength":128000},{"id":"gpt-5.1-codex-max","aliases":["openai/gpt-5.1-codex-max-20251204","gpt-5.1-codex-max-20251204","openai/gpt-5.1-codex-max","gpt-5.1-codex-max"],"description":{"en":"GPT-5.1-Codex-Max is OpenAI’s latest agentic coding model, designed for long-running, high-context software development tasks. It is based on an updated version of the 5.1 reasoning stack and trained on agentic workflows spanning software engineering, mathematics, and research. GPT-5.1-Codex-Max delivers faster performance, improved reasoning, and higher token efficiency across the development lifecycle.","de":"GPT-5.1-Codex-Max ist OpenAIs neuestes agentenbasiertes Kodierungsmodell, das für langwierige, kontextintensive Softwareentwicklungsaufgaben entwickelt wurde. Es basiert auf einer aktualisierten Version des 5.1-Reasoning-Stacks und ist auf agentenbasierte Arbeitsabläufe in den Bereichen Softwareentwicklung, Mathematik und Forschung trainiert. GPT-5.1-Codex-Max bietet schnellere Leistung, verbessertes Reasoning und höhere Token-Effizienz während des gesamten Entwicklungszyklus."},"name":"GPT-5.1-Codex-max","reasoning":true,"toolCalling":true,"knowledge":"2024-09-30","input":["text","image"],"output":["text"],"parameters":["tools","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"opencode","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10.00"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}},{"providerId":"openrouter","contextLength":400000,"price":{"usd":{"currency":"usd","input":"1.25","output":"10"},"eur":{"currency":"eur","input":"1.07","output":"8.59"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-11T00:21:45.982Z","outputLimit":128000,"contextLength":128000},{"id":"gpt-5.1-codex-mini","aliases":["openai/gpt-5.1-codex-mini-20251113","gpt-5.1-codex-mini-20251113","openai/gpt-5.1-codex-mini","gpt-5.1-codex-mini"],"description":{"en":"GPT-5.1-Codex-Mini is a smaller and faster version of GPT-5.1-Codex","de":"GPT-5.1-Codex-Mini ist eine kleinere und schnellere Version von GPT-5.1-Codex"},"name":"GPT-5.1-Codex-mini","reasoning":true,"toolCalling":true,"knowledge":"2024-09-30","input":["text","image","audio"],"output":["text","image"],"parameters":["tools","temperature","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"helicone","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"aihubmix","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"azure-cognitive-services","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"2.00"},"eur":{"currency":"eur","input":"0.21","output":"1.72"}}},{"providerId":"poe","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.22","output":"1.80"},"eur":{"currency":"eur","input":"0.19","output":"1.55"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-11T00:21:45.980Z","outputLimit":100000,"contextLength":128000},{"id":"gpt-5.1-instant","aliases":["openai/gpt-5.1-instant","gpt-5.1-instant"],"name":"GPT-5.1-Instant","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.10","output":"9.00"},"eur":{"currency":"eur","input":"0.95","output":"7.81"}}}],"lastImportedAt":"2025-11-21T07:05:42.833Z","outputLimit":16384,"contextLength":128000},{"id":"gpt-5.2","aliases":["openai/gpt-5.2-20251211","gpt-5.2-20251211","openai/gpt-5.2","gpt-5.2"],"description":{"en":"GPT-5.2 is the latest frontier-grade model in the GPT-5 series, offering stronger agentic and long context perfomance compared to GPT-5.1. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks. Built for broad task coverage, GPT-5.2 delivers consistent gains across math, coding, sciende, and tool calling workloads, with more coherent long-form answers and improved tool-use reliability.","de":"GPT-5.2 ist das neueste Spitzenmodell der GPT-5-Reihe und bietet im Vergleich zu GPT-5.1 eine stärkere Agenten- und Langzeitkontextleistung. Er nutzt adaptive Argumentation, um Berechnungen dynamisch zuzuweisen, und reagiert schnell auf einfache Anfragen, während er komplexen Aufgaben mehr Tiefe widmet. GPT-5.2 wurde für eine breite Aufgabenabdeckung entwickelt und bietet konsistente Verbesserungen in den Bereichen Mathematik, Kodierung, Wissenschaft und Werkzeugaufrufe, mit kohärenteren Antworten in langer Form und verbesserter Zuverlässigkeit bei der Werkzeugnutzung."},"name":"GPT-5.2","reasoning":true,"toolCalling":true,"knowledge":"2025-08-31","input":["text","image","file"],"output":["text"],"parameters":["tools","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.75","output":"14.00"},"eur":{"currency":"eur","input":"1.5","output":"11.98"}}},{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.75","output":"14.00"},"eur":{"currency":"eur","input":"1.5","output":"11.98"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"1.75","output":"14.00"},"eur":{"currency":"eur","input":"1.5","output":"11.98"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T12:08:56.208Z","outputLimit":64000,"contextLength":128000},{"id":"gpt-5.2-chat","aliases":["openai/gpt-5.2-chat-20251211","gpt-5.2-chat-20251211","openai/gpt-5.2-chat","gpt-5.2-chat"],"name":"OpenAI: GPT-5.2 Chat","description":{"en":"GPT-5.2 Chat (AKA Instant) is the fast, lightweight member of the 5.2 family, optimized for low-latency chat while retaining strong general intelligence. It uses adaptive reasoning to selectively “think” on harder queries, improving accuracy on math, coding, and multi-step tasks without slowing down typical conversations. The model is warmer and more conversational by default, with better instruction following and more stable short-form reasoning. GPT-5.2 Chat is designed for high-throughput, interactive workloads where responsiveness and consistency matter more than deep deliberation.","de":"GPT-5.2 Chat (AKA Instant) ist das schnelle, leichtgewichtige Mitglied der 5.2-Familie, das für Chats mit geringer Latenz optimiert ist und gleichzeitig eine starke allgemeine Intelligenz aufweist. Es nutzt adaptives Denken, um bei schwierigeren Anfragen selektiv \"mitzudenken\" und die Genauigkeit bei Mathematik, Codierung und mehrstufigen Aufgaben zu verbessern, ohne typische Unterhaltungen zu verlangsamen. Das Modell ist standardmäßig wärmer und konversationsfreudiger, mit besserer Befolgung von Anweisungen und stabilerem Kurzschlussdenken. GPT-5.2 Chat wurde für interaktive Arbeitslasten mit hohem Durchsatz entwickelt, bei denen Reaktionsfähigkeit und Konsistenz wichtiger sind als tiefes Nachdenken."},"knowledge":"2025-12-10","toolCalling":true,"input":["file","image","text"],"output":["text"],"parameters":["max_tokens","response_format","seed","structured_outputs","tool_choice","tools"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"1.75","output":"14"},"eur":{"currency":"eur","input":"1.5","output":"11.98"}}}],"lastImportedAt":"2025-12-12T00:21:33.761Z","contextLength":128000},{"id":"gpt-5.2-pro","aliases":["openai/gpt-5.2-pro-20251211","gpt-5.2-pro-20251211","openai/gpt-5.2-pro","gpt-5.2-pro"],"description":{"en":"GPT-5.2 Pro is OpenAI’s most advanced model, offering major improvements in agentic coding and long context performance over GPT-5 Pro. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.","de":"GPT-5.2 Pro ist das fortschrittlichste Modell von OpenAI und bietet im Vergleich zu GPT-5 Pro erhebliche Verbesserungen bei der agentenbasierten Kodierung und der Leistung bei langem Kontext. Es ist für komplexe Aufgaben optimiert, die schrittweises Denken, Befolgen von Anweisungen und Genauigkeit in anspruchsvollen Anwendungsfällen erfordern. Es unterstützt Test-Time-Routing-Funktionen und fortgeschrittenes Prompt-Verständnis, einschließlich benutzerspezifischer Intentionen wie \"denke intensiv darüber nach\". Zu den Verbesserungen gehören die Verringerung von Halluzinationen und Schleimerei sowie eine bessere Leistung beim Programmieren, Schreiben und bei gesundheitsbezogenen Aufgaben."},"name":"GPT-5.2 Pro","reasoning":true,"toolCalling":true,"knowledge":"2025-08-31","input":["text","image","file"],"output":["text"],"parameters":["tools","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"openai","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"21.00","output":"168.00"},"eur":{"currency":"eur","input":"17.97","output":"143.75"}}},{"providerId":"openrouter","contextLength":400000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"21.00","output":"168.00"},"eur":{"currency":"eur","input":"17.97","output":"143.75"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T00:21:33.755Z","outputLimit":128000,"contextLength":400000},{"id":"gpt-image-1","aliases":["openai/gpt-image-1","gpt-image-1"],"name":"GPT-Image-1","toolCalling":true,"input":["text","image"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":128000,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:38.039Z","contextLength":128000},{"id":"gpt-image-1-mini","aliases":["openai/gpt-image-1-mini","gpt-image-1-mini"],"name":"GPT-Image-1-Mini","toolCalling":true,"input":["text","image"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:37.547Z"},{"id":"gpt-oss-120b","aliases":["accounts/fireworks/models/gpt-oss-120b","openai/gpt-oss-120b:free","workers-ai/gpt-oss-120b","hf:openai/gpt-oss-120b","openai/gpt-oss-120b","gpt-oss-120b:free","gpt-oss-120b"],"description":{"en":"gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.","de":"gpt-oss-120b ist ein offenes, 117B-Parameter-Mixture-of-Experts (MoE)-Sprachmodell von OpenAI, das für High-Reasoning-, Agenten- und allgemeine Produktionsanwendungsfälle entwickelt wurde. Es aktiviert 5,1B Parameter pro Vorwärtsdurchlauf und ist für die Ausführung auf einer einzelnen H100 GPU mit nativer MXFP4-Quantisierung optimiert. Das Modell unterstützt eine konfigurierbare Argumentationstiefe, vollen Chain-of-Thought-Zugriff und die Verwendung nativer Tools, einschließlich Funktionsaufrufe, Browsing und strukturierte Ausgabegenerierung."},"name":"GPT OSS 120B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","seed","stop","tool_choice","frequency_penalty","logit_bias","logprobs","min_p","presence_penalty","repetition_penalty","response_format","structured_outputs","top_k","top_logprobs","top_p"],"providers":[{"providerId":"vultr","contextLength":121808,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"nvidia","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"groq","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.75"},"eur":{"currency":"eur","input":"0.13","output":"0.64"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.50"},"eur":{"currency":"eur","input":"0.09","output":"0.43"}}},{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.40"},"eur":{"currency":"eur","input":"0.03","output":"0.34"}}},{"providerId":"cortecs","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"togetherai","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.16"},"eur":{"currency":"eur","input":"0.03","output":"0.14"}}},{"providerId":"fastrouter","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"cloudflare-workers-ai","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.35","output":"0.75"},"eur":{"currency":"eur","input":"0.3","output":"0.64"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.28"},"eur":{"currency":"eur","input":"0.06","output":"0.24"}}},{"providerId":"ovhcloud","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.47"},"eur":{"currency":"eur","input":"0.08","output":"0.4"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.10"},"eur":{"currency":"eur","input":"0.09","output":"0.09"}}},{"providerId":"deepinfra","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.24"},"eur":{"currency":"eur","input":"0.04","output":"0.21"}}},{"providerId":"submodel","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.50"},"eur":{"currency":"eur","input":"0.09","output":"0.43"}}},{"providerId":"fireworks-ai","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"io-net","contextLength":131072,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.40"},"eur":{"currency":"eur","input":"0.03","output":"0.34"}}},{"providerId":"scaleway","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}},{"providerId":"cerebras","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.25","output":"0.69"},"eur":{"currency":"eur","input":"0.21","output":"0.59"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":131072,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.278Z","outputLimit":4096,"contextLength":121808},{"id":"gpt-oss-120b-maas","aliases":["openai/gpt-oss-120b-maas","gpt-oss-120b-maas"],"name":"GPT OSS 120B","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"google-vertex","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.36"},"eur":{"currency":"eur","input":"0.08","output":"0.31"}}}],"lastImportedAt":"2025-12-10T00:21:34.359Z","outputLimit":32768,"contextLength":131072},{"id":"gpt-oss-120b:exacto","aliases":["openai/gpt-oss-120b:exacto","gpt-oss-120b:exacto","openai/gpt-oss-120b","gpt-oss-120b"],"description":{"en":"gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.","de":"gpt-oss-120b ist ein offenes, 117B-Parameter-Mixture-of-Experts (MoE)-Sprachmodell von OpenAI, das für High-Reasoning-, Agenten- und allgemeine Produktionsanwendungsfälle entwickelt wurde. Es aktiviert 5,1B Parameter pro Vorwärtsdurchlauf und ist für die Ausführung auf einer einzelnen H100 GPU mit nativer MXFP4-Quantisierung optimiert. Das Modell unterstützt eine konfigurierbare Argumentationstiefe, vollen Chain-of-Thought-Zugriff und die Verwendung nativer Tools, einschließlich Funktionsaufrufe, Browsing und strukturierte Ausgabegenerierung."},"name":"GPT OSS 120B (exacto)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-08-05","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.24"},"eur":{"currency":"eur","input":"0.04","output":"0.21"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.503Z","outputLimit":32768,"contextLength":131072},{"id":"gpt-oss-20b","aliases":["accounts/fireworks/models/gpt-oss-20b","openai/gpt-oss-20b:free","workers-ai/gpt-oss-20b","openai/gpt-oss-20b","gpt-oss-20b:free","gpt-oss-20b"],"description":{"en":"gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.","de":"gpt-oss-20b ist ein offenes 21B-Parameter-Modell, das von OpenAI unter der Apache 2.0-Lizenz veröffentlicht wurde. Es verwendet eine Mixture-of-Experts (MoE)-Architektur mit 3,6B aktiven Parametern pro Vorwärtsdurchlauf, die für Inferenzen mit geringerer Latenz und für die Bereitstellung auf Consumer- oder Single-GPU-Hardware optimiert ist. Das Modell wird im Harmony-Antwortformat von OpenAI trainiert und unterstützt die Konfiguration auf der Argumentationsebene, die Feinabstimmung und agenturische Fähigkeiten wie Funktionsaufrufe, die Verwendung von Tools und strukturierte Ausgaben."},"name":"GPT OSS 20B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p","logit_bias","min_p"],"providers":[{"providerId":"groq","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.50"},"eur":{"currency":"eur","input":"0.09","output":"0.43"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}},{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.20"},"eur":{"currency":"eur","input":"0.04","output":"0.17"}}},{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.20"},"eur":{"currency":"eur","input":"0.04","output":"0.17"}}},{"providerId":"fastrouter","contextLength":131072,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.20"},"eur":{"currency":"eur","input":"0.04","output":"0.17"}}},{"providerId":"cloudflare-workers-ai","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.30"},"eur":{"currency":"eur","input":"0.17","output":"0.26"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.20"},"eur":{"currency":"eur","input":"0.04","output":"0.17"}}},{"providerId":"ovhcloud","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.18"},"eur":{"currency":"eur","input":"0.04","output":"0.15"}}},{"providerId":"deepinfra","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.14"},"eur":{"currency":"eur","input":"0.03","output":"0.12"}}},{"providerId":"lmstudio","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"fireworks-ai","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.20"},"eur":{"currency":"eur","input":"0.04","output":"0.17"}}},{"providerId":"io-net","contextLength":64000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.14"},"eur":{"currency":"eur","input":"0.03","output":"0.12"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":131072,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.326Z","outputLimit":4096,"contextLength":64000},{"id":"gpt-oss-20b-maas","aliases":["openai/gpt-oss-20b-maas","gpt-oss-20b-maas"],"name":"GPT OSS 20B","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"google-vertex","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.25"},"eur":{"currency":"eur","input":"0.06","output":"0.21"}}}],"lastImportedAt":"2025-12-10T00:21:34.400Z","outputLimit":32768,"contextLength":131072},{"id":"gpt-oss-safeguard-20b","aliases":["openai/gpt-oss-safeguard-20b","gpt-oss-safeguard-20b"],"description":{"en":"gpt-oss-safeguard-20b is a safety reasoning model from OpenAI built upon gpt-oss-20b. This open-weight, 21B-parameter Mixture-of-Experts (MoE) model offers lower latency for safety tasks like content classification, LLM filtering, and trust & safety labeling. Learn more about this model in OpenAI's gpt-oss-safeguard [user guide](https://cookbook.openai.com/articles/gpt-oss-safeguard-guide).","de":"gpt-oss-safeguard-20b ist ein Safety Reasoning Model von OpenAI, das auf gpt-oss-20b aufbaut. Dieses offengewichtige Mixture-of-Experts (MoE)-Modell mit 21B-Parametern bietet eine geringere Latenzzeit für Sicherheitsaufgaben wie Inhaltsklassifizierung, LLM-Filterung und Vertrauens- und Sicherheitskennzeichnung. Erfahren Sie mehr über dieses Modell in OpenAIs gpt-oss-safeguard [Benutzerhandbuch] (https://cookbook.openai.com/articles/gpt-oss-safeguard-guide)."},"name":"GPT OSS Safeguard 20B","reasoning":true,"toolCalling":true,"knowledge":"2025-10-29","input":["text"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","seed","stop","tool_choice","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.075","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.502Z","outputLimit":65536,"contextLength":131072},{"id":"gpt-oss:120b-cloud","aliases":["gpt-oss:120b-cloud"],"name":"GPT-OSS 120B","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.467Z","outputLimit":8192,"contextLength":200000},{"id":"gpt-oss:20b-cloud","aliases":["gpt-oss:20b-cloud"],"name":"GPT-OSS 20B","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.678Z","outputLimit":8192,"contextLength":200000},{"id":"granite-4.0-h-micro","aliases":["ibm-granite/granite-4.0-h-micro","workers-ai/granite-4.0-h-micro","granite-4.0-h-micro"],"description":{"en":"Granite-4.0-H-Micro is a 3B parameter from the Granite 4 family of models. These models are the latest in a series of models released by IBM. They are fine-tuned for long context tool calling.","de":"Granite-4.0-H-Micro ist ein 3B-Parameter aus der Granite-4-Modellfamilie. Diese Modelle sind die neuesten in einer Reihe von Modellen, die von IBM herausgegeben wurden. Sie sind auf den Aufruf von Werkzeugen mit langem Kontext abgestimmt."},"name":"@cf/ibm-granite/granite-4.0-h-micro","toolCalling":true,"knowledge":"2025-10-20","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","seed","top_k","top_p"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":131000,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.11"},"eur":{"currency":"eur","input":"0.02","output":"0.09"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":131000,"price":{"usd":{"currency":"usd","input":"0.017","output":"0.11"},"eur":{"currency":"eur","input":"0.01","output":"0.09"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:41.360Z","outputLimit":16384,"contextLength":128000},{"id":"grok-2","aliases":["xai/grok-2","grok-2"],"name":"Grok 2","toolCalling":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.00","output":"10.00"},"eur":{"currency":"eur","input":"1.74","output":"8.68"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.00","output":"10.00"},"eur":{"currency":"eur","input":"1.74","output":"8.68"}}},{"providerId":"poe","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.00","output":"10.00"},"eur":{"currency":"eur","input":"1.74","output":"8.68"}}}],"lastImportedAt":"2025-11-21T07:05:33.517Z","outputLimit":8192,"contextLength":131072},{"id":"grok-2-1212","aliases":["grok-2-1212"],"name":"Grok 2 (1212)","toolCalling":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.00","output":"10.00"},"eur":{"currency":"eur","input":"1.7254228","output":"8.627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.690Z"},{"id":"grok-2-vision","aliases":["xai/grok-2-vision","grok-2-vision"],"name":"Grok 2 Vision","toolCalling":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":8192,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"2.00","output":"10.00"},"eur":{"currency":"eur","input":"1.7254228","output":"8.627114"}}},{"providerId":"vercel","contextLength":8192,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"2.00","output":"10.00"},"eur":{"currency":"eur","input":"1.7254228","output":"8.627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.690Z"},{"id":"grok-2-vision-1212","aliases":["grok-2-vision-1212"],"name":"Grok 2 Vision (1212)","toolCalling":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":8192,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"2.00","output":"10.00"},"eur":{"currency":"eur","input":"1.7254228","output":"8.627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.690Z"},{"id":"grok-3","aliases":["x-ai/grok-3","xai/grok-3","grok-3"],"description":{"en":"Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.","de":"Grok 3 ist das neueste Modell von xAI. Es ist das Flaggschiff des Unternehmens und eignet sich hervorragend für Unternehmensanwendungen wie Datenextraktion, Codierung und Textzusammenfassung. Es verfügt über fundierte Kenntnisse in den Bereichen Finanzen, Gesundheitswesen, Recht und Wissenschaft."},"name":"Grok 3","reasoning":true,"toolCalling":true,"knowledge":"2024-06-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"xai","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"azure-cognitive-services","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"poe","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.258Z","outputLimit":8192,"contextLength":128000},{"id":"grok-3-beta","aliases":["x-ai/grok-3-beta","grok-3-beta"],"description":{"en":"Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science. Excels in structured tasks and benchmarks like GPQA, LCB, and MMLU-Pro where it outperforms Grok 3 Mini even on high thinking. Note: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead.","de":"Grok 3 ist das neueste Modell von xAI. Es ist das Flaggschiff des Unternehmens und eignet sich hervorragend für Unternehmensanwendungen wie Datenextraktion, Codierung und Textzusammenfassung. Es verfügt über fundierte Kenntnisse in den Bereichen Finanzen, Gesundheitswesen, Recht und Wissenschaft. Hervorragend in strukturierten Aufgaben und Benchmarks wie GPQA, LCB und MMLU-Pro, wo es Grok 3 Mini sogar bei hohem Denkaufwand übertrifft. Hinweis: Es gibt zwei xAI-Endpunkte für dieses Modell. Wenn Sie dieses Modell verwenden, leiten wir Sie standardmäßig immer zum Basis-Endpunkt weiter. Wenn Sie den schnellen Endpunkt wünschen, können Sie `provider: { sort: throughput}` hinzufügen, um stattdessen nach Durchsatz zu sortieren."},"name":"Grok 3 Beta","toolCalling":true,"knowledge":"2024-11-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logprobs","max_tokens","presence_penalty","response_format","seed","stop","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.58","output":"12.88"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.450Z","outputLimit":8192,"contextLength":131072},{"id":"grok-3-fast","aliases":["xai/grok-3-fast","grok-3-fast"],"name":"Grok 3 Fast","toolCalling":true,"knowledge":"2024-11-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.313557","output":"21.567785"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"5.00","output":"25.00"},"eur":{"currency":"eur","input":"4.313557","output":"21.567785"}}}],"lastImportedAt":"2025-11-19T12:06:32.689Z"},{"id":"grok-3-mini","aliases":["x-ai/grok-3-mini","xai/grok-3-mini","grok-3-mini"],"description":{"en":"A lightweight model that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible.","de":"Ein leichtgewichtiges Modell, das mitdenkt, bevor es antwortet. Schnell, intelligent und ideal für logikbasierte Aufgaben, die kein tiefes Fachwissen erfordern. Die rohen Denkspuren sind zugänglich."},"name":"Grok 3 Mini","reasoning":true,"toolCalling":true,"knowledge":"2024-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools","include_reasoning","logprobs","max_tokens","reasoning","response_format","seed","stop","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"xai","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.50"},"eur":{"currency":"eur","input":"0.26","output":"0.43"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.50"},"eur":{"currency":"eur","input":"0.26","output":"0.43"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.50"},"eur":{"currency":"eur","input":"0.26","output":"0.43"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.50"},"eur":{"currency":"eur","input":"0.26","output":"0.43"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.50"},"eur":{"currency":"eur","input":"0.26","output":"0.43"}}},{"providerId":"azure-cognitive-services","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.50"},"eur":{"currency":"eur","input":"0.26","output":"0.43"}}},{"providerId":"poe","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.50"},"eur":{"currency":"eur","input":"0.26","output":"0.43"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.258Z","outputLimit":8192,"contextLength":128000},{"id":"grok-3-mini-beta","aliases":["x-ai/grok-3-mini-beta","grok-3-mini-beta"],"description":{"en":"Grok 3 Mini is a lightweight, smaller thinking model. Unlike traditional models that generate answers immediately, Grok 3 Mini thinks before responding. It’s ideal for reasoning-heavy tasks that don’t demand extensive domain knowledge, and shines in math-specific and quantitative use cases, such as solving challenging puzzles or math problems. Transparent \"thinking\" traces accessible. Defaults to low reasoning, can boost with setting `reasoning: { effort: \"high\" }` Note: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead.","de":"Grok 3 Mini ist ein leichtes, kleineres Denkmodell. Im Gegensatz zu traditionellen Modellen, die sofort Antworten generieren, denkt Grok 3 Mini, bevor es antwortet. Es ist ideal für schlussfolgernde Aufgaben, die kein umfangreiches Fachwissen erfordern, und glänzt in mathematischen und quantitativen Anwendungsfällen, wie dem Lösen von anspruchsvollen Rätseln oder mathematischen Problemen. Transparente \"Denk\"-Spuren zugänglich. Standardmäßig niedrige Denkleistung, kann mit der Einstellung `Denkleistung: { effort: \"hoch\" }` Hinweis: Es gibt zwei xAI-Endpunkte für dieses Modell. Standardmäßig werden Sie bei Verwendung dieses Modells immer zum Basis-Endpunkt weitergeleitet. Wenn Sie den schnellen Endpunkt wünschen, können Sie `provider: { sort: throughput}` hinzufügen, um stattdessen nach Durchsatz zu sortieren."},"name":"Grok 3 Mini Beta","reasoning":true,"toolCalling":true,"knowledge":"2024-11-01","input":["text"],"output":["text"],"parameters":["temperature","tools","include_reasoning","logprobs","max_tokens","reasoning","response_format","seed","stop","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.50"},"eur":{"currency":"eur","input":"0.26","output":"0.43"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.450Z","outputLimit":8192,"contextLength":131072},{"id":"grok-3-mini-fast","aliases":["xai/grok-3-mini-fast","grok-3-mini-fast"],"name":"Grok 3 Mini Fast","reasoning":true,"toolCalling":true,"knowledge":"2024-11-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.60","output":"4.00"},"eur":{"currency":"eur","input":"0.51762684","output":"3.4508456"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.60","output":"4.00"},"eur":{"currency":"eur","input":"0.51762684","output":"3.4508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.691Z"},{"id":"grok-4","aliases":["x-ai/grok-4-07-09","grok-4-07-09","x-ai/grok-4","xai/grok-4","grok-4"],"description":{"en":"Grok 4 is xAI's latest reasoning model with a 256k context window. It supports parallel tool calling, structured outputs, and both image and text inputs. Note that reasoning is not exposed, reasoning cannot be disabled, and the reasoning effort cannot be specified. Pricing increases once the total tokens in a given request is greater than 128k tokens. See more details on the [xAI docs](https://docs.x.ai/docs/models/grok-4-0709)","de":"Grok 4 ist das neueste Argumentationsmodell von xAI mit einem 256k-Kontextfenster. Es unterstützt parallele Werkzeugaufrufe, strukturierte Ausgaben und sowohl Bild- als auch Texteingaben. Beachten Sie, dass die Argumentation nicht offengelegt wird, die Argumentation nicht deaktiviert werden kann und der Aufwand für die Argumentation nicht festgelegt werden kann. Der Preis erhöht sich, sobald die Gesamtzahl der Token in einer bestimmten Anfrage 128k Token übersteigt. Weitere Einzelheiten finden Sie in den [xAI docs](https://docs.x.ai/docs/models/grok-4-0709)"},"name":"Grok 4","reasoning":true,"toolCalling":true,"knowledge":"2024-07-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","include_reasoning","logprobs","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"xai","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"vercel","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"azure","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"helicone","contextLength":256000,"outputLimit":256000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"fastrouter","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"openrouter","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"zenmux","contextLength":256000,"outputLimit":256000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"requesty","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"azure-cognitive-services","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"poe","contextLength":256000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.257Z","outputLimit":64000,"contextLength":256000},{"id":"grok-4-1-fast","aliases":["x-ai/grok-4.1-fast","grok-4-1-fast","grok-4.1-fast","grok-41-fast"],"description":{"en":"Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world use cases like customer support and deep research. 2M context window. Reasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)","de":"Grok 4.1 Fast ist xAIs bestes Modell für den Aufruf von Agententools, das in realen Anwendungsfällen wie Kundensupport und Tiefenforschung glänzt. 2M Kontext-Fenster. Reasoning kann mit dem `reasoning` `enabled` Parameter in der API aktiviert/deaktiviert werden. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)"},"name":"Grok 4.1 Fast","reasoning":true,"toolCalling":true,"knowledge":"2024-11-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","include_reasoning","logprobs","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"xai","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"venice","contextLength":262144,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.25"},"eur":{"currency":"eur","input":"0.43","output":"1.07"}}},{"providerId":"openrouter","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.627Z","outputLimit":8192,"contextLength":262144},{"id":"grok-4-1-fast-non-reasoning","aliases":["xai/grok-4.1-fast-non-reasoning","grok-4-1-fast-non-reasoning","grok-4.1-fast-non-reasoning"],"name":"Grok 4.1 Fast (Non-Reasoning)","toolCalling":true,"knowledge":"2025-07-01","input":["text","image"],"output":["text","image"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"helicone","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"poe","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-12-10T00:21:33.970Z","outputLimit":30000,"contextLength":2000000},{"id":"grok-4-1-fast-reasoning","aliases":["xai/grok-4.1-fast-reasoning","grok-4-1-fast-reasoning","grok-4.1-fast-reasoning"],"name":"xAI Grok 4.1 Fast Reasoning","reasoning":true,"toolCalling":true,"knowledge":"2025-11-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"helicone","contextLength":2000000,"outputLimit":2000000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"poe","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-12-10T00:21:34.253Z","outputLimit":30000,"contextLength":2000000},{"id":"grok-4-fast","aliases":["x-ai/grok-4-fast","xai/grok-4-fast","grok-4-fast"],"description":{"en":"Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning. Read more about the model on xAI's [news post](http://x.ai/news/grok-4-fast). Reasoning can be enabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)","de":"Grok 4 Fast ist das neueste multimodale Modell von xAI mit SOTA-Kosteneffizienz und einem Kontextfenster von 2 Millionen Token. Es ist in zwei Varianten erhältlich: Non-Reasoning und Reasoning. Lesen Sie mehr über das Modell in der [news post] von xAI (http://x.ai/news/grok-4-fast). Reasoning kann mit dem `reasoning` `enabled` Parameter in der API aktiviert werden. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)"},"name":"Grok 4 Fast","reasoning":true,"toolCalling":true,"knowledge":"2024-11-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","include_reasoning","logprobs","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"xai","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"vercel","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"openrouter","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"zenmux","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"requesty","contextLength":2000000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.626Z","outputLimit":30000,"contextLength":2000000},{"id":"grok-4-fast-non-reasoning","aliases":["x-ai/grok-4-fast-non-reasoning","xai/grok-4-fast-non-reasoning","grok-4-fast-non-reasoning"],"name":"Grok 4 Fast (Non-Reasoning)","toolCalling":true,"knowledge":"2025-01-01","input":["text","image","audio"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"vercel","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"azure","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"helicone","contextLength":2000000,"outputLimit":2000000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"zenmux","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"azure-cognitive-services","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"poe","contextLength":2000000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}}],"lastImportedAt":"2025-12-10T00:21:33.627Z","outputLimit":30000,"contextLength":2000000},{"id":"grok-4-fast-reasoning","aliases":["xai/grok-4-fast-reasoning","grok-4-fast-reasoning"],"name":"Grok 4 Fast (Reasoning)","reasoning":true,"toolCalling":true,"knowledge":"2025-07-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"azure","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"helicone","contextLength":2000000,"outputLimit":2000000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"azure-cognitive-services","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"poe","contextLength":2000000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}}],"lastImportedAt":"2025-12-10T00:21:34.202Z","outputLimit":30000,"contextLength":2000000},{"id":"grok-4.1-fast","aliases":["x-ai/grok-4.1-fast","grok-4.1-fast"],"description":{"en":"Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world use cases like customer support and deep research. 2M context window. Reasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)","de":"Grok 4.1 Fast ist xAIs bestes Modell für den Aufruf von Agententools, das in realen Anwendungsfällen wie Kundensupport und Tiefenforschung glänzt. 2M Kontext-Fenster. Reasoning kann mit dem `reasoning` `enabled` Parameter in der API aktiviert/deaktiviert werden. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)"},"name":"Grok 4.1 Fast","reasoning":true,"toolCalling":true,"knowledge":"2024-11-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","include_reasoning","logprobs","max_tokens","reasoning","response_format","seed","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"xai","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}},{"providerId":"openrouter","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-20T12:09:08.617Z","outputLimit":30000,"contextLength":2000000,"deprecated":true},{"id":"grok-4.1-fast-non-reasoning","aliases":["grok-4.1-fast-non-reasoning"],"name":"Grok 4.1 Fast (Non-Reasoning)","toolCalling":true,"knowledge":"2025-07-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":2000000,"outputLimit":30000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.50"},"eur":{"currency":"eur","input":"0.17","output":"0.43"}}}],"lastImportedAt":"2025-11-20T12:09:08.416Z","outputLimit":30000,"contextLength":2000000,"deprecated":true},{"id":"grok-beta","aliases":["grok-beta"],"name":"Grok Beta","toolCalling":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":131072,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"5.00","output":"15.00"},"eur":{"currency":"eur","input":"4.313557","output":"12.940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.691Z"},{"id":"grok-code","aliases":["grok-code"],"name":"Grok Code Fast 1","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"opencode","contextLength":256000,"outputLimit":256000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.729Z"},{"id":"grok-code-fast-1","aliases":["x-ai/grok-code-fast-1","xai/grok-code-fast-1","grok-code-fast-1"],"description":{"en":"Grok Code Fast 1 is a speedy and economical reasoning model that excels at agentic coding. With reasoning traces visible in the response, developers can steer Grok Code for high-quality work flows.","de":"Grok Code Fast 1 ist ein schnelles und wirtschaftliches Reasoning-Modell, das sich durch agentenbasiertes Kodieren auszeichnet. Mit den in der Antwort sichtbaren Argumentationsspuren können Entwickler Grok Code für hochwertige Arbeitsabläufe steuern."},"name":"Grok Code Fast 1","reasoning":true,"toolCalling":true,"knowledge":"2023-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","include_reasoning","logprobs","max_tokens","reasoning","response_format","seed","stop","structured_outputs","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"xai","contextLength":256000,"outputLimit":10000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.50"},"eur":{"currency":"eur","input":"0.17","output":"1.29"}}},{"providerId":"github-copilot","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":256000,"outputLimit":10000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.50"},"eur":{"currency":"eur","input":"0.17","output":"1.29"}}},{"providerId":"azure","contextLength":256000,"outputLimit":10000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.50"},"eur":{"currency":"eur","input":"0.17","output":"1.29"}}},{"providerId":"helicone","contextLength":256000,"outputLimit":10000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.50"},"eur":{"currency":"eur","input":"0.17","output":"1.29"}}},{"providerId":"openrouter","contextLength":256000,"outputLimit":10000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.50"},"eur":{"currency":"eur","input":"0.17","output":"1.29"}}},{"providerId":"zenmux","contextLength":256000,"outputLimit":10000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.50"},"eur":{"currency":"eur","input":"0.17","output":"1.29"}}},{"providerId":"azure-cognitive-services","contextLength":256000,"outputLimit":10000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.50"},"eur":{"currency":"eur","input":"0.17","output":"1.29"}}},{"providerId":"poe","contextLength":256000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.50"},"eur":{"currency":"eur","input":"0.17","output":"1.29"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.257Z","outputLimit":10000,"contextLength":128000},{"id":"grok-vision-beta","aliases":["grok-vision-beta"],"name":"Grok Vision Beta","toolCalling":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"xai","contextLength":8192,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"5.00","output":"15.00"},"eur":{"currency":"eur","input":"4.313557","output":"12.940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.690Z"},{"id":"groq-llama-4-maverick-17b-128e-instruct","aliases":["groq-llama-4-maverick-17b-128e-instruct"],"name":"Groq-Llama-4-Maverick-17B-128E-Instruct","toolCalling":true,"openWeights":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"llama","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.756Z"},{"id":"hermes-2-pro-llama-3-8b","aliases":["nousresearch/hermes-2-pro-llama-3-8b","hermes-2-pro-llama-3-8b"],"description":{"en":"Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.","de":"Hermes 2 Pro ist eine aktualisierte, neu trainierte Version von Nous Hermes 2, bestehend aus einer aktualisierten und bereinigten Version des OpenHermes 2.5-Datensatzes sowie einem neu eingeführten, intern entwickelten Funktionsaufruf- und JSON-Modus-Datensatz."},"name":"Hermes 2 Pro Llama 3 8B","toolCalling":true,"knowledge":"2024-05-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"helicone","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.14"},"eur":{"currency":"eur","input":"0.12","output":"0.12"}}},{"providerId":"openrouter","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0.025","output":"0.08"},"eur":{"currency":"eur","input":"0.02","output":"0.07"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.768Z","outputLimit":131072,"contextLength":8192},{"id":"hermes-2-pro-mistral-7b","aliases":["hermes-2-pro-mistral-7b"],"name":"@hf/nousresearch/hermes-2-pro-mistral-7b","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":24000,"outputLimit":24000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.344Z","outputLimit":24000,"contextLength":24000},{"id":"hermes-3-llama-3.1-405b","aliases":["nousresearch/hermes-3-llama-3.1-405b:free","nousresearch/hermes-3-llama-3.1-405b","hermes-3-llama-3.1-405b:free","hermes-3-llama-3.1-405b"],"description":{"en":"Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board. Hermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user. The Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills. Hermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.","de":"Hermes 3 ist ein generalistisches Sprachmodell mit vielen Verbesserungen gegenüber Hermes 2, einschließlich erweiterter agenturischer Fähigkeiten, viel besserem Rollenspiel, logischem Denken, Konversation mit mehreren Runden, langer Kontextkohärenz und Verbesserungen in allen Bereichen. Hermes 3 405B ist eine Feinabstimmung des Llama-3.1 405B Basismodells auf Grenzniveau mit allen Parametern und konzentriert sich darauf, LLMs auf den Benutzer auszurichten, wobei dem Endbenutzer leistungsstarke Steuerungsmöglichkeiten und Kontrolle gegeben werden. Die Hermes-3-Serie baut auf den Hermes-2-Fähigkeiten auf und erweitert sie, einschließlich leistungsfähigerer und zuverlässigerer Funktionsaufrufe und strukturierter Ausgabefähigkeiten, generalistischer Assistentenfähigkeiten und verbesserter Codegenerierungsfähigkeiten. Hermes 3 ist konkurrenzfähig, wenn nicht sogar besser als die Llama-3.1 Instruct-Modelle bei den allgemeinen Fähigkeiten, mit unterschiedlichen Stärken und Schwächen zwischen den beiden zuzuordnen."},"name":"Hermes 3 Llama 3.1 405b","openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","max_tokens","presence_penalty","stop","top_k","top_p","min_p","repetition_penalty","response_format","seed"],"providers":[{"providerId":"venice","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.10","output":"3.00"},"eur":{"currency":"eur","input":"0.94","output":"2.58"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"1","output":"1"},"eur":{"currency":"eur","input":"0.86","output":"0.86"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":131072,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.179Z","outputLimit":8192,"contextLength":131072},{"id":"hermes-3-llama-3.1-70b","aliases":["nousresearch/hermes-3-llama-3.1-70b","hermes-3-llama-3.1-70b"],"name":"Nous: Hermes 3 70B Instruct","description":{"en":"Hermes 3 is a generalist language model with many improvements over [Hermes 2](/models/nousresearch/nous-hermes-2-mistral-7b-dpo), including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board. Hermes 3 70B is a competitive, if not superior finetune of the [Llama-3.1 70B foundation model](/models/meta-llama/llama-3.1-70b-instruct), focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user. The Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.","de":"Hermes 3 ist ein generalistisches Sprachmodell mit vielen Verbesserungen im Vergleich zu [Hermes 2](/models/nousresearch/nous-hermes-2-mistral-7b-dpo), einschließlich erweiterter agenturischer Fähigkeiten, viel besserem Rollenspiel, logischem Denken, Konversation mit mehreren Runden, langer Kontextkohärenz und Verbesserungen in allen Bereichen. Hermes 3 70B ist eine konkurrenzfähige, wenn nicht gar überlegene Feinabstimmung des [Llama-3.1 70B-Grundmodells](/models/meta-llama/llama-3.1-70b-instruct), die sich darauf konzentriert, LLMs auf den Benutzer auszurichten, wobei dem Endbenutzer leistungsstarke Steuerungsmöglichkeiten und Kontrolle gegeben werden. Die Hermes 3 Serie baut auf den Fähigkeiten von Hermes 2 auf und erweitert diese, einschließlich leistungsfähigerer und zuverlässigerer Funktionsaufrufe und strukturierter Ausgabefähigkeiten, generalistischer Assistentenfähigkeiten und verbesserter Codegenerierungsfähigkeiten."},"knowledge":"2024-08-18","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":65536,"price":{"usd":{"currency":"usd","input":"0.3","output":"0.3"},"eur":{"currency":"eur","input":"0.26","output":"0.26"}}}],"lastImportedAt":"2025-11-28T12:08:40.539Z","contextLength":65536},{"id":"hermes-4-14b","aliases":["nousresearch/hermes-4-14b","hermes-4-14b"],"description":{"en":"Hermes 4 (Qwen 3 14B) is a hybrid reasoning model designed for tasks requiring structured outputs and enhanced reasoning capabilities. It excels in math, code, STEM, and creative writing. Features include function calling, JSON output, and improved steerability, making it suitable for diverse applications in chat and decision-making contexts.","de":"Hermes 4 (Qwen 3 14B) ist ein hybrides Denkmodell, das für Aufgaben entwickelt wurde, die strukturierte Ausgaben und erweiterte Denkfähigkeiten erfordern. Es zeichnet sich durch Mathe, Code, MINT und kreatives Schreiben aus. Zu den Funktionen gehören Funktionsaufrufe, JSON-Ausgabe und verbesserte Steuerbarkeit, wodurch es sich für verschiedene Anwendungen in Chat- und Entscheidungskontexten eignet."},"name":"Hermes 4 14B","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"chutes","contextLength":40960,"outputLimit":40960,"price":{"usd":{"currency":"usd","input":"0.01","output":"0.05"},"eur":{"currency":"eur","input":"0.01","output":"0.04"}}}],"lastImportedAt":"2025-12-03T12:09:06.564Z","outputLimit":40960,"contextLength":40960},{"id":"hermes-4-405b","aliases":["nousresearch/hermes-4-405b","hermes-4-405b"],"description":{"en":"Hermes 4 is a large-scale reasoning model built on Meta-Llama-3.1-405B and released by Nous Research. It introduces a hybrid reasoning mode, where the model can choose to deliberate internally with <think>...</think> traces or respond directly, offering flexibility between speed and depth. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) The model is instruction-tuned with an expanded post-training corpus (~60B tokens) emphasizing reasoning traces, improving performance in math, code, STEM, and logical reasoning, while retaining broad assistant utility. It also supports structured outputs, including JSON mode, schema adherence, function calling, and tool use. Hermes 4 is trained for steerability, lower refusal rates, and alignment toward neutral, user-directed behavior.","de":"Hermes 4 ist ein umfangreiches Argumentationsmodell, das auf Meta-Llama-3.1-405B aufbaut und von Nous Research veröffentlicht wurde. Es führt einen hybriden Argumentationsmodus ein, bei dem das Modell entweder intern mit <think>...</think>-Spuren nachdenken oder direkt reagieren kann, was Flexibilität zwischen Geschwindigkeit und Tiefe bietet. Benutzer können das Denkverhalten mit dem Booleschen Parameter `reasoning` `enabled` steuern. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) Das Modell wurde mit einem erweiterten Post-Trainings-Korpus (~60B Token) auf Anweisungen abgestimmt, wobei der Schwerpunkt auf Argumentationsspuren liegt, was die Leistung in den Bereichen Mathematik, Code, MINT und logisches Denken verbessert, während der breite Nutzen für Assistenten erhalten bleibt. Es unterstützt auch strukturierte Ausgaben, einschließlich JSON-Modus, Schematreue, Funktionsaufrufe und Werkzeugnutzung. Hermes 4 ist auf Steuerbarkeit, niedrigere Ablehnungsraten und Ausrichtung auf neutrales, benutzergesteuertes Verhalten trainiert."},"name":"Hermes-4 405B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.8627114","output":"2.5881342"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.8627114","output":"2.5881342"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.713Z"},{"id":"hermes-4-405b-fp8","aliases":["nousresearch/hermes-4-405b-fp8","hermes-4-405b-fp8"],"description":{"en":"Hermes 4 is a hybrid reasoning model based on Llama-3.1-405B, designed for enhanced math, code, logic, and creative writing tasks. It features improved steerability, structured output generation in JSON, and support for function/tool calls. Ideal for applications requiring robust reasoning and user alignment.","de":"Hermes 4 ist ein hybrides Argumentationsmodell, das auf Llama-3.1-405B basiert und für erweiterte Aufgaben in den Bereichen Mathematik, Code, Logik und kreatives Schreiben entwickelt wurde. Es bietet verbesserte Steuerbarkeit, strukturierte Ausgabe in JSON und Unterstützung für Funktions-/Tool-Aufrufe. Ideal für Anwendungen, die robuste Argumentation und Benutzeranpassung erfordern."},"name":"Hermes 4 405B FP8","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}}],"lastImportedAt":"2025-11-19T12:06:32.718Z"},{"id":"hermes-4-70b","aliases":["nousresearch/hermes-4-70b","hermes-4-70b"],"description":{"en":"Hermes 4 70B is a hybrid reasoning model from Nous Research, built on Meta-Llama-3.1-70B. It introduces the same hybrid mode as the larger 405B release, allowing the model to either respond directly or generate explicit <think>...</think> reasoning traces before answering. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) This 70B variant is trained with the expanded post-training corpus (~60B tokens) emphasizing verified reasoning data, leading to improvements in mathematics, coding, STEM, logic, and structured outputs while maintaining general assistant performance. It supports JSON mode, schema adherence, function calling, and tool use, and is designed for greater steerability with reduced refusal rates.","de":"Hermes 4 70B ist ein hybrides Argumentationsmodell von Nous Research, das auf Meta-Llama-3.1-70B aufbaut. Es führt den gleichen hybriden Modus wie die größere Version 405B ein, der es dem Modell erlaubt, entweder direkt zu antworten oder explizite <think>...</think> Argumentationsspuren zu erzeugen, bevor es antwortet. Benutzer können das Denkverhalten mit dem Boolean `reasoning` `enabled` steuern. [Erfahren Sie mehr in unseren Dokumenten](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) Diese 70B-Variante wird mit dem erweiterten Post-Training-Korpus (~60B Token) trainiert, wobei der Schwerpunkt auf verifizierten Argumentationsdaten liegt, was zu Verbesserungen in den Bereichen Mathematik, Kodierung, MINT, Logik und strukturierte Ausgaben führt, während die allgemeine Assistentenleistung beibehalten wird. Er unterstützt den JSON-Modus, die Einhaltung von Schemata, den Aufruf von Funktionen und die Verwendung von Werkzeugen und ist für eine bessere Steuerbarkeit mit geringeren Ablehnungsraten ausgelegt."},"name":"Hermes 4 70B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.40"},"eur":{"currency":"eur","input":"0.112152482","output":"0.34508456"}}},{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.11","output":"0.38"},"eur":{"currency":"eur","input":"0.094898254","output":"0.327830332"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.40"},"eur":{"currency":"eur","input":"0.112152482","output":"0.34508456"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.713Z"},{"id":"hermes-4.3-36b","aliases":["nousresearch/hermes-4.3-36b","hermes-4.3-36b"],"description":{"en":"Hermes 4.3 is a 36B hybrid reasoning model designed for text generation, emphasizing improved math, coding, and logic capabilities. It supports structured outputs, function calling, and enhanced steerability. Ideal for applications requiring complex reasoning and diverse human-like responses, it is aligned to user values with a focus on transparency and usability.","de":"Hermes 4.3 ist ein hybrides 36B-Schlussfolgermodell, das für die Texterzeugung entwickelt wurde, wobei der Schwerpunkt auf verbesserten mathematischen, kodierenden und logischen Fähigkeiten liegt. Es unterstützt strukturierte Ausgaben, Funktionsaufrufe und verbesserte Steuerbarkeit. Es eignet sich ideal für Anwendungen, die komplexe Schlussfolgerungen und verschiedene menschenähnliche Antworten erfordern, und ist auf die Werte der Benutzer ausgerichtet, wobei der Schwerpunkt auf Transparenz und Benutzerfreundlichkeit liegt."},"name":"Hermes 4.3 36B","openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"chutes","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.51"}}}],"lastImportedAt":"2025-12-12T00:21:31.037Z","outputLimit":8192,"contextLength":32768},{"id":"horizon-alpha","aliases":["openrouter/horizon-alpha","horizon-alpha"],"name":"Horizon Alpha","toolCalling":true,"knowledge":"2025-07-01","input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"openrouter","contextLength":256000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.751Z","deprecated":true},{"id":"horizon-beta","aliases":["openrouter/horizon-beta","horizon-beta"],"name":"Horizon Beta","toolCalling":true,"knowledge":"2025-07-01","input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"openrouter","contextLength":256000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.751Z","deprecated":true},{"id":"hunyuan-a13b-instruct","aliases":["tencent/hunyuan-a13b-instruct","hunyuan-a13b-instruct"],"name":"Tencent: Hunyuan A13B Instruct","description":{"en":"Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model developed by Tencent, with a total parameter count of 80B and support for reasoning via Chain-of-Thought. It offers competitive benchmark performance across mathematics, science, coding, and multi-turn reasoning tasks, while maintaining high inference efficiency via Grouped Query Attention (GQA) and quantization support (FP8, GPTQ, etc.).","de":"Hunyuan-A13B ist ein von Tencent entwickeltes Mixture-of-Experts (MoE)-Sprachmodell mit 13B aktiven Parametern, einer Gesamtparameteranzahl von 80B und Unterstützung für Schlussfolgerungen mittels Chain-of-Thought. Es bietet eine konkurrenzfähige Benchmark-Leistung in den Bereichen Mathematik, Naturwissenschaften, Codierung und Multi-Turn-Reasoning-Aufgaben, während es gleichzeitig eine hohe Inferenz-Effizienz durch Grouped Query Attention (GQA) und Quantisierungsunterstützung (FP8, GPTQ, etc.) beibehält."},"knowledge":"2025-07-08","reasoning":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","reasoning","response_format","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.57"},"eur":{"currency":"eur","input":"0.120779596","output":"0.491745498"}}}],"lastImportedAt":"2025-11-19T12:06:32.764Z"},{"id":"ideogram","aliases":["ideogramai/ideogram","ideogram"],"name":"Ideogram","toolCalling":true,"input":["text","image"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":150,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:35.109Z","contextLength":150},{"id":"ideogram-v2","aliases":["ideogramai/ideogram-v2","ideogram-v2"],"name":"Ideogram-v2","toolCalling":true,"input":["text","image"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":150,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:35.852Z","contextLength":150},{"id":"ideogram-v2a","aliases":["ideogramai/ideogram-v2a","ideogram-v2a"],"name":"Ideogram-v2a","toolCalling":true,"input":["text"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":150,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:35.340Z","contextLength":150},{"id":"ideogram-v2a-turbo","aliases":["ideogramai/ideogram-v2a-turbo","ideogram-v2a-turbo"],"name":"Ideogram-v2a-Turbo","toolCalling":true,"input":["text"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":150,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:35.622Z","contextLength":150},{"id":"imagen-3","aliases":["google/imagen-3","imagen-3"],"name":"Imagen-3","toolCalling":true,"input":["text"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:40.314Z","contextLength":480},{"id":"imagen-3-fast","aliases":["google/imagen-3-fast","imagen-3-fast"],"name":"Imagen-3-Fast","toolCalling":true,"input":["text"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:41.227Z","contextLength":480},{"id":"imagen-4","aliases":["google/imagen-4","imagen-4"],"name":"Imagen-4","toolCalling":true,"input":["text"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:40.091Z","contextLength":480},{"id":"imagen-4-fast","aliases":["google/imagen-4-fast","imagen-4-fast"],"name":"Imagen-4-Fast","toolCalling":true,"input":["text"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:42.153Z","contextLength":480},{"id":"imagen-4-ultra","aliases":["google/imagen-4-ultra","imagen-4-ultra"],"name":"Imagen-4-Ultra","toolCalling":true,"input":["text"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:40.544Z","contextLength":480},{"id":"inclusionai-ling-flash-2.0","aliases":["inclusionai-ling-flash-2.0"],"name":"inclusionAI/Ling-flash-2.0","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.57"},"eur":{"currency":"eur","input":"0.12","output":"0.5"}}}],"lastImportedAt":"2025-11-26T00:20:46.240Z","outputLimit":131000,"contextLength":131000},{"id":"inclusionai-ling-mini-2.0","aliases":["inclusionai-ling-mini-2.0"],"name":"inclusionAI/Ling-mini-2.0","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.28"},"eur":{"currency":"eur","input":"0.06","output":"0.24"}}}],"lastImportedAt":"2025-11-26T00:20:46.408Z","outputLimit":131000,"contextLength":131000},{"id":"inclusionai-ring-flash-2.0","aliases":["inclusionai-ring-flash-2.0"],"name":"inclusionAI/Ring-flash-2.0","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.57"},"eur":{"currency":"eur","input":"0.12","output":"0.5"}}}],"lastImportedAt":"2025-11-26T00:20:47.173Z","outputLimit":131000,"contextLength":131000},{"id":"indictrans2-en-indic-1b","aliases":["workers-ai/indictrans2-en-indic-1b","indictrans2-en-indic-1b"],"name":"indictrans2 en indic 1B","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.015Z","outputLimit":16384,"contextLength":128000},{"id":"inflection-3-pi","aliases":["inflection/inflection-3-pi","inflection-3-pi"],"name":"Inflection: Inflection 3 Pi","description":{"en":"Inflection 3 Pi powers Inflection's [Pi](https://pi.ai) chatbot, including backstory, emotional intelligence, productivity, and safety. It has access to recent news, and excels in scenarios like customer support and roleplay. Pi has been trained to mirror your tone and style, if you use more emojis, so will Pi! Try experimenting with various prompts and conversation styles.","de":"Inflection 3 Pi steuert den [Pi](https://pi.ai) Chatbot von Inflection, einschließlich Hintergrundgeschichte, emotionaler Intelligenz, Produktivität und Sicherheit. Er hat Zugang zu aktuellen Nachrichten und zeichnet sich in Szenarien wie Kundensupport und Rollenspielen aus. Pi wurde darauf trainiert, Ihren Tonfall und Stil wiederzugeben. Wenn Sie mehr Emojis verwenden, wird Pi das auch tun! Experimentieren Sie mit verschiedenen Eingabeaufforderungen und Gesprächsstilen."},"knowledge":"2024-10-11","input":["text"],"output":["text"],"parameters":["max_tokens","stop","temperature","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8000,"price":{"usd":{"currency":"usd","input":"2.5","output":"10"},"eur":{"currency":"eur","input":"2.1567785","output":"8.627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"inflection-3-productivity","aliases":["inflection/inflection-3-productivity","inflection-3-productivity"],"name":"Inflection: Inflection 3 Productivity","description":{"en":"Inflection 3 Productivity is optimized for following instructions. It is better for tasks requiring JSON output or precise adherence to provided guidelines. It has access to recent news. For emotional intelligence similar to Pi, see [Inflect 3 Pi](/inflection/inflection-3-pi) See [Inflection's announcement](https://inflection.ai/blog/enterprise) for more details.","de":"Inflection 3 Productivity ist für die Befolgung von Anweisungen optimiert. Sie eignet sich besser für Aufgaben, die eine JSON-Ausgabe oder die genaue Einhaltung vorgegebener Richtlinien erfordern. Sie hat Zugang zu aktuellen Nachrichten. Für emotionale Intelligenz ähnlich wie Pi, siehe [Inflect 3 Pi](/inflection/inflection-3-pi) Weitere Einzelheiten finden Sie in [Inflection's announcement](https://inflection.ai/blog/enterprise)."},"knowledge":"2024-10-11","input":["text"],"output":["text"],"parameters":["max_tokens","stop","temperature","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8000,"price":{"usd":{"currency":"usd","input":"2.5","output":"10"},"eur":{"currency":"eur","input":"2.1567785","output":"8.627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"intel-qwen3-coder-480b-a35b-instruct-int4-mixed-ar","aliases":["intel-qwen3-coder-480b-a35b-instruct-int4-mixed-ar"],"name":"Qwen 3 Coder 480B","toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"io-intelligence","contextLength":106000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.22","output":"0.95"},"eur":{"currency":"eur","input":"0.19","output":"0.82"}}}],"lastImportedAt":"2025-11-27T12:08:36.951Z","outputLimit":4096,"contextLength":106000,"deprecated":true},{"id":"intellect-3","aliases":["prime-intellect/intellect-3-20251126","primeintellect/intellect-3-fp8","prime-intellect/intellect-3","intellect-3-20251126","intellect-3-fp8","intellect-3"],"name":"Prime Intellect: INTELLECT-3","description":{"en":"INTELLECT-3 is a 106B-parameter Mixture-of-Experts model (12B active) post-trained from GLM-4.5-Air-Base using supervised fine-tuning (SFT) followed by large-scale reinforcement learning (RL). It offers state-of-the-art performance for its size across math, code, science, and general reasoning, consistently outperforming many larger frontier models. Designed for strong multi-step problem solving, it maintains high accuracy on structured tasks while remaining efficient at inference thanks to its MoE architecture.","de":"INTELLECT-3 ist ein 106B-Parameter Mixture-of-Experts-Modell (12B aktiv), das mit GLM-4.5-Air-Base nachtrainiert wurde. Dabei wurde überwachte Feinabstimmung (SFT) gefolgt von groß angelegtem Reinforcement Learning (RL) eingesetzt. Es bietet eine für seine Größe hochmoderne Leistung in den Bereichen Mathematik, Code, Wissenschaft und allgemeines Denken und übertrifft durchweg viele größere Grenzmodelle. Es wurde für eine starke mehrstufige Problemlösung entwickelt und behält eine hohe Genauigkeit bei strukturierten Aufgaben bei, während es dank seiner MoE-Architektur effizient bei der Inferenz bleibt."},"knowledge":"2025-11-27","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.2","output":"1.1"},"eur":{"currency":"eur","input":"0.17","output":"0.95"}}}],"lastImportedAt":"2025-11-27T12:08:37.146Z","contextLength":131072},{"id":"internvl3-78b","aliases":["opengvlab/internvl3-78b","internvl3-78b"],"description":{"en":"The InternVL3 series is an advanced multimodal large language model (MLLM). Compared to InternVL 2.5, InternVL3 demonstrates stronger multimodal perception and reasoning capabilities. In addition, InternVL3 is benchmarked against the Qwen2.5 Chat models, whose pre-trained base models serve as the initialization for its language component. Benefiting from Native Multimodal Pre-Training, the InternVL3 series surpasses the Qwen2.5 series in overall text performance.","de":"Die InternVL3-Serie ist ein fortschrittliches multimodales großes Sprachmodell (MLLM). Im Vergleich zu InternVL 2.5 weist InternVL3 stärkere multimodale Wahrnehmungs- und Schlussfolgerungsfähigkeiten auf. Darüber hinaus wird InternVL3 mit den Chat-Modellen von Qwen2.5 verglichen, deren vortrainierte Basismodelle als Initialisierung für die Sprachkomponente dienen. Dank des nativen multimodalen Pre-Trainings übertrifft die InternVL3-Serie die Qwen2.5-Serie bei der Gesamtleistung im Textbereich."},"name":"InternVL3 78B","openWeights":true,"knowledge":"2025-09-15","input":["text","image"],"output":["text"],"parameters":["temperature","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"chutes","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.39"},"eur":{"currency":"eur","input":"0.09","output":"0.33"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.39"},"eur":{"currency":"eur","input":"0.09","output":"0.33"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T00:21:33.657Z","outputLimit":32768,"contextLength":32768},{"id":"jais-30b-chat","aliases":["core42/jais-30b-chat","jais-30b-chat"],"name":"JAIS 30b Chat","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.722Z"},{"id":"jamba-large-1.7","aliases":["ai21labs/ai21-jamba-large-1.7","ai21/jamba-large-1.7","ai21-jamba-large-1.7","jamba-large-1.7"],"name":"AI21: Jamba Large 1.7","description":{"en":"Jamba Large 1.7 is the latest model in the Jamba open family, offering improvements in grounding, instruction-following, and overall efficiency. Built on a hybrid SSM-Transformer architecture with a 256K context window, it delivers more accurate, contextually grounded responses and better steerability than previous versions.","de":"Jamba Large 1.7 ist das neueste Modell der Jamba open-Familie und bietet Verbesserungen in Bezug auf Erdung, Befolgung von Anweisungen und Gesamteffizienz. Es basiert auf einer hybriden SSM-Transformer-Architektur mit einem 256K-Kontextfenster und liefert genauere, kontextbezogene Antworten und eine bessere Steuerbarkeit als frühere Versionen."},"knowledge":"2025-08-08","toolCalling":true,"input":["text"],"output":["text"],"parameters":["max_tokens","response_format","stop","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":256000,"price":{"usd":{"currency":"usd","input":"2","output":"8"},"eur":{"currency":"eur","input":"1.7254228","output":"6.9016912"}}}],"lastImportedAt":"2025-11-19T12:06:32.764Z"},{"id":"jamba-mini-1.7","aliases":["ai21labs/ai21-jamba-mini-1.7","ai21/jamba-mini-1.7","ai21-jamba-mini-1.7","jamba-mini-1.7"],"name":"AI21: Jamba Mini 1.7","description":{"en":"Jamba Mini 1.7 is a compact and efficient member of the Jamba open model family, incorporating key improvements in grounding and instruction-following while maintaining the benefits of the SSM-Transformer hybrid architecture and 256K context window. Despite its compact size, it delivers accurate, contextually grounded responses and improved steerability.","de":"Jamba Mini 1.7 ist ein kompaktes und effizientes Mitglied der offenen Jamba-Modellfamilie, das wichtige Verbesserungen in Bezug auf Erdung und Befehlsverfolgung enthält und gleichzeitig die Vorteile der hybriden SSM-Transformer-Architektur und des 256K-Kontextfensters beibehält. Trotz seiner kompakten Größe liefert er genaue, kontextbezogene Antworten und eine verbesserte Steuerbarkeit."},"knowledge":"2025-08-08","toolCalling":true,"input":["text"],"output":["text"],"parameters":["max_tokens","response_format","stop","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":256000,"price":{"usd":{"currency":"usd","input":"0.2","output":"0.4"},"eur":{"currency":"eur","input":"0.17254228","output":"0.34508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.764Z"},{"id":"kat-coder-pro","aliases":["kwaipilot/kat-coder-pro:free","kwaipilot/kat-coder-pro-v1","kat-coder-pro:free","kat-coder-pro-v1","kat-coder-pro"],"description":{"en":"KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KAT-Coder series. Designed specifically for agentic coding tasks, it excels in real-world software engineering scenarios, achieving 73.4% solve rate on the SWE-Bench Verified benchmark. The model has been optimized for tool-use capability, multi-turn interaction, instruction following, generalization, and comprehensive capabilities through a multi-stage training process, including mid-training, supervised fine-tuning (SFT), reinforcement fine-tuning (RFT), and scalable agentic RL.","de":"KAT-Coder-Pro V1 ist KwaiKATs fortschrittlichstes agentenbasiertes Codierungsmodell in der KAT-Coder-Serie. Es wurde speziell für agentenbasierte Codierungsaufgaben entwickelt und zeichnet sich in realen Software-Engineering-Szenarien durch eine Lösungsrate von 73,4 % beim SWE-Bench Verified Benchmark aus. Das Modell wurde im Hinblick auf Tool-Use-Fähigkeit, Multi-Turn-Interaktion, Befolgung von Anweisungen, Generalisierung und umfassende Fähigkeiten durch einen mehrstufigen Trainingsprozess optimiert, einschließlich Mid-Training, überwachtem Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT) und skalierbarem Agentic RL."},"name":"Kat Coder Pro (free)","toolCalling":true,"knowledge":"2025-11-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"openrouter","contextLength":256000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":256000,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.749Z"},{"id":"kat-coder-pro-v1","aliases":["kuaishou/kat-coder-pro-v1","kat-coder-pro-v1"],"name":"KAT-Coder-Pro-V1","reasoning":true,"toolCalling":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"zenmux","contextLength":256000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.40"},"eur":{"currency":"eur","input":"0.51762684","output":"2.07050736"}}}],"lastImportedAt":"2025-11-19T12:06:32.753Z"},{"id":"kimi-dev-72b","aliases":["moonshotai/kimi-dev-72b:free","moonshotai/kimi-dev-72b","kimi-dev-72b:free","kimi-dev-72b"],"description":{"en":"Kimi-Dev-72B is an open-source large language model fine-tuned for software engineering and issue resolution tasks. Based on Qwen2.5-72B, it is optimized using large-scale reinforcement learning that applies code patches in real repositories and validates them via full test suite execution—rewarding only correct, robust completions. The model achieves 60.4% on SWE-bench Verified, setting a new benchmark among open-source models for software bug fixing and code reasoning.","de":"Kimi-Dev-72B ist ein quelloffenes, umfangreiches Sprachmodell, das für Softwareentwicklung und Problemlösungsaufgaben optimiert wurde. Es basiert auf Qwen2.5-72B und wurde mit Hilfe von Large-Scale Reinforcement Learning optimiert, das Code-Patches in realen Repositories anwendet und durch die Ausführung einer vollständigen Test-Suite validiert - und nur korrekte, robuste Vervollständigungen belohnt. Das Modell erreicht 60,4 % im SWE-Bench Verified und setzt damit einen neuen Maßstab unter den Open-Source-Modellen für Software-Fehlerbehebung und Code-Reasoning."},"name":"Kimi Dev 72b (free)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","reasoning","response_format","structured_outputs","top_k","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.29","output":"1.15"},"eur":{"currency":"eur","input":"0.250186306","output":"0.99211811"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.748Z"},{"id":"kimi-k2","aliases":["moonshotai/kimi-k2-instruct","moonshotai/kimi-k2:free","moonshotai/kimi-k2","kimi-k2-instruct","kimi-k2:free","kimi-k2"],"description":{"en":"Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.","de":"Kimi K2 Instruct ist ein groß angelegtes Mixture-of-Experts (MoE) Sprachmodell, das von Moonshot AI entwickelt wurde und insgesamt 1 Billion Parameter mit 32 Milliarden aktiven Parametern pro Vorwärtsdurchlauf aufweist. Es ist für die Fähigkeiten von Agenten optimiert, einschließlich der fortgeschrittenen Nutzung von Werkzeugen, Argumentation und Codesynthese. Kimi K2 zeichnet sich durch ein breites Spektrum an Benchmarks aus, insbesondere bei Codierungs- (LiveCodeBench, SWE-bench), Schlussfolgerungs- (ZebraLogic, GPQA) und Tool-Nutzungsaufgaben (Tau2, AceBench). Es unterstützt Long-Context-Inferenz mit bis zu 128K Token und wurde mit einem neuartigen Trainingsstack entwickelt, der den MuonClip-Optimierer für stabiles MoE-Training in großem Maßstab enthält."},"name":"Kimi K2 Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","max_tokens","seed","stop","frequency_penalty","min_p","presence_penalty","repetition_penalty","response_format","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"vercel","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.86","output":"2.59"}}},{"providerId":"opencode","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.40","output":"2.50"},"eur":{"currency":"eur","input":"0.35","output":"2.16"}}},{"providerId":"fastrouter","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.20"},"eur":{"currency":"eur","input":"0.47","output":"1.9"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.20"},"eur":{"currency":"eur","input":"0.47","output":"1.9"}}},{"providerId":"iflowcn","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":32768,"outputLimit":32800,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-11-28T00:20:09.672Z","outputLimit":16384,"contextLength":128000},{"id":"kimi-k2-0711","aliases":["kimi-k2-0711"],"name":"Kimi K2 (07/11)","toolCalling":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"helicone","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.57","output":"2.30"},"eur":{"currency":"eur","input":"0.49","output":"1.97"}}}],"lastImportedAt":"2025-12-09T00:20:58.722Z","outputLimit":16384,"contextLength":131072},{"id":"kimi-k2-0905","aliases":["moonshotai/kimi-k2-instruct-0905","moonshotai/kimi-k2-0905","kimi-k2-instruct-0905","kimi-k2-0905"],"description":{"en":"Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.","de":"Kimi K2 0905 ist das September-Update von [Kimi K2 0711] (moonshotai/kimi-k2). Es handelt sich um ein großes Mixture-of-Experts (MoE)-Sprachmodell, das von Moonshot AI entwickelt wurde und insgesamt 1 Billion Parameter mit 32 Milliarden aktiven Parametern pro Vorwärtsdurchlauf aufweist. Es unterstützt die Inferenz von langen Kontexten mit bis zu 256k Token, die von den vorherigen 128k erweitert wurden. Dieses Update verbessert die agentenbasierte Kodierung mit höherer Genauigkeit und besserer Generalisierung über Gerüste hinweg und verbessert die Frontend-Kodierung mit ästhetischeren und funktionaleren Ausgaben für Web, 3D und verwandte Aufgaben. Kimi K2 ist für agenturische Fähigkeiten optimiert, einschließlich fortgeschrittenem Werkzeuggebrauch, logischem Denken und Codesynthese. Es zeichnet sich in den Benchmarks für Codierung (LiveCodeBench, SWE-Bench), schlussfolgerndes Denken (ZebraLogic, GPQA) und Tool-Nutzung (Tau2, AceBench) aus. Das Modell wird mit einem neuartigen Stack trainiert, der den MuonClip-Optimierer für stabiles MoE-Training in großem Maßstab enthält."},"name":"Kimi K2 (09/05)","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"helicone","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.43","output":"1.72"}}},{"providerId":"openrouter","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.51","output":"2.15"}}},{"providerId":"zenmux","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.51","output":"2.15"}}},{"providerId":"iflowcn","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"aihubmix","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.19"},"eur":{"currency":"eur","input":"0.47","output":"1.88"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.737Z","outputLimit":16384,"contextLength":256000},{"id":"kimi-k2-0905:exacto","aliases":["moonshotai/kimi-k2-instruct-0905","moonshotai/kimi-k2-0905:exacto","moonshotai/kimi-k2-0905","kimi-k2-instruct-0905","kimi-k2-0905:exacto","kimi-k2-0905"],"description":{"en":"Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.","de":"Kimi K2 0905 ist das September-Update von [Kimi K2 0711] (moonshotai/kimi-k2). Es handelt sich um ein großes Mixture-of-Experts (MoE)-Sprachmodell, das von Moonshot AI entwickelt wurde und insgesamt 1 Billion Parameter mit 32 Milliarden aktiven Parametern pro Vorwärtsdurchlauf aufweist. Es unterstützt die Inferenz von langen Kontexten mit bis zu 256k Token, die von den vorherigen 128k erweitert wurden. Dieses Update verbessert die agentenbasierte Kodierung mit höherer Genauigkeit und besserer Generalisierung über Gerüste hinweg und verbessert die Frontend-Kodierung mit ästhetischeren und funktionaleren Ausgaben für Web, 3D und verwandte Aufgaben. Kimi K2 ist für agenturische Fähigkeiten optimiert, einschließlich fortgeschrittenem Werkzeuggebrauch, logischem Denken und Codesynthese. Es zeichnet sich in den Benchmarks für Codierung (LiveCodeBench, SWE-Bench), schlussfolgerndes Denken (ZebraLogic, GPQA) und Tool-Nutzung (Tau2, AceBench) aus. Das Modell wird mit einem neuartigen Stack trainiert, der den MuonClip-Optimierer für stabiles MoE-Training in großem Maßstab enthält."},"name":"Kimi K2 Instruct 0905 (exacto)","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"openrouter","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.51762684","output":"2.1567785"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.748Z"},{"id":"kimi-k2-instruct","aliases":["accounts/fireworks/models/kimi-k2-instruct","hf:moonshotai/kimi-k2-instruct","moonshotai/kimi-k2-instruct","kimi-k2-instruct"],"description":{"en":"Kimi K2 is a 32B parameter MoE language model optimized for reasoning, coding, and tool use. It offers variants for fine-tuning and general-purpose tasks with strong performance metrics.","de":"Kimi K2 ist ein 32B-Parameter-MoE-Sprachmodell, das für Argumentation, Kodierung und Werkzeugnutzung optimiert ist. Es bietet Varianten für Feinabstimmung und allgemeine Aufgaben mit starken Leistungsmetriken."},"name":"Kimi K2 Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vultr","contextLength":58904,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"nvidia","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"groq","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.86","output":"2.58"}}},{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.40"},"eur":{"currency":"eur","input":"0.43","output":"2.06"}}},{"providerId":"cortecs","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.65"},"eur":{"currency":"eur","input":"0.47","output":"2.28"}}},{"providerId":"togetherai","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.86","output":"2.58"}}},{"providerId":"huggingface","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.86","output":"2.58"}}},{"providerId":"wandb","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.35","output":"4.00"},"eur":{"currency":"eur","input":"1.16","output":"3.44"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.52","output":"2.15"}}},{"providerId":"deepinfra","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.43","output":"1.72"}}},{"providerId":"fireworks-ai","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.86","output":"2.58"}}}],"lastImportedAt":"2025-12-08T00:21:39.629Z","outputLimit":4096,"contextLength":58904},{"id":"kimi-k2-instruct-0905","aliases":["hf:moonshotai/kimi-k2-instruct-0905","moonshotai/kimi-k2-instruct-0905","kimi-k2-instruct-0905"],"description":{"en":"Kimi K2-Instruct-0905 is a state-of-the-art MoE language model with 32B active parameters, improved coding capabilities, and a context length of 256k tokens for long tasks.","de":"Kimi K2-Instruct-0905 ist ein hochmodernes MoE-Sprachmodell mit 32B aktiven Parametern, verbesserten Kodierungsmöglichkeiten und einer Kontextlänge von 256k Token für lange Aufgaben."},"name":"Kimi K2 0905","toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"nvidia","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"groq","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.86","output":"2.58"}}},{"providerId":"chutes","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.39","output":"1.90"},"eur":{"currency":"eur","input":"0.33","output":"1.63"}}},{"providerId":"baseten","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.52","output":"2.15"}}},{"providerId":"huggingface","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.86","output":"2.58"}}},{"providerId":"synthetic","contextLength":262144,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"1.20","output":"1.20"},"eur":{"currency":"eur","input":"1.03","output":"1.03"}}},{"providerId":"io-net","contextLength":32768,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.39","output":"1.90"},"eur":{"currency":"eur","input":"0.33","output":"1.63"}}}],"lastImportedAt":"2025-12-08T00:21:39.631Z","outputLimit":4096,"contextLength":32768},{"id":"kimi-k2-thinking","aliases":["accounts/fireworks/models/kimi-k2-thinking","moonshotai/kimi-k2-thinking-20251106","hf:moonshotai/kimi-k2-thinking","moonshotai/kimi-k2-thinking","kimi-k2-thinking-20251106","novita/kimi-k2-thinking","kimi-k2-thinking"],"description":{"en":"Kimi K2 Thinking is Moonshot AI’s most advanced open reasoning model to date, extending the K2 series into agentic, long-horizon reasoning. Built on the trillion-parameter Mixture-of-Experts (MoE) architecture introduced in Kimi K2, it activates 32 billion parameters per forward pass and supports 256 k-token context windows. The model is optimized for persistent step-by-step thought, dynamic tool invocation, and complex reasoning workflows that span hundreds of turns. It interleaves step-by-step reasoning with tool use, enabling autonomous research, coding, and writing that can persist for hundreds of sequential actions without drift. It sets new open-source benchmarks on HLE, BrowseComp, SWE-Multilingual, and LiveCodeBench, while maintaining stable multi-agent behavior through 200–300 tool calls. Built on a large-scale MoE architecture with MuonClip optimization, it combines strong reasoning depth with high inference efficiency for demanding agentic and analytical tasks.","de":"Kimi K2 Thinking ist das bisher fortschrittlichste offene Denkmodell von Moonshot AI und erweitert die K2-Serie um agentenbasiertes Denken mit langem Zeithorizont. Es basiert auf der in Kimi K2 eingeführten Billionen-Parameter-Mixture-of-Experts (MoE)-Architektur, aktiviert 32 Milliarden Parameter pro Vorwärtsdurchlauf und unterstützt 256 k-Token-Kontextfenster. Das Modell ist für anhaltendes Schritt-für-Schritt-Denken, dynamische Werkzeugaufrufe und komplexe Schlussfolgerungsworkflows optimiert, die sich über Hunderte von Runden erstrecken. Es verschränkt schrittweises Denken mit der Verwendung von Werkzeugen und ermöglicht so autonomes Forschen, Kodieren und Schreiben, das über Hunderte von aufeinanderfolgenden Aktionen hinweg fortbestehen kann, ohne abzudriften. Es setzt neue Open-Source-Benchmarks auf HLE, BrowseComp, SWE-Multilingual und LiveCodeBench, während es ein stabiles Multi-Agenten-Verhalten über 200-300 Tool-Aufrufe aufrechterhält. Aufgebaut auf einer groß angelegten MoE-Architektur mit MuonClip-Optimierung, kombiniert es eine starke Argumentationstiefe mit hoher Inferenz-Effizienz für anspruchsvolle agenturische und analytische Aufgaben."},"name":"Kimi K2 Thinking","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"moonshotai","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.51","output":"2.14"}}},{"providerId":"nvidia","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"venice","contextLength":262144,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.75","output":"3.20"},"eur":{"currency":"eur","input":"0.64","output":"2.74"}}},{"providerId":"chutes","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.45","output":"2.35"},"eur":{"currency":"eur","input":"0.39","output":"2.01"}}},{"providerId":"kimi-for-coding","contextLength":262144,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"togetherai","contextLength":262144,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"1.20","output":"4.00"},"eur":{"currency":"eur","input":"1.03","output":"3.42"}}},{"providerId":"baseten","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.51","output":"2.14"}}},{"providerId":"helicone","contextLength":256000,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.48","output":"2.00"},"eur":{"currency":"eur","input":"0.41","output":"1.71"}}},{"providerId":"opencode","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.40","output":"2.50"},"eur":{"currency":"eur","input":"0.34","output":"2.14"}}},{"providerId":"openrouter","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.51","output":"2.14"}}},{"providerId":"zenmux","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.51","output":"2.14"}}},{"providerId":"iflowcn","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"synthetic","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.19"},"eur":{"currency":"eur","input":"0.47","output":"1.87"}}},{"providerId":"fireworks-ai","contextLength":256000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.60","output":"2.50"},"eur":{"currency":"eur","input":"0.51","output":"2.14"}}},{"providerId":"io-net","contextLength":32768,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.25"},"eur":{"currency":"eur","input":"0.47","output":"1.93"}}},{"providerId":"poe","contextLength":256000,"price":{"usd":{"currency":"usd"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T12:08:56.152Z","outputLimit":4096,"contextLength":32768},{"id":"kimi-k2-thinking-turbo","aliases":["moonshotai/kimi-k2-thinking-turbo","kimi-k2-thinking-turbo"],"name":"Kimi K2 Thinking Turbo","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"moonshotai","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"1.15","output":"8.00"},"eur":{"currency":"eur","input":"0.99211811","output":"6.9016912"}}},{"providerId":"zenmux","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"1.15","output":"8.00"},"eur":{"currency":"eur","input":"0.99211811","output":"6.9016912"}}}],"lastImportedAt":"2025-11-19T12:06:32.680Z"},{"id":"kimi-k2-thinking:cloud","aliases":["kimi-k2-thinking:cloud"],"name":"Kimi K2 Thinking","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":256000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.330Z","outputLimit":8192,"contextLength":256000},{"id":"kimi-k2:1t-cloud","aliases":["kimi-k2:1t-cloud"],"name":"Kimi K2","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":256000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.810Z","outputLimit":8192,"contextLength":256000},{"id":"kimi-linear-48b-a3b-instruct","aliases":["moonshotai/kimi-linear-48b-a3b-instruct-20251029","moonshotai/kimi-linear-48b-a3b-instruct","kimi-linear-48b-a3b-instruct-20251029","kimi-linear-48b-a3b-instruct"],"name":"MoonshotAI: Kimi Linear 48B A3B Instruct","description":{"en":"Kimi Linear is a hybrid linear attention architecture that outperforms traditional full attention methods across various contexts, including short, long, and reinforcement learning (RL) scaling regimes. At its core is Kimi Delta Attention (KDA)—a refined version of Gated DeltaNet that introduces a more efficient gating mechanism to optimize the use of finite-state RNN memory. Kimi Linear achieves superior performance and hardware efficiency, especially for long-context tasks. It reduces the need for large KV caches by up to 75% and boosts decoding throughput by up to 6x for contexts as long as 1M tokens.","de":"Kimi Linear ist eine hybride lineare Aufmerksamkeitsarchitektur, die herkömmliche volle Aufmerksamkeitsmethoden in verschiedenen Kontexten übertrifft, einschließlich kurzer, langer und Verstärkungslernen (RL)-Skalierungsregime. Das Herzstück ist Kimi Delta Attention (KDA) - eine verfeinerte Version von Gated DeltaNet, die einen effizienteren Gating-Mechanismus einführt, um die Nutzung des RNN-Speichers mit endlichen Zuständen zu optimieren. Kimi Linear erreicht eine überlegene Leistung und Hardware-Effizienz, insbesondere bei Aufgaben mit langem Kontext. Es reduziert den Bedarf an großen KV-Caches um bis zu 75 % und steigert den Dekodierdurchsatz um das bis zu 6-fache für Kontexte mit einer Länge von bis zu 1 Mio. Token."},"knowledge":"2025-11-08","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":1048576,"price":{"usd":{"currency":"usd","input":"0.7","output":"0.9"},"eur":{"currency":"eur","input":"0.6","output":"0.77"}}}],"lastImportedAt":"2025-12-08T00:21:42.509Z","contextLength":1048576},{"id":"l3-euryale-70b","aliases":["sao10k/l3-70b-euryale-v2.1","sao10k/l3-euryale-70b","l3-70b-euryale-v2.1","l3-euryale-70b"],"name":"Sao10k: Llama 3 Euryale 70B v2.1","description":{"en":"Euryale 70B v2.1 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). - Better prompt adherence. - Better anatomy / spatial awareness. - Adapts much better to unique and custom formatting / reply formats. - Very creative, lots of unique swipes. - Is not restrictive during roleplays.","de":"Euryale 70B v2.1 ist ein auf kreatives Rollenspiel ausgerichtetes Modell von [Sao10k] (https://ko-fi.com/sao10k). - Bessere Befolgung der Anweisungen. - Bessere Anatomie / räumliches Bewusstsein. - Passt sich viel besser an einzigartige und benutzerdefinierte Formatierungen / Antwortformate an. - Sehr kreativ, viele einzigartige Swipes. - Ist bei Rollenspielen nicht restriktiv."},"knowledge":"2024-06-18","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","repetition_penalty","seed","stop","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8192,"price":{"usd":{"currency":"usd","input":"1.48","output":"1.48"},"eur":{"currency":"eur","input":"1.276812872","output":"1.276812872"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"l3-lunaris-8b","aliases":["sao10k/l3-8b-lunaris-v1","sao10k/l3-lunaris-8b","l3-8b-lunaris-v1","l3-lunaris-8b"],"name":"Sao10K: Llama 3 8B Lunaris","description":{"en":"Lunaris 8B is a versatile generalist and roleplaying model based on Llama 3. It's a strategic merge of multiple models, designed to balance creativity with improved logic and general knowledge. Created by [Sao10k](https://huggingface.co/Sao10k), this model aims to offer an improved experience over Stheno v3.2, with enhanced creativity and logical reasoning. For best results, use with Llama 3 Instruct context template, temperature 1.4, and min_p 0.1.","de":"Lunaris 8B ist ein vielseitiges Generalisten- und Rollenspielmodell, das auf Llama 3 basiert. Es ist eine strategische Verschmelzung mehrerer Modelle, um Kreativität mit verbesserter Logik und Allgemeinwissen zu verbinden. Erstellt von [Sao10k] (https://huggingface.co/Sao10k), soll dieses Modell eine verbesserte Erfahrung gegenüber Stheno v3.2 bieten, mit verbesserter Kreativität und logischem Denken. Die besten Ergebnisse erzielen Sie mit der Vorlage Llama 3 Instruct context, Temperatur 1,4 und min_p 0,1."},"knowledge":"2024-08-13","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.05"},"eur":{"currency":"eur","input":"0.034508456","output":"0.04313557"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"l3.1-70b-hanami-x1","aliases":["sao10k/l3.1-70b-hanami-x1","l3.1-70b-hanami-x1"],"name":"Sao10K: Llama 3.1 70B Hanami x1","description":{"en":"This is [Sao10K](/sao10k)'s experiment over [Euryale v2.2](/sao10k/l3.1-euryale-70b).","de":"Dies ist das Experiment von [Sao10K](/sao10k) über [Euryale v2.2](/sao10k/l3.1-euryale-70b)."},"knowledge":"2025-01-08","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":16000,"price":{"usd":{"currency":"usd","input":"3","output":"3"},"eur":{"currency":"eur","input":"2.5881342","output":"2.5881342"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"l3.1-euryale-70b","aliases":["sao10k/l3.1-70b-euryale-v2.2","sao10k/l3.1-euryale-70b","l3.1-70b-euryale-v2.2","l3.1-euryale-70b"],"name":"Sao10K: Llama 3.1 Euryale 70B v2.2","description":{"en":"Euryale L3.1 70B v2.2 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.1](/models/sao10k/l3-euryale-70b).","de":"Euryale L3.1 70B v2.2 ist ein auf kreatives Rollenspiel ausgerichtetes Modell von [Sao10k](https://ko-fi.com/sao10k). Es ist der Nachfolger von [Euryale L3 70B v2.1](/models/sao10k/l3-euryale-70b)."},"knowledge":"2024-08-28","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.65","output":"0.75"},"eur":{"currency":"eur","input":"0.56076241","output":"0.64703355"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"l3.3-euryale-70b","aliases":["sao10k/l3.3-euryale-70b-v2.3","sao10k/l3.3-70b-euryale-v2.3","sao10k/l3.3-euryale-70b","l3.3-euryale-70b-v2.3","l3.3-70b-euryale-v2.3","l3.3-euryale-70b"],"name":"Sao10K: Llama 3.3 Euryale 70B","description":{"en":"Euryale L3.3 70B is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.2](/models/sao10k/l3-euryale-70b).","de":"Euryale L3.3 70B ist ein auf kreatives Rollenspiel ausgerichtetes Modell von [Sao10k](https://ko-fi.com/sao10k). Es ist der Nachfolger von [Euryale L3 70B v2.2](/models/sao10k/l3-euryale-70b)."},"knowledge":"2024-12-18","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.65","output":"0.75"},"eur":{"currency":"eur","input":"0.56076241","output":"0.64703355"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"lfm-2.2-6b","aliases":["liquidai/lfm2-2.6b","liquid/lfm-2.2-6b","lfm-2.2-6b","lfm2-2.6b"],"name":"LiquidAI/LFM2-2.6B","description":{"en":"LFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.","de":"LFM2 ist eine neue Generation von Hybridmodellen, die von Liquid AI entwickelt wurden und speziell für Edge AI und den Einsatz auf Geräten konzipiert sind. Sie setzt einen neuen Standard in Bezug auf Qualität, Geschwindigkeit und Speichereffizienz."},"knowledge":"2025-10-20","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.1"},"eur":{"currency":"eur","input":"0.04313557","output":"0.08627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.762Z"},{"id":"lfm2-8b-a1b","aliases":["liquidai/lfm2-8b-a1b","liquid/lfm2-8b-a1b","lfm2-8b-a1b"],"name":"LiquidAI/LFM2-8B-A1B","description":{"en":"Model created via inbox interface","de":"Modell über Posteingangsschnittstelle erstellt"},"knowledge":"2025-10-20","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.1"},"eur":{"currency":"eur","input":"0.04313557","output":"0.08627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.762Z"},{"id":"ling-1t","aliases":["ling-1t"],"name":"Ling-1T","toolCalling":true,"openWeights":true,"knowledge":"2024-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"bailing","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.57","output":"2.29"},"eur":{"currency":"eur","input":"0.49","output":"1.99"}}}],"lastImportedAt":"2025-11-27T00:20:22.890Z","outputLimit":32000,"contextLength":128000},{"id":"lint-1t","aliases":["inclusionai/lint-1t","lint-1t"],"name":"Ling-1T","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"zenmux","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.56","output":"2.24"},"eur":{"currency":"eur","input":"0.483118384","output":"1.932473536"}}}],"lastImportedAt":"2025-11-19T12:06:32.752Z"},{"id":"llama-2-13b-chat-awq","aliases":["llama-2-13b-chat-awq"],"name":"@hf/thebloke/llama-2-13b-chat-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.457Z","outputLimit":4096,"contextLength":4096},{"id":"llama-2-7b-chat-fp16","aliases":["workers-ai/llama-2-7b-chat-fp16","llama-2-7b-chat-fp16"],"name":"@cf/meta/llama-2-7b-chat-fp16","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.56","output":"6.67"},"eur":{"currency":"eur","input":"0.48","output":"5.73"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"6.67"},"eur":{"currency":"eur","input":"0","output":"5.73"}}}],"lastImportedAt":"2025-12-08T00:21:40.579Z","outputLimit":4096,"contextLength":4096},{"id":"llama-2-7b-chat-hf-lora","aliases":["llama-2-7b-chat-hf-lora"],"name":"@cf/meta-llama/llama-2-7b-chat-hf-lora","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.676Z","outputLimit":8192,"contextLength":8192},{"id":"llama-2-7b-chat-int8","aliases":["llama-2-7b-chat-int8"],"name":"@cf/meta/llama-2-7b-chat-int8","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.56","output":"6.67"},"eur":{"currency":"eur","input":"0.48","output":"5.73"}}}],"lastImportedAt":"2025-12-08T00:21:41.539Z","outputLimit":8192,"contextLength":8192},{"id":"llama-3_1-405b-instruct","aliases":["meta-llama/meta-llama-3.1-405b-instruct","hf:meta-llama/llama-3.1-405b-instruct","meta-llama/llama-3_1-405b-instruct","meta-llama/llama-3.1-405b-instruct","meta-llama-3.1-405b-instruct","llama-3_1-405b-instruct","llama-3.1-405b-instruct"],"description":{"en":"The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs. Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations. To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","de":"Die mit Spannung erwartete 400B-Klasse von Llama3 ist da! Mit einer Taktrate von 128k Context und beeindruckenden Eval-Scores setzt das Meta AI-Team die Grenzen der Open-Source-LLMs weiter nach oben. Metas neueste Modellklasse (Llama 3.1) ist in verschiedenen Größen und Geschmacksrichtungen erhältlich. Diese 405B instruct-tuned Version ist für hochqualitative Dialoge optimiert. Es hat in Evaluierungen eine starke Leistung im Vergleich zu führenden Closed-Source-Modellen wie GPT-4o und Claude 3.5 Sonnet gezeigt. Um mehr über die Veröffentlichung des Modells zu erfahren, [klicken Sie hier] (https://ai.meta.com/blog/meta-llama-3-1/). Die Nutzung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."},"name":"Llama 3.1 405B Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.00","output":"3.00"},"eur":{"currency":"eur","input":"0.8627114","output":"2.5881342"}}},{"providerId":"cortecs","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"3.00","output":"3.00"},"eur":{"currency":"eur","input":"2.5881342","output":"2.5881342"}}},{"providerId":"openrouter","contextLength":130815,"price":{"usd":{"currency":"usd","input":"3.5","output":"3.5"},"eur":{"currency":"eur","input":"3.0194899","output":"3.0194899"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.713Z"},{"id":"llama-3-70b-instruct","aliases":["meta-llama/meta-llama-3-70b-instruct","meta-llama/llama-3-70b-instruct","meta-llama-3-70b-instruct","llama-3-70b-instruct"],"name":"Meta: Llama 3 70B Instruct","description":{"en":"Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 70B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations. To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","de":"Metas neueste Modellklasse (Llama 3) wurde mit einer Vielzahl von Größen und Geschmacksrichtungen eingeführt. Diese 70B instruct-tuned Version wurde für hochwertige Dialoge optimiert. Im Vergleich zu führenden Closed-Source-Modellen hat es in menschlichen Evaluierungen eine starke Leistung gezeigt. Um mehr über die Veröffentlichung des Modells zu erfahren, [klicken Sie hier] (https://ai.meta.com/blog/meta-llama-3/). Die Nutzung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."},"knowledge":"2024-04-18","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0.3","output":"0.4"},"eur":{"currency":"eur","input":"0.25881342","output":"0.34508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"llama-3-8b-instruct","aliases":["meta-llama/meta-llama-3-8b-instruct","workers-ai/llama-3-8b-instruct","meta-llama/llama-3-8b-instruct","meta-llama-3-8b-instruct","llama-3-8b-instruct"],"description":{"en":"Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 8B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations. To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","de":"Metas neueste Modellklasse (Llama 3) wurde mit einer Vielzahl von Größen und Geschmacksrichtungen eingeführt. Diese 8B instruct-tuned Version wurde für hochwertige Dialoge optimiert. Im Vergleich zu führenden Closed-Source-Modellen hat sie in menschlichen Evaluierungen eine starke Leistung gezeigt. Um mehr über die Veröffentlichung des Modells zu erfahren, [klicken Sie hier] (https://ai.meta.com/blog/meta-llama-3/). Die Verwendung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."},"name":"@cf/meta/llama-3-8b-instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-18","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":7968,"outputLimit":7968,"price":{"usd":{"currency":"usd","input":"0.28","output":"0.83"},"eur":{"currency":"eur","input":"0.24","output":"0.71"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.06"},"eur":{"currency":"eur","input":"0.03","output":"0.05"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:40.676Z","outputLimit":7968,"contextLength":7968},{"id":"llama-3-8b-instruct-awq","aliases":["workers-ai/llama-3-8b-instruct-awq","llama-3-8b-instruct-awq"],"name":"@cf/meta/llama-3-8b-instruct-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.12","output":"0.27"},"eur":{"currency":"eur","input":"0.1","output":"0.23"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.428Z","outputLimit":8192,"contextLength":8192},{"id":"llama-3.1-405b","aliases":["meta-llama/llama-3.1-405b","llama-3.1-405b"],"name":"Meta: Llama 3.1 405B (base)","description":{"en":"Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This is the base 405B pre-trained version. It has demonstrated strong performance compared to leading closed-source models in human evaluations. To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","de":"Metas neueste Modellklasse (Llama 3.1) wurde in verschiedenen Größen und Geschmacksrichtungen eingeführt. Dies ist die Basisversion 405B, die vortrainiert ist. Im Vergleich zu führenden Closed-Source-Modellen hat es bei menschlichen Bewertungen eine starke Leistung gezeigt. Um mehr über die Veröffentlichung des Modells zu erfahren, [klicken Sie hier] (https://ai.meta.com/blog/meta-llama-3/). Die Verwendung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."},"knowledge":"2024-08-02","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"4","output":"4"},"eur":{"currency":"eur","input":"3.43","output":"3.43"}}}],"lastImportedAt":"2025-12-10T00:21:34.675Z","contextLength":32768},{"id":"llama-3.1-70b","aliases":["facebook/llama-3.1-70b","llama-3.1-70b"],"name":"Llama-3.1-70B","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0.90","output":"0.90"},"eur":{"currency":"eur","input":"0.78","output":"0.78"}}}],"lastImportedAt":"2025-11-21T07:05:34.586Z","contextLength":8192,"deprecated":true},{"id":"llama-3.1-70b-instruct","aliases":["meta-llama/meta-llama-3.1-70b-instruct","hf:meta-llama/llama-3.1-70b-instruct","meta-llama/llama-3.1-70b-instruct","meta-llama-3.1-70b-instruct","llama-3.1-70b-instruct"],"description":{"en":"Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 70B instruct-tuned version is optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations. To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","de":"Metas neueste Modellklasse (Llama 3.1) ist mit einer Vielzahl von Größen und Geschmacksrichtungen auf den Markt gekommen. Diese auf 70B abgestimmte Version ist für den Einsatz in hochwertigen Dialogen optimiert. Im Vergleich zu führenden Closed-Source-Modellen hat es in menschlichen Evaluierungen eine starke Leistung gezeigt. Um mehr über die Veröffentlichung des Modells zu erfahren, [klicken Sie hier] (https://ai.meta.com/blog/meta-llama-3-1/). Die Nutzung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."},"name":"@cf/meta/llama-3.1-70b-instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":24000,"outputLimit":24000,"price":{"usd":{"currency":"usd","input":"0.29","output":"2.25"},"eur":{"currency":"eur","input":"0.25","output":"1.93"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.90","output":"0.90"},"eur":{"currency":"eur","input":"0.77","output":"0.77"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.4","output":"0.4"},"eur":{"currency":"eur","input":"0.34","output":"0.34"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:41.854Z","outputLimit":24000,"contextLength":24000},{"id":"llama-3.1-8b","aliases":["facebook/llama-3.1-8b","llama-3.1-8b"],"name":"Llama-3.1-8B","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}}],"lastImportedAt":"2025-11-21T07:05:34.356Z","contextLength":8192,"deprecated":true},{"id":"llama-3.1-8b-instant","aliases":["llama-3.1-8b-instant"],"name":"Llama 3.1 8B Instant","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"groq","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.08"},"eur":{"currency":"eur","input":"0.04","output":"0.07"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":32678,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.08"},"eur":{"currency":"eur","input":"0.04","output":"0.07"}}}],"lastImportedAt":"2025-12-09T00:20:58.280Z","outputLimit":8192,"contextLength":131072},{"id":"llama-3.1-8b-instruct","aliases":["meta-llama/meta-llama-3.1-8b-instruct","hf:meta-llama/llama-3.1-8b-instruct","meta-llama/llama-3.1-8b-instruct","workers-ai/llama-3.1-8b-instruct","meta/llama-3.1-8b-instruct","meta-llama-3.1-8b-instruct","llama-3.1-8b-instruct"],"description":{"en":"Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruct-tuned version is fast and efficient. It has demonstrated strong performance compared to leading closed-source models in human evaluations. To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","de":"Metas neueste Modellklasse (Llama 3.1) ist mit einer Vielzahl von Größen und Geschmacksrichtungen auf den Markt gekommen. Diese 8B instruct-tuned Version ist schnell und effizient. Sie hat im Vergleich zu führenden Closed-Source-Modellen in menschlichen Bewertungen eine starke Leistung gezeigt. Um mehr über die Veröffentlichung des Modells zu erfahren, [klicken Sie hier](https://ai.meta.com/blog/meta-llama-3-1/). Die Nutzung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."},"name":"Meta Llama 3.1 8B Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"helicone","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.05"},"eur":{"currency":"eur","input":"0.02","output":"0.04"}}},{"providerId":"cloudflare-workers-ai","contextLength":7968,"outputLimit":7968,"price":{"usd":{"currency":"usd","input":"0.28","output":"0.83"},"eur":{"currency":"eur","input":"0.24","output":"0.71"}}},{"providerId":"wandb","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.22","output":"0.22"},"eur":{"currency":"eur","input":"0.19","output":"0.19"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"ovhcloud","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.11","output":"0.11"},"eur":{"currency":"eur","input":"0.09","output":"0.09"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"inference","contextLength":16000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.03"},"eur":{"currency":"eur","input":"0.03","output":"0.03"}}},{"providerId":"scaleway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.03"},"eur":{"currency":"eur","input":"0.02","output":"0.03"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.666Z","outputLimit":4096,"contextLength":7968},{"id":"llama-3.1-8b-instruct-awq","aliases":["workers-ai/llama-3.1-8b-instruct-awq","llama-3.1-8b-instruct-awq"],"name":"@cf/meta/llama-3.1-8b-instruct-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.12","output":"0.27"},"eur":{"currency":"eur","input":"0.1","output":"0.23"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.885Z","outputLimit":8192,"contextLength":8192},{"id":"llama-3.1-8b-instruct-fast","aliases":["llama-3.1-8b-instruct-fast"],"name":"@cf/meta/llama-3.1-8b-instruct-fast","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.38"},"eur":{"currency":"eur","input":"0.03","output":"0.33"}}}],"lastImportedAt":"2025-12-08T00:21:41.800Z","outputLimit":128000,"contextLength":128000},{"id":"llama-3.1-8b-instruct-fp8","aliases":["workers-ai/llama-3.1-8b-instruct-fp8","llama-3.1-8b-instruct-fp8"],"name":"@cf/meta/llama-3.1-8b-instruct-fp8","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.29"},"eur":{"currency":"eur","input":"0.13","output":"0.25"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.474Z","outputLimit":16384,"contextLength":32000},{"id":"llama-3.1-8b-instruct-turbo","aliases":["llama-3.1-8b-instruct-turbo"],"name":"Meta Llama 3.1 8B Instruct Turbo","toolCalling":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"helicone","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.03"},"eur":{"currency":"eur","input":"0.02","output":"0.03"}}}],"lastImportedAt":"2025-12-09T00:20:58.554Z","outputLimit":128000,"contextLength":128000},{"id":"llama-3.1-lumimaid-8b","aliases":["neversleep/llama-3.1-lumimaid-8b","neversleep/lumimaid-v0.2-8b","llama-3.1-lumimaid-8b","lumimaid-v0.2-8b"],"name":"NeverSleep: Lumimaid v0.2 8B","description":{"en":"Lumimaid v0.2 8B is a finetune of [Llama 3.1 8B](/models/meta-llama/llama-3.1-8b-instruct) with a \"HUGE step up dataset wise\" compared to Lumimaid v0.1. Sloppy chats output were purged. Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","de":"Lumimaid v0.2 8B ist eine Feinabstimmung von [Llama 3.1 8B](/models/meta-llama/llama-3.1-8b-instruct) mit einem \"RIESIGEN Schritt nach oben in Bezug auf die Datenmenge\" im Vergleich zu Lumimaid v0.1. Schlampige Chats wurden bereinigt. Die Nutzung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."},"knowledge":"2024-09-15","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_a","top_k","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.6"},"eur":{"currency":"eur","input":"0.077644026","output":"0.51762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"llama-3.1-nemotron-70b-instruct","aliases":["nvidia/llama-3.1-nemotron-70b-instruct-hf","nvidia/llama-3.1-nemotron-70b-instruct","llama-3.1-nemotron-70b-instruct-hf","llama-3.1-nemotron-70b-instruct"],"name":"NVIDIA: Llama 3.1 Nemotron 70B Instruct","description":{"en":"NVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains. Usage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).","de":"NVIDIAs Llama 3.1 Nemotron 70B ist ein Sprachmodell, das für die Erzeugung präziser und nützlicher Antworten entwickelt wurde. Es nutzt die [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct)-Architektur und das Verstärkungslernen aus menschlichem Feedback (RLHF), um bei automatischen Ausrichtungsbenchmarks zu glänzen. Dieses Modell ist auf Anwendungen zugeschnitten, die eine hohe Genauigkeit bei der Hilfestellung und der Generierung von Antworten erfordern, und eignet sich für verschiedene Benutzeranfragen in unterschiedlichen Domänen. Die Nutzung dieses Modells unterliegt der [Meta's Acceptable Use Policy] (https://www.llama.com/llama3/use-policy/)."},"knowledge":"2024-10-15","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"1.2","output":"1.2"},"eur":{"currency":"eur","input":"1.03525368","output":"1.03525368"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"llama-3.1-nemotron-ultra-253b-v1","aliases":["nvidia/llama-3.1-nemotron-ultra-253b-v1","nvidia/llama-3_1-nemotron-ultra-253b-v1","llama-3.1-nemotron-ultra-253b-v1","llama-3_1-nemotron-ultra-253b-v1"],"description":{"en":"Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, human-interactive chat, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta’s Llama-3.1-405B-Instruct, it has been significantly customized using Neural Architecture Search (NAS), resulting in enhanced efficiency, reduced memory usage, and improved inference latency. The model supports a context length of up to 128K tokens and can operate efficiently on an 8x NVIDIA H100 node. Note: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.","de":"Llama-3.1-Nemotron-Ultra-253B-v1 ist ein großes Sprachmodell (Large Language Model, LLM), das für fortgeschrittenes Reasoning, human-interaktiven Chat, Retrieval-Augmented Generation (RAG) und Tool-Calling-Aufgaben optimiert wurde. Es wurde von Meta's Llama-3.1-405B-Instruct abgeleitet und mit Hilfe von Neural Architecture Search (NAS) erheblich angepasst, was zu erhöhter Effizienz, reduziertem Speicherverbrauch und verbesserter Inferenzlatenz führt. Das Modell unterstützt eine Kontextlänge von bis zu 128K Token und kann effizient auf einem 8x NVIDIA H100 Knoten betrieben werden. Hinweis: Sie müssen `detailed thinking on` in der System-Eingabeaufforderung angeben, um Schlussfolgerungen zu ermöglichen. Weitere Informationen finden Sie unter [Verwendungsempfehlungen] (https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations)."},"name":"Llama-3.1-Nemotron-Ultra-253B-v1","reasoning":true,"toolCalling":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","structured_outputs","top_k","top_p"],"providers":[{"providerId":"nvidia","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.60","output":"1.80"},"eur":{"currency":"eur","input":"0.51762684","output":"1.55288052"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.6","output":"1.8"},"eur":{"currency":"eur","input":"0.51762684","output":"1.55288052"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.693Z"},{"id":"llama-3.2-11b-vision-instruct","aliases":["workers-ai/llama-3.2-11b-vision-instruct","meta-llama/llama-3.2-11b-vision-instruct","meta/llama-3.2-11b-vision-instruct","llama-3.2-11b-vision-instruct"],"description":{"en":"Llama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis. Its ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research. Click here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md). Usage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).","de":"Llama 3.2 11B Vision ist ein multimodales Modell mit 11 Milliarden Parametern, das für die Bearbeitung von Aufgaben entwickelt wurde, die visuelle und textuelle Daten kombinieren. Es zeichnet sich durch Aufgaben wie Bildunterschriften und die Beantwortung visueller Fragen aus und überbrückt die Lücke zwischen Sprachgenerierung und visuellem Denken. Es wurde mit einem umfangreichen Datensatz von Bild-Text-Paaren trainiert und erbringt gute Leistungen bei komplexen, hochpräzisen Bildanalysen. Seine Fähigkeit, visuelles Verständnis mit Sprachverarbeitung zu verbinden, macht es zu einer idealen Lösung für Branchen, die umfassende visuell-linguistische KI-Anwendungen benötigen, wie z. B. die Erstellung von Inhalten, KI-gesteuerter Kundenservice und Forschung. Klicken Sie hier für die [Originalmodellkarte](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md). Die Nutzung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/)."},"name":"Llama-3.2-11B-Vision-Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text","image","audio"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","top_k","top_p"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.37","output":"0.37"},"eur":{"currency":"eur","input":"0.32","output":"0.32"}}},{"providerId":"cloudflare-workers-ai","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.68"},"eur":{"currency":"eur","input":"0.04","output":"0.58"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.049","output":"0.049"},"eur":{"currency":"eur","input":"0.04","output":"0.04"}}},{"providerId":"inference","contextLength":16000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.06"},"eur":{"currency":"eur","input":"0.05","output":"0.05"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.37","output":"0.37"},"eur":{"currency":"eur","input":"0.32","output":"0.32"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.999Z","outputLimit":4096,"contextLength":16000},{"id":"llama-3.2-1b-instruct","aliases":["workers-ai/llama-3.2-1b-instruct","meta-llama/llama-3.2-1b-instruct","meta/llama-3.2-1b-instruct","llama-3.2-1b-instruct"],"description":{"en":"Llama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance. Supporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models. Click here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md). Usage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).","de":"Llama 3.2 1B ist ein Sprachmodell mit 1 Milliarde Parametern, das sich auf die effiziente Durchführung von Aufgaben der natürlichen Sprache konzentriert, wie z.B. Zusammenfassungen, Dialoge und mehrsprachige Textanalysen. Dank seiner geringen Größe kann es auch in Umgebungen mit geringen Ressourcen effizient arbeiten und gleichzeitig eine hohe Aufgabenleistung beibehalten. Llama 1.3B unterstützt acht Kernsprachen und kann für weitere Sprachen feinabgestimmt werden. Damit ist es ideal für Unternehmen oder Entwickler, die leichtgewichtige und dennoch leistungsstarke KI-Lösungen suchen, die in verschiedenen mehrsprachigen Umgebungen ohne den hohen Rechenaufwand größerer Modelle eingesetzt werden können. Klicken Sie hier für die [Originalmodellkarte](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md). Die Nutzung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/)."},"name":"@cf/meta/llama-3.2-1b-instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","seed","top_k","top_p"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":60000,"outputLimit":60000,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.20"},"eur":{"currency":"eur","input":"0.03","output":"0.17"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"inference","contextLength":16000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.01","output":"0.01"},"eur":{"currency":"eur","input":"0.01","output":"0.01"}}},{"providerId":"openrouter","contextLength":60000,"price":{"usd":{"currency":"usd","input":"0.027","output":"0.2"},"eur":{"currency":"eur","input":"0.02","output":"0.17"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:41.555Z","outputLimit":4096,"contextLength":16000},{"id":"llama-3.2-3b","aliases":["llama-3.2-3b"],"name":"Llama 3.2 3B","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"venice","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.12940671","output":"0.51762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.717Z"},{"id":"llama-3.2-3b-instruct","aliases":["meta-llama/llama-3.2-3b-instruct:free","workers-ai/llama-3.2-3b-instruct","meta-llama/llama-3.2-3b-instruct","meta/llama-3.2-3b-instruct","llama-3.2-3b-instruct:free","llama-3.2-3b-instruct"],"description":{"en":"Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Designed with the latest transformer architecture, it supports eight languages, including English, Spanish, and Hindi, and is adaptable for additional languages. Trained on 9 trillion tokens, the Llama 3.2 3B model excels in instruction-following, complex reasoning, and tool use. Its balanced performance makes it ideal for applications needing accuracy and efficiency in text generation across multilingual settings. Click here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md). Usage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).","de":"Llama 3.2 3B ist ein mehrsprachiges großes Sprachmodell mit 3 Milliarden Parametern, das für fortgeschrittene Aufgaben der natürlichen Sprachverarbeitung wie Dialoggenerierung, Schlussfolgerungen und Zusammenfassungen optimiert ist. Es wurde mit der neuesten Transformer-Architektur entwickelt und unterstützt acht Sprachen, darunter Englisch, Spanisch und Hindi, und ist für weitere Sprachen anpassbar. Das Modell Llama 3.2 3B wurde auf der Grundlage von 9 Billionen Token trainiert und zeichnet sich durch die Befolgung von Anweisungen, komplexe Schlussfolgerungen und die Verwendung von Tools aus. Seine ausgewogene Leistung macht es ideal für Anwendungen, die Genauigkeit und Effizienz bei der Texterstellung in mehrsprachigen Umgebungen erfordern. Klicken Sie hier für die [Original-Modellkarte](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md). Die Nutzung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/)."},"name":"@cf/meta/llama-3.2-3b-instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","stop","top_k","top_p","logit_bias","min_p","repetition_penalty","response_format","seed","tool_choice"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.34"},"eur":{"currency":"eur","input":"0.04","output":"0.29"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"inference","contextLength":16000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.02"},"eur":{"currency":"eur","input":"0.02","output":"0.02"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.02"},"eur":{"currency":"eur","input":"0.02","output":"0.02"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":131072,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:41.026Z","outputLimit":4096,"contextLength":16000},{"id":"llama-3.2-90b-vision-instruct","aliases":["meta-llama/llama-3.2-90b-vision-instruct","meta/llama-3.2-90b-vision-instruct","llama-3.2-90b-vision-instruct"],"description":{"en":"The Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks. This model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis. Click here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md). Usage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).","de":"Das Llama 90B Vision-Modell ist ein multimodales Modell der Spitzenklasse mit 90 Milliarden Parametern, das für die anspruchsvollsten visuellen Schlussfolgerungen und Sprachaufgaben entwickelt wurde. Es bietet eine unvergleichliche Genauigkeit bei Bildunterschriften, der Beantwortung visueller Fragen und einem erweiterten Bild-Text-Verständnis. Das Llama 90B Vision wurde mit umfangreichen multimodalen Datensätzen trainiert und mit menschlichem Feedback fein abgestimmt, um die anspruchsvollsten bildbasierten KI-Aufgaben zu bewältigen. Dieses Modell eignet sich perfekt für Branchen, die modernste multimodale KI-Funktionen benötigen, insbesondere für solche, die komplexe visuelle und textuelle Analysen in Echtzeit durchführen. Klicken Sie hier für die [Originalmodellkarte](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md). Die Nutzung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/)."},"name":"Llama-3.2-90B-Vision-Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text","image","audio"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","top_k","top_p"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.04","output":"2.04"},"eur":{"currency":"eur","input":"1.76","output":"1.76"}}},{"providerId":"io-net","contextLength":16000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.35","output":"0.40"},"eur":{"currency":"eur","input":"0.3","output":"0.35"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.04","output":"2.04"},"eur":{"currency":"eur","input":"1.76","output":"1.76"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.35","output":"0.4"},"eur":{"currency":"eur","input":"0.3","output":"0.35"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-30T12:07:57.509Z","outputLimit":4096,"contextLength":16000},{"id":"llama-3.3-70b","aliases":["meta/llama-3.3-70b","llama-3.3-70b"],"name":"Llama-3.3-70B-Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"venice","contextLength":65536,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.70","output":"2.80"},"eur":{"currency":"eur","input":"0.60389798","output":"2.41559192"}}}],"lastImportedAt":"2025-11-19T12:06:32.712Z"},{"id":"llama-3.3-70b-instruct","aliases":["meta-llama/llama-3.3-70b-instruct:free","hf:meta-llama/llama-3.3-70b-instruct","meta-llama/llama-3.3-70b-instruct","meta/llama-3.3-70b-instruct","llama-3.3-70b-instruct:free","llama-3.3-70b-instruct"],"description":{"en":"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks. Supported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. [Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)","de":"Das Meta Llama 3.3 multilinguale große Sprachmodell (LLM) ist ein vortrainiertes und anweisungsabgestimmtes generatives Modell in 70B (Text in/Text out). Das Llama 3.3 anweisungsabgestimmte Nur-Text-Modell ist für mehrsprachige Dialoganwendungsfälle optimiert und übertrifft viele der verfügbaren Open-Source- und geschlossenen Chat-Modelle bei gängigen Industrie-Benchmarks. Unterstützte Sprachen: Englisch, Deutsch, Französisch, Italienisch, Portugiesisch, Hindi, Spanisch und Thai. [Modellkarte](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)"},"name":"Llama-3.3-70B-Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","seed","stop","tool_choice","top_k","top_p","logit_bias","logprobs","min_p","response_format","structured_outputs","top_logprobs"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.71","output":"0.71"},"eur":{"currency":"eur","input":"0.61","output":"0.61"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":16400,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.39"},"eur":{"currency":"eur","input":"0.11","output":"0.33"}}},{"providerId":"wandb","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.71","output":"0.71"},"eur":{"currency":"eur","input":"0.61","output":"0.61"}}},{"providerId":"synthetic","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.90","output":"0.90"},"eur":{"currency":"eur","input":"0.77","output":"0.77"}}},{"providerId":"io-net","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.38"},"eur":{"currency":"eur","input":"0.11","output":"0.33"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.71","output":"0.71"},"eur":{"currency":"eur","input":"0.61","output":"0.61"}}},{"providerId":"llama","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"scaleway","contextLength":100000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.90","output":"0.90"},"eur":{"currency":"eur","input":"0.77","output":"0.77"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.32"},"eur":{"currency":"eur","input":"0.09","output":"0.27"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":65536,"outputLimit":65536,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-11T12:09:14.416Z","outputLimit":4096,"contextLength":100000},{"id":"llama-3.3-70b-instruct-base","aliases":["meta-llama/llama-3.3-70b-instruct-base","llama-3.3-70b-instruct-base"],"name":"Llama-3.3-70B-Instruct (Base)","reasoning":true,"toolCalling":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.40"},"eur":{"currency":"eur","input":"0.112152482","output":"0.34508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.713Z"},{"id":"llama-3.3-70b-instruct-fast","aliases":["meta-llama/llama-3.3-70b-instruct-fast","llama-3.3-70b-instruct-fast"],"name":"Llama-3.3-70B-Instruct (Fast)","reasoning":true,"toolCalling":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"nebius","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.25","output":"0.75"},"eur":{"currency":"eur","input":"0.21567785","output":"0.64703355"}}}],"lastImportedAt":"2025-11-19T12:06:32.713Z"},{"id":"llama-3.3-70b-instruct-fp8-fast","aliases":["workers-ai/llama-3.3-70b-instruct-fp8-fast","llama-3.3-70b-instruct-fp8-fast"],"name":"@cf/meta/llama-3.3-70b-instruct-fp8-fast","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":24000,"outputLimit":24000,"price":{"usd":{"currency":"usd","input":"0.29","output":"2.25"},"eur":{"currency":"eur","input":"0.25","output":"1.93"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"2.25"},"eur":{"currency":"eur","input":"0","output":"1.93"}}}],"lastImportedAt":"2025-12-08T00:21:41.379Z","outputLimit":16384,"contextLength":24000},{"id":"llama-3.3-70b-instruct-turbo","aliases":["meta-llama/llama-3.3-70b-instruct-turbo","llama-3.3-70b-instruct-turbo"],"name":"Llama 3.3 70B","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"togetherai","contextLength":131072,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.88","output":"0.88"},"eur":{"currency":"eur","input":"0.759186032","output":"0.759186032"}}}],"lastImportedAt":"2025-11-19T12:06:32.726Z"},{"id":"llama-3.3-70b-versatile","aliases":["llama-3.3-70b-versatile"],"name":"Llama 3.3 70B Versatile","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"groq","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.59","output":"0.79"},"eur":{"currency":"eur","input":"0.51","output":"0.68"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":32678,"price":{"usd":{"currency":"usd","input":"0.59","output":"0.79"},"eur":{"currency":"eur","input":"0.51","output":"0.68"}}}],"lastImportedAt":"2025-12-09T00:20:58.307Z","outputLimit":32678,"contextLength":131072},{"id":"llama-3.3-8b-instruct","aliases":["llama-3.3-8b-instruct"],"name":"Llama-3.3-8B-Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"llama","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.756Z"},{"id":"llama-3.3-nemotron-super-49b-v1.5","aliases":["nvidia/llama-3.3-nemotron-super-49b-v1.5","nvidia/llama-3_3-nemotron-super-49b-v1_5","llama-3.3-nemotron-super-49b-v1.5","llama-3_3-nemotron-super-49b-v1_5"],"name":"NVIDIA: Llama 3.3 Nemotron Super 49B V1.5","description":{"en":"Llama-3.3-Nemotron-Super-49B-v1.5 is a 49B-parameter, English-centric reasoning/chat model derived from Meta’s Llama-3.3-70B-Instruct with a 128K context. It’s post-trained for agentic workflows (RAG, tool calling) via SFT across math, code, science, and multi-turn chat, followed by multiple RL stages; Reward-aware Preference Optimization (RPO) for alignment, RL with Verifiable Rewards (RLVR) for step-wise reasoning, and iterative DPO to refine tool-use behavior. A distillation-driven Neural Architecture Search (“Puzzle”) replaces some attention blocks and varies FFN widths to shrink memory footprint and improve throughput, enabling single-GPU (H100/H200) deployment while preserving instruction following and CoT quality. In internal evaluations (NeMo-Skills, up to 16 runs, temp = 0.6, top_p = 0.95), the model reports strong reasoning/coding results, e.g., MATH500 pass@1 = 97.4, AIME-2024 = 87.5, AIME-2025 = 82.71, GPQA = 71.97, LiveCodeBench (24.10–25.02) = 73.58, and MMLU-Pro (CoT) = 79.53. The model targets practical inference efficiency (high tokens/s, reduced VRAM) with Transformers/vLLM support and explicit “reasoning on/off” modes (chat-first defaults, greedy recommended when disabled). Suitable for building agents, assistants, and long-context retrieval systems where balanced accuracy-to-cost and reliable tool use matter.","de":"Llama-3.3-Nemotron-Super-49B-v1.5 ist ein englischsprachiges Argumentations-/Chatmodell mit 49B-Parametern, das von Meta's Llama-3.3-70B-Instruct mit einem 128K-Kontext abgeleitet ist. Es wird für agenturische Arbeitsabläufe (RAG, Werkzeugaufrufe) über SFT in den Bereichen Mathematik, Code, Wissenschaft und Multi-Turn-Chat nachtrainiert, gefolgt von mehreren RL-Stufen: Reward-aware Preference Optimization (RPO) für die Ausrichtung, RL with Verifiable Rewards (RLVR) für schrittweises Denken und iterative DPO zur Verfeinerung des Werkzeuggebrauchsverhaltens. Eine destillationsgesteuerte neuronale Architektursuche (\"Puzzle\") ersetzt einige Aufmerksamkeitsblöcke und variiert die FFN-Breiten, um den Speicherplatzbedarf zu verringern und den Durchsatz zu verbessern, was den Einsatz von Single-GPUs (H100/H200) unter Beibehaltung der Befehlsfolge und der CoT-Qualität ermöglicht. In internen Evaluierungen (NeMo-Skills, bis zu 16 Läufe, temp = 0.6, top_p = 0.95) meldet das Modell starke Schlussfolgerungs-/Codierungsergebnisse, z.B. MATH500 pass@1 = 97.4, AIME-2024 = 87.5, AIME-2025 = 82.71, GPQA = 71.97, LiveCodeBench (24.10-25.02) = 73.58, und MMLU-Pro (CoT) = 79.53. Das Modell zielt auf praktische Inferenz-Effizienz (hohe Token/s, reduzierter VRAM) mit Transformers/vLLM-Unterstützung und expliziten \"Reasoning on/off\"-Modi (Chat-first-Standardwerte, greedy empfohlen, wenn deaktiviert). Es eignet sich für die Entwicklung von Agenten, Assistenten und Systemen für die Suche nach langen Kontexten, bei denen ein ausgewogenes Verhältnis zwischen Genauigkeit und Kosten und ein zuverlässiger Einsatz von Tools wichtig sind."},"knowledge":"2025-10-10","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","temperature","tool_choice","tools","top_k","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.4"},"eur":{"currency":"eur","input":"0.08627114","output":"0.34508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.763Z"},{"id":"llama-4-maverick","aliases":["meta-llama/llama-4-maverick-17b-128e-instruct","llama-4-maverick-17b-128e-instruct","meta-llama/llama-4-maverick","meta/llama-4-maverick","llama-4-maverick"],"description":{"en":"Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction. Maverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.","de":"Llama 4 Maverick 17B Instruct (128E) ist ein leistungsstarkes multimodales Sprachmodell von Meta, das auf einer Mixture-of-Experts (MoE)-Architektur mit 128 Experten und 17 Milliarden aktiven Parametern pro Vorwärtsdurchlauf (insgesamt 400B) basiert. Es unterstützt mehrsprachige Text- und Bildeingaben und erzeugt mehrsprachige Text- und Codeausgaben in 12 unterstützten Sprachen. Maverick wurde für visuell-sprachliche Aufgaben optimiert und ist für assistentenähnliches Verhalten, Image Reasoning und allgemeine multimodale Interaktion ausgelegt. Maverick bietet eine frühe Fusion für native Multimodalität und ein Kontextfenster mit 1 Million Token. Es wurde auf einer kuratierten Mischung aus öffentlichen, lizenzierten und Meta-Plattform-Daten trainiert, die ~22 Billionen Token abdeckt, mit einem Wissens-Cutoff im August 2024. Maverick wurde am 5. April 2025 unter der Llama 4 Community License veröffentlicht und eignet sich für Forschung und kommerzielle Anwendungen, die ein fortgeschrittenes multimodales Verständnis und einen hohen Modelldurchsatz erfordern."},"name":"Llama-4-Maverick-17B-128E-Instruct-FP8","toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.52"}}},{"providerId":"openrouter","contextLength":1048576,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.6"},"eur":{"currency":"eur","input":"0.13","output":"0.52"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T12:09:03.088Z","outputLimit":4096,"contextLength":128000},{"id":"llama-4-maverick-17b-128e-instruct","aliases":["meta-llama/llama-4-maverick-17b-128e-instruct","llama-4-maverick-17b-128e-instruct"],"name":"Llama 4 Maverick 17B","toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"groq","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.60"},"eur":{"currency":"eur","input":"0.17","output":"0.52"}}}],"lastImportedAt":"2025-12-08T00:21:39.900Z","outputLimit":8192,"contextLength":131072},{"id":"llama-4-maverick-17b-128e-instruct-fp8","aliases":["hf:meta-llama/llama-4-maverick-17b-128e-instruct-fp8","meta-llama/llama-4-maverick-17b-128e-instruct-fp8","meta/llama-4-maverick-17b-128e-instruct-fp8","llama-4-maverick-17b-128e-instruct-fp8"],"name":"Llama 4 Maverick 17B 128E Instruct FP8","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.00"},"eur":{"currency":"eur","input":"0.22","output":"0.86"}}},{"providerId":"synthetic","contextLength":524000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.22","output":"0.88"},"eur":{"currency":"eur","input":"0.19","output":"0.76"}}},{"providerId":"io-net","contextLength":430000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.52"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.00"},"eur":{"currency":"eur","input":"0.22","output":"0.86"}}},{"providerId":"llama","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-30T12:07:56.034Z","outputLimit":4096,"contextLength":128000},{"id":"llama-4-scout","aliases":["meta-llama/llama-4-scout-17b-16e-instruct","llama-4-scout-17b-16e-instruct","meta-llama/llama-4-scout:free","meta-llama/llama-4-scout","meta/llama-4-scout","llama-4-scout:free","llama-4-scout"],"description":{"en":"Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens. Built for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.","de":"Llama 4 Scout 17B Instruct (16E) ist ein von Meta entwickeltes Mixed-of-Experts (MoE) Sprachmodell, das 17 Milliarden Parameter von insgesamt 109B aktiviert. Es unterstützt native multimodale Eingaben (Text und Bild) und mehrsprachige Ausgaben (Text und Code) in 12 unterstützten Sprachen. Scout wurde für eine assistentenähnliche Interaktion und visuelles Reasoning entwickelt, verwendet 16 Experten pro Vorwärtsdurchlauf und verfügt über eine Kontextlänge von 10 Millionen Token, mit einem Trainingskorpus von ~40 Billionen Token. Llama 4 Scout wurde für hohe Effizienz und den lokalen oder kommerziellen Einsatz entwickelt und beinhaltet eine frühe Fusion für eine nahtlose Integration der Modalitäten. Es ist für den Einsatz in mehrsprachigen Chats, Untertitelung und Bildverstehensaufgaben abgestimmt. Llama 4 Scout wurde unter der Llama 4 Community License veröffentlicht, zuletzt mit Daten bis August 2024 trainiert und am 5. April 2025 der Öffentlichkeit vorgestellt."},"name":"Llama-4-Scout-17B-16E-Instruct-FP8","toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.08","output":"0.30"},"eur":{"currency":"eur","input":"0.07","output":"0.26"}}},{"providerId":"openrouter","contextLength":327680,"price":{"usd":{"currency":"usd","input":"0.08","output":"0.3"},"eur":{"currency":"eur","input":"0.07","output":"0.26"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":64000,"outputLimit":64000,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.339Z","outputLimit":4096,"contextLength":128000},{"id":"llama-4-scout-17b-16e-instruct","aliases":["hf:meta-llama/llama-4-scout-17b-16e-instruct","meta-llama/llama-4-scout-17b-16e-instruct","workers-ai/llama-4-scout-17b-16e-instruct","meta/llama-4-scout-17b-16e-instruct","llama-4-scout-17b-16e-instruct"],"name":"Llama 4 Scout 17B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"groq","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.11","output":"0.34"},"eur":{"currency":"eur","input":"0.09","output":"0.29"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.78"},"eur":{"currency":"eur","input":"0.17","output":"0.67"}}},{"providerId":"cloudflare-workers-ai","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.85"},"eur":{"currency":"eur","input":"0.23","output":"0.73"}}},{"providerId":"wandb","contextLength":64000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.17","output":"0.66"},"eur":{"currency":"eur","input":"0.15","output":"0.57"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"synthetic","contextLength":328000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.52"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.78"},"eur":{"currency":"eur","input":"0.17","output":"0.67"}}}],"lastImportedAt":"2025-12-08T00:21:39.802Z","outputLimit":4096,"contextLength":64000},{"id":"llama-4-scout-17b-16e-instruct-fp8","aliases":["llama-4-scout-17b-16e-instruct-fp8"],"name":"Llama-4-Scout-17B-16E-Instruct-FP8","toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"llama","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.756Z"},{"id":"llama-405b-instruct-vllm","aliases":["replicate/replicate-internal/llama-405b-instruct-vllm","llama-405b-instruct-vllm"],"name":"llama 405u instruct vllm","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"9.50","output":"9.50"},"eur":{"currency":"eur","input":"8.16","output":"8.16"}}}],"lastImportedAt":"2025-12-08T00:21:42.389Z","outputLimit":16384,"contextLength":128000},{"id":"llama-embed-nemotron-8b","aliases":["nvidia/llama-embed-nemotron-8b","llama-embed-nemotron-8b"],"description":{"en":"Multilingual text embedding model optimized for retrieval, reranking, and semantic similarity. Suitable for RAG systems, with state-of-the-art performance on MTEB leaderboard.","de":"Mehrsprachiges Texteinbettungsmodell, optimiert für Retrieval, Reranking und semantische Ähnlichkeit. Geeignet für RAG-Systeme, mit Spitzenleistungen auf dem MTEB-Leaderboard."},"name":"Llama Embed Nemotron 8B","knowledge":"2025-03-01","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"nvidia","contextLength":32768,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.692Z"},{"id":"llama-guard-2-8b","aliases":["meta-llama/meta-llama-guard-2-8b","meta-llama/llama-guard-2-8b","meta-llama-guard-2-8b","llama-guard-2-8b"],"name":"Meta: LlamaGuard 2 8B","description":{"en":"This safeguard model has 8B parameters and is based on the Llama 3 family. Just like is predecessor, [LlamaGuard 1](https://huggingface.co/meta-llama/LlamaGuard-7b), it can do both prompt and response classification. LlamaGuard 2 acts as a normal LLM would, generating text that indicates whether the given input/output is safe/unsafe. If deemed unsafe, it will also share the content categories violated. For best results, please use raw prompt input or the `/completions` endpoint, instead of the chat API. It has demonstrated strong performance compared to leading closed-source models in human evaluations. To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","de":"Dieses Schutzmodell hat 8B Parameter und basiert auf der Llama 3 Familie. Wie sein Vorgänger, [LlamaGuard 1] (https://huggingface.co/meta-llama/LlamaGuard-7b), kann es sowohl Prompt- als auch Response-Klassifizierung durchführen. LlamaGuard 2 verhält sich wie ein normaler LLM und erzeugt einen Text, der angibt, ob die gegebene Eingabe/Ausgabe sicher/unsicher ist. Wenn er als unsicher eingestuft wird, teilt er auch die verletzten Inhaltskategorien mit. Die besten Ergebnisse erzielen Sie, wenn Sie rohe Eingabeaufforderungen oder den Endpunkt `/completions` anstelle der Chat-API verwenden. Es hat in menschlichen Bewertungen eine starke Leistung im Vergleich zu führenden Closed-Source-Modellen gezeigt. Um mehr über die Veröffentlichung des Modells zu erfahren, [klicken Sie hier] (https://ai.meta.com/blog/meta-llama-3/). Die Verwendung dieses Modells unterliegt den [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."},"knowledge":"2024-05-13","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8192,"price":{"usd":{"currency":"usd","input":"0.2","output":"0.2"},"eur":{"currency":"eur","input":"0.17254228","output":"0.17254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"llama-guard-3-8b","aliases":["workers-ai/llama-guard-3-8b","meta-llama/llama-guard-3-8b","llama-guard-3-8b"],"description":{"en":"Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated. Llama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.","de":"Llama Guard 3 ist ein vortrainiertes Llama-3.1-8B-Modell, das auf die Klassifizierung von Sicherheitsinhalten abgestimmt ist. Ähnlich wie die Vorgängerversionen kann es zur Klassifizierung von Inhalten sowohl in LLM-Eingaben (Prompt-Klassifizierung) als auch in LLM-Antworten (Response-Klassifizierung) verwendet werden. Es agiert als LLM - es generiert Text in seiner Ausgabe, der angibt, ob ein bestimmter Prompt oder eine Antwort sicher oder unsicher ist, und falls unsicher, listet es auch die verletzten Inhaltskategorien auf. Llama Guard 3 wurde an die standardisierte Gefahrentaxonomie von MLCommons angepasst und zur Unterstützung der Fähigkeiten von Llama 3.1 entwickelt. Insbesondere bietet es Inhaltsmoderation in 8 Sprachen und wurde optimiert, um Sicherheit und Schutz für Such- und Code-Interpreter-Aufrufe zu unterstützen."},"name":"Llama Guard 3 8B","openWeights":true,"knowledge":"2025-02-12","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","top_k","top_p"],"providers":[{"providerId":"groq","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"cloudflare-workers-ai","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.48","output":"0.03"},"eur":{"currency":"eur","input":"0.41","output":"0.03"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.06"},"eur":{"currency":"eur","input":"0.02","output":"0.05"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.779Z","outputLimit":8192,"contextLength":8192},{"id":"llama-guard-4","aliases":["llama-guard-4"],"name":"Meta Llama Guard 4 12B","knowledge":"2025-01-01","input":["text","image"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"helicone","contextLength":131072,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.21","output":"0.21"},"eur":{"currency":"eur","input":"0.18","output":"0.18"}}}],"lastImportedAt":"2025-12-09T00:20:58.574Z","outputLimit":1024,"contextLength":131072},{"id":"llama-guard-4-12b","aliases":["meta-llama/llama-guard-4-12b","llama-guard-4-12b"],"description":{"en":"Llama Guard 4 is a Llama 4 Scout-derived multimodal pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM—generating text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated. Llama Guard 4 was aligned to safeguard against the standardized MLCommons hazards taxonomy and designed to support multimodal Llama 4 capabilities. Specifically, it combines features from previous Llama Guard models, providing content moderation for English and multiple supported languages, along with enhanced capabilities to handle mixed text-and-image prompts, including multiple images. Additionally, Llama Guard 4 is integrated into the Llama Moderations API, extending robust safety classification to text and images.","de":"Llama Guard 4 ist ein von Llama 4 Scout abgeleitetes, multimodales, vortrainiertes Modell, das für die Sicherheitsklassifizierung von Inhalten optimiert wurde. Ähnlich wie bei früheren Versionen kann es zur Klassifizierung von Inhalten sowohl in LLM-Eingaben (Prompt-Klassifizierung) als auch in LLM-Antworten (Response-Klassifizierung) verwendet werden. In seiner Ausgabe fungiert es als LLM-generierender Text, der angibt, ob ein bestimmter Prompt oder eine Antwort sicher oder unsicher ist, und falls unsicher, listet es auch die verletzten Inhaltskategorien auf. Llama Guard 4 wurde auf die standardisierte MLCommons-Gefahrentaxonomie abgestimmt und zur Unterstützung der multimodalen Llama 4-Funktionen entwickelt. Insbesondere kombiniert es Funktionen aus früheren Llama Guard-Modellen und bietet Inhaltsmoderation für Englisch und mehrere unterstützte Sprachen sowie erweiterte Funktionen für gemischte Text- und Bildaufforderungen, einschließlich mehrerer Bilder. Darüber hinaus ist Llama Guard 4 in die Llama Moderations-API integriert, wodurch die robuste Sicherheitsklassifizierung auf Text und Bilder ausgeweitet wird."},"name":"Llama Guard 4 12B","openWeights":true,"knowledge":"2025-04-30","input":["text","image"],"output":["text"],"parameters":["temperature","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","top_k","top_p"],"providers":[{"providerId":"groq","contextLength":131072,"outputLimit":128,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"openrouter","contextLength":163840,"price":{"usd":{"currency":"usd","input":"0.18","output":"0.18"},"eur":{"currency":"eur","input":"0.15","output":"0.15"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.943Z","outputLimit":128,"contextLength":131072},{"id":"llama-prompt-guard-2-22m","aliases":["llama-prompt-guard-2-22m"],"name":"Meta Llama Prompt Guard 2 22M","knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"helicone","contextLength":512,"outputLimit":2,"price":{"usd":{"currency":"usd","input":"0.01","output":"0.01"},"eur":{"currency":"eur","input":"0.01","output":"0.01"}}}],"lastImportedAt":"2025-12-09T00:20:58.666Z","outputLimit":2,"contextLength":512},{"id":"llama-prompt-guard-2-86m","aliases":["llama-prompt-guard-2-86m"],"name":"Meta Llama Prompt Guard 2 86M","knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"helicone","contextLength":512,"outputLimit":2,"price":{"usd":{"currency":"usd","input":"0.01","output":"0.01"},"eur":{"currency":"eur","input":"0.01","output":"0.01"}}}],"lastImportedAt":"2025-12-09T00:20:58.486Z","outputLimit":2,"contextLength":512},{"id":"llama3-70b-8192","aliases":["llama3-70b-8192"],"name":"Llama 3 70B","toolCalling":true,"openWeights":true,"knowledge":"2023-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"groq","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.59","output":"0.79"},"eur":{"currency":"eur","input":"0.51","output":"0.68"}}}],"lastImportedAt":"2025-12-08T00:21:39.761Z","outputLimit":8192,"contextLength":8192},{"id":"llama3-8b-8192","aliases":["llama3-8b-8192"],"name":"Llama 3 8B","toolCalling":true,"openWeights":true,"knowledge":"2023-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"groq","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.08"},"eur":{"currency":"eur","input":"0.04","output":"0.07"}}}],"lastImportedAt":"2025-12-08T00:21:39.711Z","outputLimit":8192,"contextLength":8192},{"id":"llamaguard-7b-awq","aliases":["llamaguard-7b-awq"],"name":"@hf/thebloke/llamaguard-7b-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.265Z","outputLimit":4096,"contextLength":4096},{"id":"llava-1.5-7b-hf","aliases":["llava-1.5-7b-hf"],"name":"@cf/llava-hf/llava-1.5-7b-hf","openWeights":true,"input":["image","text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.697Z"},{"id":"llava-next-mistral-7b","aliases":["llava-next-mistral-7b"],"name":"llava-next-mistral-7b","openWeights":true,"input":["text","image"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"ovhcloud","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.32","output":"0.32"},"eur":{"currency":"eur","input":"0.276067648","output":"0.276067648"}}}],"lastImportedAt":"2025-11-19T12:06:32.753Z"},{"id":"llemma_7b","aliases":["eleutherai/llemma_7b","llemma_7b"],"name":"EleutherAI: Llemma 7b","description":{"en":"Llemma 7B is a language model for mathematics. It was initialized with Code Llama 7B weights, and trained on the Proof-Pile-2 for 200B tokens. Llemma models are particularly strong at chain-of-thought mathematical reasoning and using computational tools for mathematics, such as Python and formal theorem provers.","de":"Llemma 7B ist ein Sprachmodell für Mathematik. Es wurde mit Code Llama 7B Gewichten initialisiert und auf dem Proof-Pile-2 für 200B Token trainiert. Llemma-Modelle sind besonders stark im mathematischen Kettenschluss und in der Verwendung von Rechenwerkzeugen für Mathematik, wie Python und formalen Theorembeweisern."},"knowledge":"2025-04-14","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":4096,"price":{"usd":{"currency":"usd","input":"0.8","output":"1.2"},"eur":{"currency":"eur","input":"0.69016912","output":"1.03525368"}}}],"lastImportedAt":"2025-11-19T12:06:32.765Z"},{"id":"longcat-flash-chat","aliases":["meituan-longcat/longcat-flash-chat","meituan/longcat-flash-chat:free","meituan/longcat-flash-chat","longcat-flash-chat:free","longcat-flash-chat"],"name":"Meituan: LongCat Flash Chat (free)","description":{"en":"LongCat-Flash-Chat is a large-scale Mixture-of-Experts (MoE) model with 560B total parameters, of which 18.6B–31.3B (≈27B on average) are dynamically activated per input. It introduces a shortcut-connected MoE design to reduce communication overhead and achieve high throughput while maintaining training stability through advanced scaling strategies such as hyperparameter transfer, deterministic computation, and multi-stage optimization. This release, LongCat-Flash-Chat, is a non-thinking foundation model optimized for conversational and agentic tasks. It supports long context windows up to 128K tokens and shows competitive performance across reasoning, coding, instruction following, and domain benchmarks, with particular strengths in tool use and complex multi-step interactions.","de":"LongCat-Flash-Chat ist ein groß angelegtes Mixture-of-Experts (MoE)-Modell mit 560B Gesamtparametern, von denen 18,6B-31,3B (≈27B im Durchschnitt) dynamisch pro Eingabe aktiviert werden. Es wird ein MoE-Design mit Shortcut-Verbindungen eingeführt, um den Kommunikations-Overhead zu reduzieren und einen hohen Durchsatz zu erreichen, während die Trainingsstabilität durch fortschrittliche Skalierungsstrategien wie Hyperparameter-Transfer, deterministische Berechnung und mehrstufige Optimierung erhalten bleibt. Diese Version, LongCat-Flash-Chat, ist ein nicht denkendes Basismodell, das für Konversations- und Agentenaufgaben optimiert ist. Es unterstützt lange Kontextfenster mit bis zu 128K Token und zeigt eine konkurrenzfähige Leistung in den Bereichen Argumentation, Codierung, Befehlsverfolgung und Domain-Benchmarks, mit besonderen Stärken bei der Verwendung von Tools und komplexen mehrstufigen Interaktionen."},"knowledge":"2025-09-09","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.75"},"eur":{"currency":"eur","input":"0.12940671","output":"0.64703355"}}}],"lastImportedAt":"2025-11-19T12:06:32.763Z"},{"id":"longcat-flash-chat-fp8","aliases":["meituan-longcat/longcat-flash-chat-fp8","longcat-flash-chat-fp8"],"description":{"en":"LongCat-Flash-Chat is a text-generation model with 560 billion parameters, utilizing a Mixture-of-Experts architecture for dynamic computation. It excels in agentic tasks and features efficient training and inference strategies. Designed for scalability, it supports complex reasoning and multi-turn interactions, suitable for diverse applications.","de":"LongCat-Flash-Chat ist ein Textgenerierungsmodell mit 560 Milliarden Parametern, das eine Mixture-of-Experts-Architektur für dynamische Berechnungen verwendet. Es eignet sich hervorragend für agentenbasierte Aufgaben und bietet effiziente Trainings- und Inferenzstrategien. Es ist auf Skalierbarkeit ausgelegt und unterstützt komplexe Schlussfolgerungen und Multiturn-Interaktionen, die sich für verschiedene Anwendungen eignen."},"name":"LongCat Flash Chat FP8","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.718Z","deprecated":true},{"id":"lucid-origin","aliases":["lucid-origin"],"name":"@cf/leonardo/lucid-origin","input":["text"],"output":["image"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.01","output":"0.01"},"eur":{"currency":"eur","input":"0.01","output":"0.01"}}}],"lastImportedAt":"2025-12-08T00:21:40.727Z"},{"id":"lucidnova-rf1-100b","aliases":["lucidnova-rf1-100b"],"name":"LucidNova RF1 100B","reasoning":true,"toolCalling":true,"knowledge":"2025-09-16","input":["text"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"lucidquery","contextLength":120000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"2.00","output":"5.00"},"eur":{"currency":"eur","input":"1.7254228","output":"4.313557"}}}],"lastImportedAt":"2025-11-19T12:06:32.682Z"},{"id":"lucidquery-nexus-coder","aliases":["lucidquery-nexus-coder"],"name":"LucidQuery Nexus Coder","reasoning":true,"toolCalling":true,"knowledge":"2025-08-01","input":["text"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"lucidquery","contextLength":250000,"outputLimit":60000,"price":{"usd":{"currency":"usd","input":"2.00","output":"5.00"},"eur":{"currency":"eur","input":"1.7254228","output":"4.313557"}}}],"lastImportedAt":"2025-11-19T12:06:32.682Z"},{"id":"lyria","aliases":["google/lyria","lyria"],"name":"Lyria","toolCalling":true,"input":["text"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:41.463Z"},{"id":"m2m100-1.2b","aliases":["workers-ai/m2m100-1.2b","m2m100-1.2b"],"name":"@cf/meta/m2m100-1.2b","openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.34","output":"0.34"},"eur":{"currency":"eur","input":"0.29","output":"0.29"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.991Z","outputLimit":16384,"contextLength":128000},{"id":"maestro-reasoning","aliases":["arcee-ai/maestro-reasoning","maestro-reasoning"],"name":"Arcee AI: Maestro Reasoning","description":{"en":"Maestro Reasoning is Arcee's flagship analysis model: a 32 B‑parameter derivative of Qwen 2.5‑32 B tuned with DPO and chain‑of‑thought RL for step‑by‑step logic. Compared to the earlier 7 B preview, the production 32 B release widens the context window to 128 k tokens and doubles pass‑rate on MATH and GSM‑8K, while also lifting code completion accuracy. Its instruction style encourages structured \"thought → answer\" traces that can be parsed or hidden according to user preference. That transparency pairs well with audit‑focused industries like finance or healthcare where seeing the reasoning path matters. In Arcee Conductor, Maestro is automatically selected for complex, multi‑constraint queries that smaller SLMs bounce.","de":"Maestro Reasoning ist das Vorzeige-Analysemodell von Arcee: ein 32-B-Parameter-Derivat von Qwen 2.5-32 B, das mit DPO und Chain-of-Thought-RL für schrittweise Logik abgestimmt ist. Im Vergleich zur früheren 7-B-Vorschau erweitert die 32-B-Produktionsversion das Kontextfenster auf 128 k Token und verdoppelt die Erfolgsrate bei MATH und GSM-8K, während sie gleichzeitig die Genauigkeit der Code-Vervollständigung erhöht. Der Anweisungsstil fördert strukturierte \"Gedanke → Antwort\"-Spuren, die je nach Benutzerpräferenz geparst oder ausgeblendet werden können. Diese Transparenz passt gut zu prüfungsorientierten Branchen wie dem Finanzwesen oder dem Gesundheitswesen, wo es wichtig ist, den Gedankengang zu erkennen. In Arcee Conductor wird Maestro automatisch für komplexe Abfragen mit mehreren Einschränkungen ausgewählt, an denen kleinere SLMs scheitern."},"knowledge":"2025-05-05","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.9","output":"3.3"},"eur":{"currency":"eur","input":"0.77644026","output":"2.84694762"}}}],"lastImportedAt":"2025-11-19T12:06:32.765Z"},{"id":"magistral-medium","aliases":["mistral/magistral-medium","magistral-medium"],"name":"Magistral Medium","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.00","output":"5.00"},"eur":{"currency":"eur","input":"1.7254228","output":"4.313557"}}}],"lastImportedAt":"2025-11-19T12:06:32.704Z"},{"id":"magistral-small","aliases":["mistral/magistral-small","magistral-small"],"name":"Magistral Small","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"mistral","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.50"},"eur":{"currency":"eur","input":"0.4313557","output":"1.2940671"}}},{"providerId":"vercel","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.50"},"eur":{"currency":"eur","input":"0.4313557","output":"1.2940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.703Z"},{"id":"magnum-v4-72b","aliases":["anthracite-org/magnum-v4-72b","magnum-v4-72b"],"name":"Magnum v4 72B","description":{"en":"This is a series of models designed to replicate the prose quality of the Claude 3 models, specifically Sonnet(https://openrouter.ai/anthropic/claude-3.5-sonnet) and Opus(https://openrouter.ai/anthropic/claude-3-opus). The model is fine-tuned on top of [Qwen2.5 72B](https://openrouter.ai/qwen/qwen-2.5-72b-instruct).","de":"Es handelt sich um eine Reihe von Modellen, die die Prosaqualität der Claude-3-Modelle, insbesondere Sonnet (https://openrouter.ai/anthropic/claude-3.5-sonnet) und Opus (https://openrouter.ai/anthropic/claude-3-opus), nachahmen sollen. Das Modell wurde auf der Grundlage von [Qwen2.5 72B](https://openrouter.ai/qwen/qwen-2.5-72b-instruct) feinabgestimmt."},"knowledge":"2024-10-22","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","temperature","top_a","top_k","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":16384,"price":{"usd":{"currency":"usd","input":"3","output":"5"},"eur":{"currency":"eur","input":"2.5881342","output":"4.313557"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"mai-ds-r1","aliases":["microsoft/mai-ds-r1:free","microsoft/mai-ds-r1","mai-ds-r1:free","mai-ds-r1"],"description":{"en":"MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed by the Microsoft AI team to improve the model’s responsiveness on previously blocked topics while enhancing its safety profile. Built on top of DeepSeek-R1’s reasoning foundation, it integrates 110k examples from the Tulu-3 SFT dataset and 350k internally curated multilingual safety-alignment samples. The model retains strong reasoning, coding, and problem-solving capabilities, while unblocking a wide range of prompts previously restricted in R1. MAI-DS-R1 demonstrates improved performance on harm mitigation benchmarks and maintains competitive results across general reasoning tasks. It surpasses R1-1776 in satisfaction metrics for blocked queries and reduces leakage in harmful content categories. The model is based on a transformer MoE architecture and is suitable for general-purpose use cases, excluding high-stakes domains such as legal, medical, or autonomous systems.","de":"MAI-DS-R1 ist eine nachtrainierte Variante von DeepSeek-R1, die vom Microsoft-KI-Team entwickelt wurde, um die Reaktionsfähigkeit des Modells bei zuvor blockierten Themen zu verbessern und gleichzeitig sein Sicherheitsprofil zu erhöhen. Es baut auf der Grundlage von DeepSeek-R1 auf und integriert 110.000 Beispiele aus dem Tulu-3 SFT-Datensatz sowie 350.000 intern kuratierte mehrsprachige Beispiele für Sicherheitsabgleiche. Das Modell behält seine starken Argumentations-, Codierungs- und Problemlösungsfähigkeiten bei, während es eine breite Palette von Aufforderungen freigibt, die zuvor in R1 eingeschränkt waren. MAI-DS-R1 zeigt eine verbesserte Leistung bei Benchmarks zur Schadensbegrenzung und behält seine konkurrenzfähigen Ergebnisse bei allgemeinen Denkaufgaben bei. Es übertrifft R1-1776 in den Zufriedenheitsmetriken für blockierte Abfragen und reduziert Leckagen in schädlichen Inhaltskategorien. Das Modell basiert auf einer Transformator-MoE-Architektur und ist für allgemeine Anwendungsfälle geeignet, mit Ausnahme von Bereichen mit hohem Risiko wie Recht, Medizin oder autonome Systeme."},"name":"MAI-DS-R1","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"github-models","contextLength":65536,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.35","output":"5.40"},"eur":{"currency":"eur","input":"1.16","output":"4.66"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.35","output":"5.40"},"eur":{"currency":"eur","input":"1.16","output":"4.66"}}},{"providerId":"openrouter","contextLength":163840,"price":{"usd":{"currency":"usd","input":"0.3","output":"1.2"},"eur":{"currency":"eur","input":"0.26","output":"1.04"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":163840,"outputLimit":163840,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-11-30T12:07:55.928Z","outputLimit":8192,"contextLength":65536},{"id":"mai-ds-r1-fp8","aliases":["microsoft/mai-ds-r1-fp8","mai-ds-r1-fp8"],"description":{"en":"MAI-DS-R1 is a post-trained reasoning model based on DeepSeek-R1, enhancing responsiveness on restricted topics while maintaining strong reasoning capabilities. It excels in text generation, general knowledge tasks, and complex problem-solving. Intended use includes coding assistance and scientific applications, with caution advised in sensitive areas due to potential biases and limitations.","de":"MAI-DS-R1 ist ein auf DeepSeek-R1 basierendes, nachtrainiertes Schlussfolgerungsmodell, das die Reaktionsfähigkeit bei eingeschränkten Themen verbessert und gleichzeitig starke Schlussfolgerungsfähigkeiten beibehält. Es zeichnet sich bei der Texterstellung, bei allgemeinen Wissensaufgaben und bei komplexen Problemlösungen aus. Es ist für die Unterstützung bei der Codierung und für wissenschaftliche Anwendungen vorgesehen, wobei in sensiblen Bereichen aufgrund möglicher Verzerrungen und Einschränkungen Vorsicht geboten ist."},"name":"MAI DS R1 FP8","reasoning":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}}],"lastImportedAt":"2025-11-19T12:06:32.719Z","deprecated":true},{"id":"melotts","aliases":["workers-ai/melotts","melotts"],"name":"@cf/myshell-ai/melotts","openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.729Z","outputLimit":16384,"contextLength":128000},{"id":"mercury","aliases":["inception/mercury","mercury"],"description":{"en":"Mercury is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like GPT-4.1 Nano and Claude 3.5 Haiku while matching their performance. Mercury's speed enables developers to provide responsive user experiences, including with voice agents, search interfaces, and chatbots. Read more in the [blog post] (https://www.inceptionlabs.ai/blog/introducing-mercury) here.","de":"Mercury ist das erste Diffusionsmodell für große Sprachen (dLLM). Durch die Anwendung eines bahnbrechenden diskreten Diffusionsansatzes läuft das Modell 5-10x schneller als selbst geschwindigkeitsoptimierte Modelle wie GPT-4.1 Nano und Claude 3.5 Haiku, während es deren Leistung entspricht. Die Geschwindigkeit von Mercury ermöglicht es Entwicklern, reaktionsschnelle Benutzererfahrungen zu bieten, unter anderem mit Sprachagenten, Suchoberflächen und Chatbots. Lesen Sie mehr im [Blogpost] (https://www.inceptionlabs.ai/blog/introducing-mercury) hier."},"name":"Mercury","toolCalling":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"inception","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.00"},"eur":{"currency":"eur","input":"0.21567785","output":"0.8627114"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"1"},"eur":{"currency":"eur","input":"0.21567785","output":"0.8627114"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.736Z"},{"id":"mercury-coder","aliases":["inception/mercury-coder-small-beta","mercury-coder-small-beta","inception/mercury-coder","mercury-coder"],"description":{"en":"Mercury Coder is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like Claude 3.5 Haiku and GPT-4o Mini while matching their performance. Mercury Coder's speed means that developers can stay in the flow while coding, enjoying rapid chat-based iteration and responsive code completion suggestions. On Copilot Arena, Mercury Coder ranks 1st in speed and ties for 2nd in quality. Read more in the [blog post here](https://www.inceptionlabs.ai/blog/introducing-mercury).","de":"Mercury Coder ist das erste Diffusionsmodell für große Sprachen (dLLM). Durch die Anwendung eines bahnbrechenden diskreten Diffusionsansatzes läuft das Modell 5-10x schneller als selbst geschwindigkeitsoptimierte Modelle wie Claude 3.5 Haiku und GPT-4o Mini, während es deren Leistung entspricht. Die Geschwindigkeit von Mercury Coder bedeutet, dass Entwickler während des Programmierens im Fluss bleiben können, indem sie schnelle Chat-basierte Iteration und reaktionsschnelle Vorschläge zur Vervollständigung des Codes genießen. Auf Copilot Arena belegt Mercury Coder Platz 1 bei der Geschwindigkeit und Platz 2 bei der Qualität. Lesen Sie mehr im [Blogbeitrag hier] (https://www.inceptionlabs.ai/blog/introducing-mercury)."},"name":"Mercury Coder","toolCalling":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"inception","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.00"},"eur":{"currency":"eur","input":"0.21567785","output":"0.8627114"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.25","output":"1"},"eur":{"currency":"eur","input":"0.21567785","output":"0.8627114"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.735Z"},{"id":"meta-llama-3_3-70b-instruct","aliases":["meta-llama-3_3-70b-instruct"],"name":"Meta-Llama-3_3-70B-Instruct","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ovhcloud","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.74","output":"0.74"},"eur":{"currency":"eur","input":"0.638406436","output":"0.638406436"}}}],"lastImportedAt":"2025-11-19T12:06:32.753Z"},{"id":"meta-llama-3-70b-instruct","aliases":["replicate/meta/meta-llama-3-70b-instruct","meta/meta-llama-3-70b-instruct","meta-llama-3-70b-instruct"],"name":"Meta-Llama-3-70B-Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"2.68","output":"3.54"},"eur":{"currency":"eur","input":"2.3","output":"3.04"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"2.75"},"eur":{"currency":"eur","input":"0","output":"2.36"}}},{"providerId":"azure-cognitive-services","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"2.68","output":"3.54"},"eur":{"currency":"eur","input":"2.3","output":"3.04"}}}],"lastImportedAt":"2025-12-08T00:21:40.084Z","outputLimit":2048,"contextLength":8192},{"id":"meta-llama-3-8b-instruct","aliases":["replicate/meta/meta-llama-3-8b-instruct","meta/meta-llama-3-8b-instruct","meta-llama-3-8b-instruct"],"name":"Meta-Llama-3-8B-Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.61"},"eur":{"currency":"eur","input":"0.26","output":"0.52"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure-cognitive-services","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.61"},"eur":{"currency":"eur","input":"0.26","output":"0.52"}}}],"lastImportedAt":"2025-12-08T00:21:40.176Z","outputLimit":2048,"contextLength":8192},{"id":"meta-llama-3.1-405b-instruct","aliases":["replicate/meta/meta-llama-3.1-405b-instruct","meta/meta-llama-3.1-405b-instruct","meta-llama-3.1-405b-instruct"],"name":"Meta-Llama-3.1-405B-Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"5.33","output":"16.00"},"eur":{"currency":"eur","input":"4.58","output":"13.74"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"9.50","output":"9.50"},"eur":{"currency":"eur","input":"8.16","output":"8.16"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"5.33","output":"16.00"},"eur":{"currency":"eur","input":"4.58","output":"13.74"}}}],"lastImportedAt":"2025-12-08T00:21:39.999Z","outputLimit":16384,"contextLength":128000},{"id":"meta-llama-3.1-70b-instruct","aliases":["meta/meta-llama-3.1-70b-instruct","meta-llama-3.1-70b-instruct","meta-llama-3_1-70b-instruct"],"name":"Meta-Llama-3.1-70B-Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.68","output":"3.54"},"eur":{"currency":"eur","input":"2.31","output":"3.05"}}},{"providerId":"ovhcloud","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.74","output":"0.74"},"eur":{"currency":"eur","input":"0.64","output":"0.64"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"2.68","output":"3.54"},"eur":{"currency":"eur","input":"2.31","output":"3.05"}}}],"lastImportedAt":"2025-11-30T12:07:57.253Z","outputLimit":32768,"contextLength":128000},{"id":"meta-llama-3.1-8b-instruct","aliases":["meta/meta-llama-3.1-8b-instruct","meta-llama-3.1-8b-instruct"],"name":"Meta-Llama-3.1-8B-Instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.61"},"eur":{"currency":"eur","input":"0.26","output":"0.53"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.61"},"eur":{"currency":"eur","input":"0.26","output":"0.53"}}}],"lastImportedAt":"2025-11-30T12:07:57.615Z","outputLimit":32768,"contextLength":128000},{"id":"meta-llama-llama-3-2-90b-vision-instruct","aliases":["meta-llama-llama-3-2-90b-vision-instruct"],"name":"Llama 3.2 90B Vision Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"io-intelligence","contextLength":16000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.35","output":"0.40"},"eur":{"currency":"eur","input":"0.3","output":"0.35"}}}],"lastImportedAt":"2025-11-27T12:08:37.094Z","outputLimit":4096,"contextLength":16000,"deprecated":true},{"id":"meta-llama-llama-3-3-70b-instruct","aliases":["meta-llama-llama-3-3-70b-instruct"],"name":"Llama 3.3 70B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"io-intelligence","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.38"},"eur":{"currency":"eur","input":"0.11","output":"0.33"}}}],"lastImportedAt":"2025-11-27T12:08:37.048Z","outputLimit":4096,"contextLength":128000,"deprecated":true},{"id":"meta-llama-llama-4-maverick-17b-128e-instruct-fp8","aliases":["meta-llama-llama-4-maverick-17b-128e-instruct-fp8"],"name":"Llama 4 Maverick 17B 128E Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"io-intelligence","contextLength":430000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.52"}}}],"lastImportedAt":"2025-11-27T12:08:37.001Z","outputLimit":4096,"contextLength":430000,"deprecated":true},{"id":"meta-llama-meta-llama-3.1-8b-instruct","aliases":["meta-llama-meta-llama-3.1-8b-instruct"],"name":"meta-llama/Meta-Llama-3.1-8B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.06"},"eur":{"currency":"eur","input":"0.05","output":"0.05"}}}],"lastImportedAt":"2025-11-26T00:20:50.298Z","outputLimit":4000,"contextLength":33000},{"id":"meta.llama3-1-70b-instruct-v1:0","aliases":["meta.llama3-1-70b-instruct-v1:0"],"name":"Llama 3.1 70B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.72","output":"0.72"},"eur":{"currency":"eur","input":"0.621152208","output":"0.621152208"}}}],"lastImportedAt":"2025-11-19T12:06:32.758Z"},{"id":"meta.llama3-1-8b-instruct-v1:0","aliases":["meta.llama3-1-8b-instruct-v1:0"],"name":"Llama 3.1 8B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.22","output":"0.22"},"eur":{"currency":"eur","input":"0.189796508","output":"0.189796508"}}}],"lastImportedAt":"2025-11-19T12:06:32.758Z"},{"id":"meta.llama3-2-11b-instruct-v1:0","aliases":["meta.llama3-2-11b-instruct-v1:0"],"name":"Llama 3.2 11B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.16","output":"0.16"},"eur":{"currency":"eur","input":"0.138033824","output":"0.138033824"}}}],"lastImportedAt":"2025-11-19T12:06:32.757Z"},{"id":"meta.llama3-2-1b-instruct-v1:0","aliases":["meta.llama3-2-1b-instruct-v1:0"],"name":"Llama 3.2 1B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":131000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.10"},"eur":{"currency":"eur","input":"0.08627114","output":"0.08627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.757Z"},{"id":"meta.llama3-2-3b-instruct-v1:0","aliases":["meta.llama3-2-3b-instruct-v1:0"],"name":"Llama 3.2 3B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":131000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.12940671","output":"0.12940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.759Z"},{"id":"meta.llama3-2-90b-instruct-v1:0","aliases":["meta.llama3-2-90b-instruct-v1:0"],"name":"Llama 3.2 90B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.72","output":"0.72"},"eur":{"currency":"eur","input":"0.621152208","output":"0.621152208"}}}],"lastImportedAt":"2025-11-19T12:06:32.757Z"},{"id":"meta.llama3-3-70b-instruct-v1:0","aliases":["meta.llama3-3-70b-instruct-v1:0"],"name":"Llama 3.3 70B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.72","output":"0.72"},"eur":{"currency":"eur","input":"0.621152208","output":"0.621152208"}}}],"lastImportedAt":"2025-11-19T12:06:32.758Z"},{"id":"meta.llama3-70b-instruct-v1:0","aliases":["meta.llama3-70b-instruct-v1:0"],"name":"Llama 3 70B Instruct","openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"amazon-bedrock","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"2.65","output":"3.50"},"eur":{"currency":"eur","input":"2.28618521","output":"3.0194899"}}}],"lastImportedAt":"2025-11-19T12:06:32.759Z"},{"id":"meta.llama3-8b-instruct-v1:0","aliases":["meta.llama3-8b-instruct-v1:0"],"name":"Llama 3 8B Instruct","openWeights":true,"knowledge":"2023-03-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"amazon-bedrock","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.30","output":"0.60"},"eur":{"currency":"eur","input":"0.25881342","output":"0.51762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.760Z"},{"id":"meta.llama4-maverick-17b-instruct-v1:0","aliases":["meta.llama4-maverick-17b-instruct-v1:0"],"name":"Llama 4 Maverick 17B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":1000000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.24","output":"0.97"},"eur":{"currency":"eur","input":"0.207050736","output":"0.836830058"}}}],"lastImportedAt":"2025-11-19T12:06:32.760Z"},{"id":"meta.llama4-scout-17b-instruct-v1:0","aliases":["meta.llama4-scout-17b-instruct-v1:0"],"name":"Llama 4 Scout 17B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-08-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":3500000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.17","output":"0.66"},"eur":{"currency":"eur","input":"0.146660938","output":"0.569389524"}}}],"lastImportedAt":"2025-11-19T12:06:32.760Z"},{"id":"minimax-01","aliases":["minimaxai/minimax-text-01","minimax/minimax-01","minimax-text-01","minimax-01"],"description":{"en":"MiniMax-01 is a combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding. It has 456 billion parameters, with 45.9 billion parameters activated per inference, and can handle a context of up to 4 million tokens. The text model adopts a hybrid architecture that combines Lightning Attention, Softmax Attention, and Mixture-of-Experts (MoE). The image model adopts the “ViT-MLP-LLM” framework and is trained on top of the text model. To read more about the release, see: https://www.minimaxi.com/en/news/minimax-01-series-2","de":"MiniMax-01 ist eine Kombination aus MiniMax-Text-01 für die Texterstellung und MiniMax-VL-01 für das Bildverständnis. Er hat 456 Milliarden Parameter, wobei 45,9 Milliarden Parameter pro Inferenz aktiviert werden, und kann einen Kontext von bis zu 4 Millionen Token verarbeiten. Das Textmodell verwendet eine hybride Architektur, die Lightning Attention, Softmax Attention und Mixture-of-Experts (MoE) kombiniert. Das Bildmodell basiert auf dem \"ViT-MLP-LLM\"-Framework und wird auf dem Textmodell trainiert. Weitere Informationen über die neue Version finden Sie unter: https://www.minimaxi.com/en/news/minimax-01-series-2"},"name":"MiniMax-01","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-01-15","input":["text","image"],"output":["text"],"parameters":["temperature","tools","max_tokens","top_p"],"providers":[{"providerId":"openrouter","contextLength":1000000,"outputLimit":1000000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.10"},"eur":{"currency":"eur","input":"0.17254228","output":"0.94898254"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.750Z"},{"id":"minimax-m1","aliases":["minimax/minimax-m1","minimax-m1"],"description":{"en":"MiniMax-M1 is a large-scale, open-weight reasoning model designed for extended context and high-efficiency inference. It leverages a hybrid Mixture-of-Experts (MoE) architecture paired with a custom \"lightning attention\" mechanism, allowing it to process long sequences—up to 1 million tokens—while maintaining competitive FLOP efficiency. With 456 billion total parameters and 45.9B active per token, this variant is optimized for complex, multi-step reasoning tasks. Trained via a custom reinforcement learning pipeline (CISPO), M1 excels in long-context understanding, software engineering, agentic tool use, and mathematical reasoning. Benchmarks show strong performance across FullStackBench, SWE-bench, MATH, GPQA, and TAU-Bench, often outperforming other open models like DeepSeek R1 and Qwen3-235B.","de":"MiniMax-M1 ist ein groß angelegtes, offenes Schlussfolgerungsmodell, das für erweiterten Kontext und hocheffiziente Schlussfolgerungen entwickelt wurde. Es nutzt eine hybride Mixture-of-Experts (MoE)-Architektur, gepaart mit einem benutzerdefinierten \"Lightning-Attention\"-Mechanismus, der es ihm ermöglicht, lange Sequenzen - bis zu 1 Million Token - zu verarbeiten und dabei eine wettbewerbsfähige FLOP-Effizienz beizubehalten. Mit 456 Milliarden Gesamtparametern und 45,9B aktiv pro Token ist diese Variante für komplexe, mehrstufige Schlussfolgerungen optimiert. M1 wurde über eine kundenspezifische Pipeline für verstärkendes Lernen (CISPO) trainiert und zeichnet sich durch langes Kontextverständnis, Software-Engineering, den Einsatz von Agententools und mathematische Schlussfolgerungen aus. Benchmarks zeigen eine starke Leistung in FullStackBench, SWE-Bench, MATH, GPQA und TAU-Bench und übertreffen oft andere offene Modelle wie DeepSeek R1 und Qwen3-235B."},"name":"MiniMax M1","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-06-17","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","seed","stop","tool_choice","top_k","top_p"],"providers":[{"providerId":"openrouter","contextLength":1000000,"outputLimit":40000,"price":{"usd":{"currency":"usd","input":"0.40","output":"2.20"},"eur":{"currency":"eur","input":"0.34508456","output":"1.89796508"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.750Z"},{"id":"minimax-m2","aliases":["accounts/fireworks/models/minimax-m2","hf:minimaxai/minimax-m2","minimaxai/minimax-m2","minimax/minimax-m2","minimax-m2"],"description":{"en":"MiniMax-M2 is a compact, high-efficiency large language model optimized for end-to-end coding and agentic workflows. With 10 billion activated parameters (230 billion total), it delivers near-frontier intelligence across general reasoning, tool use, and multi-step task execution while maintaining low latency and deployment efficiency. The model excels in code generation, multi-file editing, compile-run-fix loops, and test-validated repair, showing strong results on SWE-Bench Verified, Multi-SWE-Bench, and Terminal-Bench. It also performs competitively in agentic evaluations such as BrowseComp and GAIA, effectively handling long-horizon planning, retrieval, and recovery from execution errors. Benchmarked by [Artificial Analysis](https://artificialanalysis.ai/models/minimax-m2), MiniMax-M2 ranks among the top open-source models for composite intelligence, spanning mathematics, science, and instruction-following. Its small activation footprint enables fast inference, high concurrency, and improved unit economics, making it well-suited for large-scale agents, developer assistants, and reasoning-driven applications that require responsiveness and cost efficiency. To avoid degrading this model's performance, MiniMax highly recommends preserving reasoning between turns. Learn more about using reasoning_details to pass back reasoning in our [docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks).","de":"MiniMax-M2 ist ein kompaktes, hocheffizientes Großsprachenmodell, das für End-to-End-Codierung und agentenbasierte Workflows optimiert ist. Mit 10 Milliarden aktivierten Parametern (230 Milliarden insgesamt) bietet es nahezu grenzenlose Intelligenz für allgemeine Schlussfolgerungen, die Verwendung von Werkzeugen und die Ausführung von Aufgaben in mehreren Schritten, während es gleichzeitig niedrige Latenzzeiten und eine effiziente Bereitstellung gewährleistet. Das Modell zeichnet sich durch Code-Generierung, Bearbeitung mehrerer Dateien, Compile-Run-Fix-Schleifen und testvalidierte Reparaturen aus und zeigt starke Ergebnisse bei SWE-Bench Verified, Multi-SWE-Bench und Terminal-Bench. Auch bei agentenbasierten Evaluierungen wie BrowseComp und GAIA ist es konkurrenzfähig, da es Planung, Abruf und Wiederherstellung nach Ausführungsfehlern mit langem Zeithorizont effektiv handhabt. Im Benchmarking von [Artificial Analysis] (https://artificialanalysis.ai/models/minimax-m2) zählt MiniMax-M2 zu den besten Open-Source-Modellen für zusammengesetzte Intelligenz, die Mathematik, Naturwissenschaften und das Befolgen von Anweisungen abdecken. Sein kleiner Aktivierungsfußabdruck ermöglicht schnelle Inferenz, hohe Gleichzeitigkeit und eine verbesserte Wirtschaftlichkeit der Einheiten, wodurch es sich gut für große Agenten, Entwicklerassistenten und schlussfolgernde Anwendungen eignet, die Reaktionsfähigkeit und Kosteneffizienz erfordern. Um die Leistung dieses Modells nicht zu verschlechtern, empfiehlt MiniMax dringend, die Schlussfolgerungen zwischen den einzelnen Runden beizubehalten. Erfahren Sie mehr über die Verwendung von reasoning_details zur Rückgabe von Argumenten in unseren [docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks)."},"name":"MiniMax-M2","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"nvidia","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":205000,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.04"}}},{"providerId":"chutes","contextLength":196608,"outputLimit":196608,"price":{"usd":{"currency":"usd","input":"0.26","output":"1.02"},"eur":{"currency":"eur","input":"0.22","output":"0.88"}}},{"providerId":"huggingface","contextLength":204800,"outputLimit":204800,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.04"}}},{"providerId":"minimax","contextLength":196608,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.04"}}},{"providerId":"openrouter","contextLength":196600,"outputLimit":118000,"price":{"usd":{"currency":"usd","input":"0.28","output":"1.15"},"eur":{"currency":"eur","input":"0.24","output":"0.99"}}},{"providerId":"zenmux","contextLength":204800,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.04"}}},{"providerId":"iflowcn","contextLength":204800,"outputLimit":131100,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"synthetic","contextLength":196608,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.19"},"eur":{"currency":"eur","input":"0.47","output":"1.89"}}},{"providerId":"fireworks-ai","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.04"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-29T00:20:01.222Z","outputLimit":16384,"contextLength":128000},{"id":"minimax-m2:cloud","aliases":["minimax-m2:cloud"],"name":"MiniMax M2","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.852Z","outputLimit":8192,"contextLength":200000},{"id":"minimaxai-minimax-m1-80k","aliases":["minimaxai-minimax-m1-80k"],"name":"MiniMaxAI/MiniMax-M1-80k","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.20"},"eur":{"currency":"eur","input":"0.48","output":"1.91"}}}],"lastImportedAt":"2025-11-26T00:20:47.751Z","outputLimit":131000,"contextLength":131000},{"id":"minimaxai-minimax-m2","aliases":["minimaxai-minimax-m2"],"name":"MiniMaxAI/MiniMax-M2","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":197000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.04"}}}],"lastImportedAt":"2025-11-26T00:20:49.406Z","outputLimit":131000,"contextLength":197000},{"id":"ministral-3b","aliases":["mistral-ai/ministral-3b","mistralai/ministral-3b","mistral/ministral-3b","ministral-3b"],"description":{"en":"Ministral 3B is a 3B parameter model optimized for on-device and edge computing. It excels in knowledge, commonsense reasoning, and function-calling, outperforming larger models like Mistral 7B on most benchmarks. Supporting up to 128k context length, it’s ideal for orchestrating agentic workflows and specialist tasks with efficient inference.","de":"Ministral 3B ist ein 3B-Parameter-Modell, das für On-Device- und Edge-Computing optimiert ist. Es zeichnet sich durch Wissen, Commonsense-Reasoning und Funktionsaufrufe aus und übertrifft bei den meisten Benchmarks größere Modelle wie Mistral 7B. Es unterstützt bis zu 128k Kontextlängen und ist ideal für die Orchestrierung von agentenbasierten Workflows und Spezialaufgaben mit effizienter Inferenz."},"name":"Ministral 3B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.04"},"eur":{"currency":"eur","input":"0.03","output":"0.03"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.04"},"eur":{"currency":"eur","input":"0.03","output":"0.03"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.04"},"eur":{"currency":"eur","input":"0.03","output":"0.03"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.04"},"eur":{"currency":"eur","input":"0.03","output":"0.03"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-30T00:23:18.822Z","outputLimit":8192,"contextLength":128000},{"id":"ministral-8b","aliases":["mistralai/ministral-8b","mistral/ministral-8b","ministral-8b"],"description":{"en":"Ministral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.","de":"Ministral 8B ist ein 8B-Parametermodell mit einem einzigartigen, verschachtelten Sliding-Window-Attention-Muster für schnellere, speichereffiziente Inferenz. Es wurde für Grenzfälle entwickelt, unterstützt bis zu 128k Kontextlänge und zeichnet sich bei Wissens- und Schlussfolgerungsaufgaben aus. Es übertrifft die Konkurrenz in der Sub-10B-Kategorie und eignet sich daher perfekt für Anwendungen mit geringer Latenz, bei denen die Privatsphäre im Vordergrund steht."},"name":"Ministral 8B","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.10"},"eur":{"currency":"eur","input":"0.08627114","output":"0.08627114"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.1"},"eur":{"currency":"eur","input":"0.08627114","output":"0.08627114"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.705Z"},{"id":"mistral-31-24b","aliases":["mistral-31-24b"],"name":"Venice Medium","toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"venice","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.4313557","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.716Z"},{"id":"mistral-7b-instruct","aliases":["mistralai/mistral-7b-instruct:free","mistralai/mistral-7b-instruct-v0.3","mistralai/mistral-7b-instruct","mistral-7b-instruct:free","mistral-7b-instruct-v0.3","mistral-7b-instruct"],"description":{"en":"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length. *Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*","de":"Ein leistungsfähiges, dem Industriestandard entsprechendes 7,3B-Parameter-Modell mit Optimierungen für Geschwindigkeit und Kontextlänge. *Mistral 7B Instruct hat mehrere Versionsvarianten, und dies soll die neueste Version sein."},"name":"Mistral 7B Instruct (free)","toolCalling":true,"openWeights":true,"knowledge":"2024-05-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","tool_choice","top_k","top_p","logit_bias"],"providers":[{"providerId":"openrouter","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.028","output":"0.054"},"eur":{"currency":"eur","input":"0.02","output":"0.05"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":32768,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.503Z","outputLimit":32768,"contextLength":32768},{"id":"mistral-7b-instruct-v0.1","aliases":["workers-ai/mistral-7b-instruct-v0.1","mistralai/mistral-7b-instruct-v0.1","mistral-7b-instruct-v0.1"],"description":{"en":"A 7.3B parameter model that outperforms Llama 2 13B on all benchmarks, with optimizations for speed and context length.","de":"Ein 7,3B-Parameter-Modell, das Llama 2 13B bei allen Benchmarks übertrifft, mit Optimierungen für Geschwindigkeit und Kontextlänge."},"name":"@cf/mistral/mistral-7b-instruct-v0.1","toolCalling":true,"openWeights":true,"knowledge":"2023-09-28","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","seed","top_k","top_p"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":2824,"outputLimit":2824,"price":{"usd":{"currency":"usd","input":"0.11","output":"0.19"},"eur":{"currency":"eur","input":"0.09","output":"0.16"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":2824,"price":{"usd":{"currency":"usd","input":"0.11","output":"0.19"},"eur":{"currency":"eur","input":"0.09","output":"0.16"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:41.697Z","outputLimit":2824,"contextLength":2824},{"id":"mistral-7b-instruct-v0.1-awq","aliases":["mistral-7b-instruct-v0.1-awq"],"name":"@hf/thebloke/mistral-7b-instruct-v0.1-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.371Z","outputLimit":4096,"contextLength":4096},{"id":"mistral-7b-instruct-v0.2","aliases":["mistralai/mistral-7b-instruct-v0.2","mistral-7b-instruct-v0.2"],"description":{"en":"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length. An improved version of [Mistral 7B Instruct](/modelsmistralai/mistral-7b-instruct-v0.1), with the following changes: - 32k context window (vs 8k context in v0.1) - Rope-theta = 1e6 - No Sliding-Window Attention","de":"Ein leistungsfähiges, dem Industriestandard entsprechendes 7,3B-Parameter-Modell mit Optimierungen für Geschwindigkeit und Kontextlänge. Eine verbesserte Version von [Mistral 7B Instruct](/modelsmistralai/mistral-7b-instruct-v0.1), mit den folgenden Änderungen: - 32k Kontextfenster (gegenüber 8k Kontext in v0.1) - Seil-Theta = 1e6 - Kein Sliding-Window Aufmerksamkeit"},"name":"@hf/mistral/mistral-7b-instruct-v0.2","toolCalling":true,"openWeights":true,"knowledge":"2023-12-28","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","stop","top_k","top_p"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":3072,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.2","output":"0.2"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:40.426Z","outputLimit":4096,"contextLength":3072},{"id":"mistral-7b-instruct-v0.2-lora","aliases":["mistral-7b-instruct-v0.2-lora"],"name":"@cf/mistral/mistral-7b-instruct-v0.2-lora","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":15000,"outputLimit":15000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.915Z","outputLimit":15000,"contextLength":15000},{"id":"mistral-7b-instruct-v0.3","aliases":["mistralai/mistral-7b-instruct-v0.3","mistral-7b-instruct-v0.3"],"description":{"en":"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length. An improved version of [Mistral 7B Instruct v0.2](/models/mistralai/mistral-7b-instruct-v0.2), with the following changes: - Extended vocabulary to 32768 - Supports v3 Tokenizer - Supports function calling NOTE: Support for function calling depends on the provider.","de":"Ein leistungsfähiges, dem Industriestandard entsprechendes 7,3B-Parameter-Modell mit Optimierungen für Geschwindigkeit und Kontextlänge. Eine verbesserte Version von [Mistral 7B Instruct v0.2](/models/mistralai/mistral-7b-instruct-v0.2), mit den folgenden Änderungen: - Erweiterung des Vokabulars auf 32768 - Unterstützt v3 Tokenizer - Unterstützt Funktionsaufrufe HINWEIS: Die Unterstützung für Funktionsaufrufe hängt vom jeweiligen Anbieter ab."},"name":"Mistral-7B-Instruct-v0.3","toolCalling":true,"openWeights":true,"knowledge":"2024-05-27","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","stop","top_k","top_p"],"providers":[{"providerId":"ovhcloud","contextLength":127000,"outputLimit":127000,"price":{"usd":{"currency":"usd","input":"0.11","output":"0.11"},"eur":{"currency":"eur","input":"0.094898254","output":"0.094898254"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.2","output":"0.2"},"eur":{"currency":"eur","input":"0.17254228","output":"0.17254228"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.753Z"},{"id":"mistral-embed","aliases":["mistral-embed"],"name":"Mistral Embed","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"mistral","contextLength":8000,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.00"},"eur":{"currency":"eur","input":"0.09","output":"0"}}}],"lastImportedAt":"2025-12-12T00:21:30.951Z","outputLimit":3072,"contextLength":8000},{"id":"mistral-large","aliases":["mistralai/mistral-large","mistral/mistral-large","mistral-large"],"description":{"en":"This is Mistral AI's flagship model, Mistral Large 2 (version `mistral-large-2407`). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/). It supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.","de":"Dies ist das Vorzeigemodell von Mistral AI, Mistral Large 2 (Version `mistral-large-2407`). Es ist ein proprietäres, gewichtetes Modell und zeichnet sich durch Argumentation, Code, JSON, Chat und mehr aus. Lesen Sie die Ankündigung der Markteinführung [hier] (https://mistral.ai/news/mistral-large-2407/). Es unterstützt Dutzende von Sprachen, darunter Französisch, Deutsch, Spanisch, Italienisch, Portugiesisch, Arabisch, Hindi, Russisch, Chinesisch, Japanisch und Koreanisch, sowie über 80 Programmiersprachen, darunter Python, Java, C, C++, JavaScript und Bash. Das lange Kontextfenster ermöglicht den präzisen Abruf von Informationen aus großen Dokumenten."},"name":"Mistral Large","toolCalling":true,"openWeights":true,"knowledge":"2024-02-26","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"vercel","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.50"},"eur":{"currency":"eur","input":"0.43","output":"1.29"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"2","output":"6"},"eur":{"currency":"eur","input":"1.72","output":"5.15"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.335Z","outputLimit":262144,"contextLength":128000},{"id":"mistral-medium-3","aliases":["mistralai/mistral-medium-3","mistral-medium-3"],"description":{"en":"Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases. The model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.","de":"Mistral Medium 3 ist ein hochleistungsfähiges Sprachmodell für Unternehmen, das entwickelt wurde, um Fähigkeiten der Spitzenklasse zu deutlich reduzierten Betriebskosten zu bieten. Es vereint modernste Argumentation und multimodale Leistung mit 8-fach geringeren Kosten im Vergleich zu herkömmlichen großen Modellen und eignet sich damit für skalierbare Einsätze in professionellen und industriellen Anwendungsfällen. Das Modell eignet sich hervorragend für Bereiche wie Codierung, STEM-Reasoning und Unternehmensanpassung. Es unterstützt Hybrid-, On-Prem- und In-VPC-Implementierungen und ist für die Integration in benutzerdefinierte Workflows optimiert. Mistral Medium 3 bietet eine wettbewerbsfähige Genauigkeit im Vergleich zu größeren Modellen wie Claude Sonnet 3.5/3.7, Llama 4 Maverick und Command R+, wobei eine breite Kompatibilität über Cloud-Umgebungen hinweg gewährleistet ist."},"name":"Mistral Medium 3","toolCalling":true,"knowledge":"2025-05-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.40","output":"2.00"},"eur":{"currency":"eur","input":"0.34","output":"1.72"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.503Z","outputLimit":131072,"contextLength":131072},{"id":"mistral-medium-3.1","aliases":["mistralai/mistral-medium-3.1","mistral-medium-3.1"],"description":{"en":"Mistral Medium 3.1 is an updated version of Mistral Medium 3, which is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases. The model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3.1 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.","de":"Mistral Medium 3.1 ist eine aktualisierte Version von Mistral Medium 3, einem hochleistungsfähigen Sprachmodell für Unternehmen, das entwickelt wurde, um Fähigkeiten auf dem neuesten Stand der Technik bei deutlich reduzierten Betriebskosten zu bieten. Es vereint modernste Argumentation und multimodale Leistung mit 8-fach geringeren Kosten im Vergleich zu herkömmlichen großen Modellen und eignet sich damit für skalierbare Einsätze in professionellen und industriellen Anwendungsfällen. Das Modell eignet sich hervorragend für Bereiche wie Codierung, STEM-Reasoning und Unternehmensanpassung. Es unterstützt hybride, On-Prem- und In-VPC-Implementierungen und ist für die Integration in benutzerdefinierte Workflows optimiert. Mistral Medium 3.1 bietet eine wettbewerbsfähige Genauigkeit im Vergleich zu größeren Modellen wie Claude Sonnet 3.5/3.7, Llama 4 Maverick und Command R+, wobei eine breite Kompatibilität über Cloud-Umgebungen hinweg erhalten bleibt."},"name":"Mistral Medium 3.1","toolCalling":true,"knowledge":"2025-05-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.40","output":"2.00"},"eur":{"currency":"eur","input":"0.34","output":"1.72"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.503Z","outputLimit":262144,"contextLength":131072},{"id":"mistral-nemo","aliases":["mistralai/mistral-nemo-instruct-2407","mistralai/mistral-nemo:free","mistral-nemo-instruct-2407","mistral-ai/mistral-nemo","mistralai/mistral-nemo","mistral-nemo:free","mistral-nemo"],"description":{"en":"A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA. The model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi. It supports function calling and is released under the Apache 2.0 license.","de":"Ein 12B-Parameter-Modell mit einer 128k-Token-Kontextlänge, das von Mistral in Zusammenarbeit mit NVIDIA entwickelt wurde. Das Modell ist mehrsprachig und unterstützt Englisch, Französisch, Deutsch, Spanisch, Italienisch, Portugiesisch, Chinesisch, Japanisch, Koreanisch, Arabisch und Hindi. Es unterstützt Funktionsaufrufe und ist unter der Apache 2.0 Lizenz veröffentlicht."},"name":"Mistral Nemo","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-03-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"mistral","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.13","output":"0.13"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.13","output":"0.13"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":16400,"price":{"usd":{"currency":"usd","input":"20.00","output":"40.00"},"eur":{"currency":"eur","input":"17.16","output":"34.33"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.13","output":"0.13"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.04"},"eur":{"currency":"eur","input":"0.02","output":"0.03"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":131072,"outputLimit":131072,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.335Z","outputLimit":8192,"contextLength":128000},{"id":"mistral-nemo-12b-instruct","aliases":["mistral/mistral-nemo-12b-instruct","mistral-nemo-12b-instruct"],"name":"Mistral Nemo 12B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"inference","contextLength":16000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.10"},"eur":{"currency":"eur","input":"0.034508456","output":"0.08627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.754Z"},{"id":"mistral-saba","aliases":["mistralai/mistral-saba-2502","mistralai/mistral-saba","mistral-saba-2502","mistral-saba"],"name":"Mistral: Saba","description":{"en":"Mistral Saba is a 24B-parameter language model specifically designed for the Middle East and South Asia, delivering accurate and contextually relevant responses while maintaining efficient performance. Trained on curated regional datasets, it supports multiple Indian-origin languages—including Tamil and Malayalam—alongside Arabic. This makes it a versatile option for a range of regional and multilingual applications. Read more at the blog post [here](https://mistral.ai/en/news/mistral-saba)","de":"Mistral Saba ist ein Sprachmodell mit 24 B-Parametern, das speziell für den Nahen Osten und Südasien entwickelt wurde. Es liefert genaue und kontextrelevante Antworten bei gleichzeitig effizienter Leistung. Es wurde auf kuratierten regionalen Datensätzen trainiert und unterstützt neben Arabisch mehrere Sprachen indischen Ursprungs, darunter Tamil und Malayalam. Dies macht sie zu einer vielseitigen Option für eine Reihe von regionalen und mehrsprachigen Anwendungen. Lesen Sie mehr in dem Blogbeitrag [hier] (https://mistral.ai/en/news/mistral-saba)"},"knowledge":"2025-02-17","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.2","output":"0.6"},"eur":{"currency":"eur","input":"0.17254228","output":"0.51762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"mistral-saba-24b","aliases":["mistral-saba-24b"],"name":"Mistral Saba 24B","toolCalling":true,"knowledge":"2024-08-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"groq","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.79","output":"0.79"},"eur":{"currency":"eur","input":"0.68","output":"0.68"}}}],"lastImportedAt":"2025-12-08T00:21:39.692Z","outputLimit":32768,"contextLength":32768},{"id":"mistral-small","aliases":["mistral/mistral-small","mistral-small"],"description":{"en":"With 22 billion parameters, Mistral Small v24.09 offers a convenient mid-point between (Mistral NeMo 12B)[/mistralai/mistral-nemo] and (Mistral Large 2)[/mistralai/mistral-large], providing a cost-effective solution that can be deployed across various platforms and environments. It has better reasoning, exhibits more capabilities, can produce and reason about code, and is multiligual, supporting English, French, German, Italian, and Spanish.","de":"Mit 22 Milliarden Parametern bietet Mistral Small v24.09 einen bequemen Mittelweg zwischen (Mistral NeMo 12B)[/mistralai/mistral-nemo] und (Mistral Large 2)[/mistralai/mistral-large] und stellt eine kostengünstige Lösung dar, die auf verschiedenen Plattformen und Umgebungen eingesetzt werden kann. Es hat eine bessere Argumentation, weist mehr Fähigkeiten auf, kann Code erzeugen und argumentieren und ist mehrsprachig und unterstützt Englisch, Französisch, Deutsch, Italienisch und Spanisch."},"name":"Mistral Small","toolCalling":true,"openWeights":true,"knowledge":"2024-02-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.30"},"eur":{"currency":"eur","input":"0.09","output":"0.26"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"75.00","output":"200.00"},"eur":{"currency":"eur","input":"64.37","output":"171.64"}}}],"lastImportedAt":"2025-12-09T00:20:58.335Z","outputLimit":16384,"contextLength":128000},{"id":"mistral-small-3.1-24b-instruct","aliases":["mistralai/mistral-small-3.1-24b-instruct:free","mistralai/mistral-small-3.1-24b-instruct-2503","workers-ai/mistral-small-3.1-24b-instruct","mistralai/mistral-small-3.1-24b-instruct","mistral-small-3.1-24b-instruct:free","mistral-small-3.1-24b-instruct-2503","mistral-small-3.1-24b-instruct"],"description":{"en":"Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)","de":"Mistral Small 3.1 24B Instruct ist eine aktualisierte Variante von Mistral Small 3 (2501) mit 24 Milliarden Parametern und erweiterten multimodalen Fähigkeiten. Es bietet State-of-the-Art-Leistung bei textbasierten Schlussfolgerungen und Sehaufgaben, einschließlich Bildanalyse, Programmierung, mathematische Schlussfolgerungen und mehrsprachige Unterstützung für Dutzende von Sprachen. Ausgestattet mit einem umfangreichen 128k-Token-Kontextfenster und optimiert für effiziente lokale Inferenz, unterstützt es Anwendungsfälle wie Konversationsagenten, Funktionsaufrufe, das Verstehen langer Dokumente und datenschutzsensitive Einsätze. Die aktualisierte Version ist [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)"},"name":"@cf/mistralai/mistral-small-3.1-24b-instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","stop","structured_outputs","tool_choice","top_k","top_p","repetition_penalty","seed"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.35","output":"0.56"},"eur":{"currency":"eur","input":"0.3","output":"0.48"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.11"},"eur":{"currency":"eur","input":"0.03","output":"0.09"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":128000,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:41.207Z","outputLimit":8192,"contextLength":128000},{"id":"mistral-small-3.2-24b-instruct","aliases":["mistralai/mistral-small-3.2-24b-instruct:free","mistralai/mistral-small-3.2-24b-instruct-2506","mistralai/mistral-small-3.2-24b-instruct","mistral-small-3.2-24b-instruct:free","mistral-small-3.2-24b-instruct-2506","mistral-small-3.2-24b-instruct"],"description":{"en":"Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistral optimized for instruction following, repetition reduction, and improved function calling. Compared to the 3.1 release, version 3.2 significantly improves accuracy on WildBench and Arena Hard, reduces infinite generations, and delivers gains in tool use and structured output tasks. It supports image and text inputs with structured outputs, function/tool calling, and strong performance across coding (HumanEval+, MBPP), STEM (MMLU, MATH, GPQA), and vision benchmarks (ChartQA, DocVQA).","de":"Mistral-Small-3.2-24B-Instruct-2506 ist ein aktualisiertes 24B-Parameter-Modell von Mistral, das für das Verfolgen von Anweisungen, die Reduzierung von Wiederholungen und verbesserte Funktionsaufrufe optimiert ist. Im Vergleich zur Version 3.1 verbessert die Version 3.2 die Genauigkeit auf WildBench und Arena Hard erheblich, reduziert unendliche Generationen und bietet Vorteile bei der Verwendung von Tools und strukturierten Ausgabeaufgaben. Sie unterstützt Bild- und Texteingaben mit strukturierten Ausgaben, Funktions-/Werkzeugaufrufe und eine starke Leistung bei Codierungs- (HumanEval+, MBPP), MINT- (MMLU, MATH, GPQA) und Vision-Benchmarks (ChartQA, DocVQA)."},"name":"Mistral Small 3.2 24B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"openrouter","contextLength":96000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.18"},"eur":{"currency":"eur","input":"0.05","output":"0.15"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":96000,"outputLimit":96000,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:42.503Z","outputLimit":8192,"contextLength":96000},{"id":"mistral-tiny","aliases":["mistralai/mistral-tiny","mistral-tiny"],"name":"Mistral Tiny","description":{"en":"Note: This model is being deprecated. Recommended replacement is the newer [Ministral 8B](/mistral/ministral-8b) This model is currently powered by Mistral-7B-v0.2, and incorporates a \"better\" fine-tuning than [Mistral 7B](/models/mistralai/mistral-7b-instruct-v0.1), inspired by community work. It's best used for large batch processing tasks where cost is a significant factor but reasoning capabilities are not crucial.","de":"Hinweis: Dieses Modell wird nicht mehr verwendet. Als Ersatz wird das neuere Modell [Ministral 8B](/mistral/ministral-8b) empfohlen. Dieses Modell wird derzeit von Mistral-7B-v0.2 angetrieben und beinhaltet eine \"bessere\" Feinabstimmung als [Mistral 7B](/models/mistralai/mistral-7b-instruct-v0.1), inspiriert durch die Arbeit der Gemeinschaft. Er eignet sich am besten für große Stapelverarbeitungsaufgaben, bei denen die Kosten ein wichtiger Faktor sind, die Argumentationsfähigkeiten aber nicht entscheidend sind."},"knowledge":"2024-01-10","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.25","output":"0.25"},"eur":{"currency":"eur","input":"0.21567785","output":"0.21567785"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"mixtral-8x22b-instruct","aliases":["mistralai/mixtral-8x22b-instruct-v0.1","mistralai/mixtral-8x22b-instruct","mistral/mixtral-8x22b-instruct","mixtral-8x22b-instruct-v0.1","mixtral-8x22b-instruct"],"description":{"en":"Mistral's official instruct fine-tuned version of [Mixtral 8x22B](/models/mistralai/mixtral-8x22b). It uses 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Its strengths include: - strong math, coding, and reasoning - large context length (64k) - fluency in English, French, Italian, German, and Spanish See benchmarks on the launch announcement [here](https://mistral.ai/news/mixtral-8x22b/). #moe","de":"Die offizielle, von Mistral beauftragte, fein abgestimmte Version des [Mixtral 8x22B](/models/mistralai/mixtral-8x22b). Es verwendet 39B aktive Parameter von 141B und bietet damit eine unvergleichliche Kosteneffizienz für seine Größe. Zu seinen Stärken gehören: - starke Mathematik, Kodierung und Argumentation - große Kontextlänge (64k) - fließende Beherrschung von Englisch, Französisch, Italienisch, Deutsch und Spanisch Siehe Benchmarks in der Ankündigung der Markteinführung [hier] (https://mistral.ai/news/mixtral-8x22b/). #moe"},"name":"Mixtral 8x22B","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"vercel","contextLength":64000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"2.00","output":"6.00"},"eur":{"currency":"eur","input":"1.7254228","output":"5.1762684"}}},{"providerId":"openrouter","contextLength":65536,"price":{"usd":{"currency":"usd","input":"2","output":"6"},"eur":{"currency":"eur","input":"1.7254228","output":"5.1762684"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.706Z"},{"id":"mixtral-8x7b-instruct","aliases":["mistralai/mixtral-8x7b-instruct-v0.1","mistralai/mixtral-8x7b-instruct","mixtral-8x7b-instruct-v0.1","mixtral-8x7b-instruct"],"name":"Mistral: Mixtral 8x7B Instruct","description":{"en":"Mixtral 8x7B Instruct is a pretrained generative Sparse Mixture of Experts, by Mistral AI, for chat and instruction use. Incorporates 8 experts (feed-forward networks) for a total of 47 billion parameters. Instruct model fine-tuned by Mistral. #moe","de":"Mixtral 8x7B Instruct ist eine vortrainierte generative Sparse Mixture of Experts, von Mistral AI, für den Einsatz im Chat und im Unterricht. Enthält 8 Experten (Feed-Forward-Netzwerke) für insgesamt 47 Milliarden Parameter. Das Instruct-Modell wurde von Mistral feinabgestimmt. #moe"},"knowledge":"2023-12-10","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.54","output":"0.54"},"eur":{"currency":"eur","input":"0.465864156","output":"0.465864156"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"mixtral-8x7b-instruct-v0.1","aliases":["mixtral-8x7b-instruct-v0.1"],"name":"Mixtral-8x7B-Instruct-v0.1","openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"ovhcloud","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.70","output":"0.70"},"eur":{"currency":"eur","input":"0.60389798","output":"0.60389798"}}}],"lastImportedAt":"2025-11-19T12:06:32.753Z"},{"id":"model-router","aliases":["model-router"],"name":"Model Router","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"azure","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.00"},"eur":{"currency":"eur","input":"0.12","output":"0"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.00"},"eur":{"currency":"eur","input":"0.12","output":"0"}}}],"lastImportedAt":"2025-11-30T00:23:20.714Z","outputLimit":16384,"contextLength":128000},{"id":"moonshot-kimi-k2-instruct","aliases":["moonshot-kimi-k2-instruct"],"name":"Moonshot Kimi K2 Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.57","output":"2.29"},"eur":{"currency":"eur","input":"0.491745498","output":"1.975609106"}}}],"lastImportedAt":"2025-11-19T12:06:32.715Z"},{"id":"moonshotai-kimi-dev-72b","aliases":["moonshotai-kimi-dev-72b"],"name":"moonshotai/Kimi-Dev-72B","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.29","output":"1.15"},"eur":{"currency":"eur","input":"0.25","output":"1"}}}],"lastImportedAt":"2025-11-26T00:20:45.445Z","outputLimit":131000,"contextLength":131000},{"id":"moonshotai-kimi-k2-instruct","aliases":["moonshotai-kimi-k2-instruct"],"name":"moonshotai/Kimi-K2-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.58","output":"2.29"},"eur":{"currency":"eur","input":"0.5","output":"1.99"}}}],"lastImportedAt":"2025-11-26T00:20:46.323Z","outputLimit":131000,"contextLength":131000},{"id":"moonshotai-kimi-k2-instruct-0905","aliases":["moonshotai-kimi-k2-instruct-0905"],"name":"moonshotai/Kimi-K2-Instruct-0905","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.40","output":"2.00"},"eur":{"currency":"eur","input":"0.35","output":"1.73"}}}],"lastImportedAt":"2025-11-30T00:23:20.833Z","outputLimit":262000,"contextLength":262000},{"id":"moonshotai-kimi-k2-thinking","aliases":["moonshotai-kimi-k2-thinking"],"name":"moonshotai/Kimi-K2-Thinking","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.55","output":"2.50"},"eur":{"currency":"eur","input":"0.47","output":"2.16"}}}],"lastImportedAt":"2025-11-30T00:23:20.974Z","outputLimit":262000,"contextLength":262000},{"id":"morph-v3-fast","aliases":["morph/morph-v3-fast","morph-v3-fast"],"description":{"en":"Morph's fastest apply model for code edits. ~10,500 tokens/sec with 96% accuracy for rapid code transformations. The model requires the prompt to be in the following format: <instruction>{instruction}</instruction> <code>{initial_code}</code> <update>{edit_snippet}</update> Zero Data Retention is enabled for Morph. Learn more about this model in their [documentation](https://docs.morphllm.com/quickstart)","de":"Morphs schnellstes Anwendungsmodell für Code-Bearbeitungen. ~10.500 Token/Sek. mit 96 % Genauigkeit für schnelle Code-Transformationen. Das Modell erfordert, dass die Eingabeaufforderung das folgende Format hat: <Anweisung>{Anweisung}</Anweisung> <code>{Anfangscode}</code> <update>{edit_snippet}</update> Zero Data Retention ist für Morph aktiviert. Erfahren Sie mehr über dieses Modell in der [Dokumentation] (https://docs.morphllm.com/quickstart)"},"name":"Morph v3 Fast","knowledge":"2025-07-07","input":["text"],"output":["text"],"parameters":["max_tokens","stop","temperature"],"providers":[{"providerId":"vercel","contextLength":16000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.80","output":"1.20"},"eur":{"currency":"eur","input":"0.69016912","output":"1.03525368"}}},{"providerId":"morph","contextLength":16000,"outputLimit":16000,"price":{"usd":{"currency":"usd","input":"0.80","output":"1.20"},"eur":{"currency":"eur","input":"0.69016912","output":"1.03525368"}}},{"providerId":"openrouter","contextLength":81920,"price":{"usd":{"currency":"usd","input":"0.8","output":"1.2"},"eur":{"currency":"eur","input":"0.69016912","output":"1.03525368"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.711Z"},{"id":"morph-v3-large","aliases":["morph/morph-v3-large","morph-v3-large"],"description":{"en":"Morph's high-accuracy apply model for complex code edits. ~4,500 tokens/sec with 98% accuracy for precise code transformations. The model requires the prompt to be in the following format: <instruction>{instruction}</instruction> <code>{initial_code}</code> <update>{edit_snippet}</update> Zero Data Retention is enabled for Morph. Learn more about this model in their [documentation](https://docs.morphllm.com/quickstart)","de":"Morphs hochpräzises Anwendungsmodell für komplexe Code-Bearbeitungen. ~4.500 Token/Sek. mit 98 % Genauigkeit für präzise Code-Transformationen. Das Modell verlangt, dass die Eingabeaufforderung das folgende Format hat: <Anweisung>{Anweisung}</Anweisung> <code>{Anfangscode}</code> <update>{edit_snippet}</update> Zero Data Retention ist für Morph aktiviert. Erfahren Sie mehr über dieses Modell in der [Dokumentation] (https://docs.morphllm.com/quickstart)"},"name":"Morph v3 Large","knowledge":"2025-07-07","input":["text"],"output":["text"],"parameters":["max_tokens","stop","temperature"],"providers":[{"providerId":"vercel","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.90","output":"1.90"},"eur":{"currency":"eur","input":"0.77644026","output":"1.63915166"}}},{"providerId":"morph","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.90","output":"1.90"},"eur":{"currency":"eur","input":"0.77644026","output":"1.63915166"}}},{"providerId":"openrouter","contextLength":262144,"price":{"usd":{"currency":"usd","input":"0.9","output":"1.9"},"eur":{"currency":"eur","input":"0.77644026","output":"1.63915166"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.711Z"},{"id":"mythomax-l2-13b","aliases":["gryphe/mythomax-l2-13b","mythomax-l2-13b"],"name":"MythoMax 13B","description":{"en":"One of the highest performing and most popular fine-tunes of Llama 2 13B, with rich descriptions and roleplay. #merge","de":"Eine der leistungsstärksten und beliebtesten Feinheiten von Llama 2 13B, mit reichen Beschreibungen und Rollenspiel. #merge"},"knowledge":"2023-07-02","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_a","top_k","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":4096,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.06"},"eur":{"currency":"eur","input":"0.051762684","output":"0.051762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.768Z"},{"id":"nano-banana","aliases":["google/nano-banana","nano-banana"],"name":"Nano-Banana","toolCalling":true,"input":["text","image"],"output":["text","image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.21","output":"1.80"},"eur":{"currency":"eur","input":"0.18","output":"1.55"}}}],"lastImportedAt":"2025-12-10T00:21:34.460Z","contextLength":32768},{"id":"nano-banana-pro","aliases":["google/nano-banana-pro","nano-banana-pro"],"name":"Nano-Banana-Pro","toolCalling":true,"input":["text","image"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":65536,"price":{"usd":{"currency":"usd","input":"1.60","output":"9.60"},"eur":{"currency":"eur","input":"1.37","output":"8.24"}}}],"lastImportedAt":"2025-12-10T00:21:34.510Z","contextLength":65536},{"id":"nemoretriever-ocr-v1","aliases":["nvidia/nemoretriever-ocr-v1","nemoretriever-ocr-v1"],"description":{"en":"Nemotron OCR v1 is an advanced text recognition model for OCR tasks, integrating text detection, transcription, and layout analysis. It's suitable for diverse applications in document and scene image processing.","de":"Nemotron OCR v1 ist ein fortschrittliches Texterkennungsmodell für OCR-Aufgaben, das Texterkennung, Transkription und Layoutanalyse integriert. Es ist für verschiedene Anwendungen in der Dokumenten- und Szenenbildverarbeitung geeignet."},"name":"NeMo Retriever OCR v1","knowledge":"2024-01-01","input":["image"],"output":["text"],"parameters":[],"providers":[{"providerId":"nvidia","outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.693Z"},{"id":"nemotron-nano-12b-v2-vl","aliases":["nvidia/nvidia-nemotron-nano-12b-v2-vl-bf16","nvidia/nemotron-nano-12b-v2-vl:free","nvidia-nemotron-nano-12b-v2-vl-bf16","nvidia/nemotron-nano-12b-v2-vl","nemotron-nano-12b-v2-vl:free","nemotron-nano-12b-v2-vl"],"name":"NVIDIA: Nemotron Nano 12B 2 VL (free)","description":{"en":"NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for video understanding and document intelligence. It introduces a hybrid Transformer-Mamba architecture, combining transformer-level accuracy with Mamba’s memory-efficient sequence modeling for significantly higher throughput and lower latency. The model supports inputs of text and multi-image documents, producing natural-language outputs. It is trained on high-quality NVIDIA-curated synthetic datasets optimized for optical-character recognition, chart reasoning, and multimodal comprehension. Nemotron Nano 2 VL achieves leading results on OCRBench v2 and scores ≈ 74 average across MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, and Video-MME—surpassing prior open VL baselines. With Efficient Video Sampling (EVS), it handles long-form videos while reducing inference cost. Open-weights, training data, and fine-tuning recipes are released under a permissive NVIDIA open license, with deployment supported across NeMo, NIM, and major inference runtimes.","de":"NVIDIA Nemotron Nano 2 VL ist ein offenes multimodales Schlussfolgerungsmodell mit 12 Milliarden Parametern, das für das Verstehen von Videos und Dokumenten entwickelt wurde. Er führt eine hybride Transformer-Mamba-Architektur ein, die die Genauigkeit auf Transformer-Ebene mit der speichereffizienten Sequenzmodellierung von Mamba kombiniert und so einen deutlich höheren Durchsatz und eine geringere Latenzzeit ermöglicht. Das Modell unterstützt Eingaben von Text- und Multibilddokumenten und erzeugt natürlichsprachliche Ausgaben. Es wurde auf hochwertigen, von NVIDIA kuratierten synthetischen Datensätzen trainiert, die für die Erkennung optischer Zeichen, die Erstellung von Diagrammen und das multimodale Verstehen optimiert wurden. Nemotron Nano 2 VL erzielt führende Ergebnisse in der OCRBench v2 und erreicht einen Durchschnitt von ≈ 74 über MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA und Video-MME - und übertrifft damit frühere offene VL-Baselines. Mit Efficient Video Sampling (EVS) können lange Videos verarbeitet und gleichzeitig die Inferenzkosten reduziert werden. Offene Gewichte, Trainingsdaten und Feinabstimmungsrezepte werden unter einer offenen NVIDIA-Lizenz veröffentlicht, wobei der Einsatz in NeMo, NIM und den wichtigsten Inferenz-Laufzeiten unterstützt wird."},"knowledge":"2025-10-28","reasoning":true,"toolCalling":true,"input":["image","text","video"],"output":["text"],"parameters":["include_reasoning","max_tokens","reasoning","seed","temperature","tool_choice","tools","top_p","frequency_penalty","min_p","presence_penalty","repetition_penalty","response_format","stop","top_k"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.2","output":"0.6"},"eur":{"currency":"eur","input":"0.17","output":"0.51"}}}],"lastImportedAt":"2025-12-12T00:21:33.763Z","contextLength":128000},{"id":"nemotron-nano-9b-v2","aliases":["nvidia/nvidia-nemotron-nano-9b-v2","nvidia/nemotron-nano-9b-v2:free","nvidia/nemotron-nano-9b-v2","nvidia-nemotron-nano-9b-v2","nemotron-nano-9b-v2:free","nemotron-nano-9b-v2"],"description":{"en":"NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so.","de":"NVIDIA-Nemotron-Nano-9B-v2 ist ein großes Sprachmodell (LLM), das von Grund auf von NVIDIA trainiert wurde und als einheitliches Modell für logische und nicht logische Aufgaben konzipiert ist. Es reagiert auf Benutzeranfragen und -aufgaben, indem es zunächst eine Argumentationsspur erzeugt und dann mit einer endgültigen Antwort abschließt. Die Argumentationsfähigkeiten des Modells können über eine Systemabfrage gesteuert werden. Wenn der Benutzer es vorzieht, dass das Modell seine endgültige Antwort ohne zwischengeschaltete Argumentationsspuren liefert, kann es so konfiguriert werden, dass es dies tut."},"name":"nvidia-nemotron-nano-9b-v2","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-09-01","input":["text"],"output":["text"],"parameters":["temperature","tools","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice","top_p","frequency_penalty","logit_bias","min_p","presence_penalty","repetition_penalty","stop","top_k"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.16"},"eur":{"currency":"eur","input":"0.03","output":"0.14"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":128000,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T00:21:33.755Z","outputLimit":131072,"contextLength":131072},{"id":"neural-chat-7b-v3-1-awq","aliases":["neural-chat-7b-v3-1-awq"],"name":"@hf/thebloke/neural-chat-7b-v3-1-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.637Z","outputLimit":4096,"contextLength":4096},{"id":"nex-agi-deepseek-v3.1-nex-n1","aliases":["nex-agi-deepseek-v3.1-nex-n1"],"name":"nex-agi/DeepSeek-V3.1-Nex-N1","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.43","output":"1.74"}}}],"lastImportedAt":"2025-11-26T00:20:49.823Z","outputLimit":131000,"contextLength":131000},{"id":"noromaid-20b","aliases":["neversleep/noromaid-20b-v0.1.1","neversleep/noromaid-20b","noromaid-20b-v0.1.1","noromaid-20b"],"name":"Noromaid 20B","description":{"en":"A collab between IkariDev and Undi. This merge is suitable for RP, ERP, and general knowledge. #merge #uncensored","de":"Eine Kollaboration zwischen IkariDev und Undi. Dieser Merge ist für RP, ERP und Allgemeinwissen geeignet. #merge #unzensiert"},"knowledge":"2023-11-26","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_a","top_k","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":4096,"price":{"usd":{"currency":"usd","input":"1","output":"1.75"},"eur":{"currency":"eur","input":"0.8627114","output":"1.50974495"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"nova-2-lite-v1","aliases":["amazon/nova-2-lite-v1:free","amazon/nova-2-lite-v1","nova-2-lite-v1:free","nova-2-lite-v1"],"name":"Amazon: Nova 2 Lite (free)","description":{"en":"Nova 2 Lite is a fast, cost-effective reasoning model for everyday workloads that can process text, images, and videos to generate text. Nova 2 Lite demonstrates standout capabilities in processing documents, extracting information from videos, generating code, providing accurate grounded answers, and automating multi-step agentic workflows.","de":"Nova 2 Lite ist ein schnelles, kosteneffizientes Reasoning-Modell für alltägliche Arbeitslasten, das Text, Bilder und Videos verarbeiten kann, um Text zu erzeugen. Nova 2 Lite zeigt herausragende Fähigkeiten bei der Verarbeitung von Dokumenten, der Extraktion von Informationen aus Videos, der Generierung von Code, der Bereitstellung präziser, fundierter Antworten und der Automatisierung von mehrstufigen agentenbasierten Workflows."},"knowledge":"2025-12-02","reasoning":true,"toolCalling":true,"input":["text","image","video","file"],"output":["text"],"parameters":["include_reasoning","max_tokens","reasoning","seed","stop","temperature","tool_choice","tools","top_p","top_k"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":1000000,"price":{"usd":{"currency":"usd","input":"0.3","output":"2.5"},"eur":{"currency":"eur","input":"0.26","output":"2.14"}}}],"lastImportedAt":"2025-12-05T00:21:32.545Z","contextLength":1000000},{"id":"nova-3","aliases":["workers-ai/nova-3","nova-3"],"name":"@cf/deepgram/nova-3","openWeights":true,"input":["audio","text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.01","output":"0.01"},"eur":{"currency":"eur","input":"0.01","output":"0.01"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.816Z","outputLimit":16384,"contextLength":128000},{"id":"nova-lite","aliases":["amazon/nova-lite","nova-lite"],"name":"Nova Lite","toolCalling":true,"knowledge":"2024-10-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":300000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.24"},"eur":{"currency":"eur","input":"0.051762684","output":"0.207050736"}}}],"lastImportedAt":"2025-11-19T12:06:32.711Z"},{"id":"nova-lite-v1","aliases":["amazon/nova-lite-v1","nova-lite-v1"],"name":"Amazon: Nova Lite 1.0","description":{"en":"Amazon Nova Lite 1.0 is a very low-cost multimodal model from Amazon that focused on fast processing of image, video, and text inputs to generate text output. Amazon Nova Lite can handle real-time customer interactions, document analysis, and visual question-answering tasks with high accuracy. With an input context of 300K tokens, it can analyze multiple images or up to 30 minutes of video in a single input.","de":"Amazon Nova Lite 1.0 ist ein sehr kostengünstiges multimodales Modell von Amazon, das sich auf die schnelle Verarbeitung von Bild-, Video- und Texteingaben zur Erzeugung von Textausgaben konzentriert. Amazon Nova Lite kann Kundeninteraktionen in Echtzeit, Dokumentenanalysen und visuelle Fragebeantwortungsaufgaben mit hoher Genauigkeit verarbeiten. Mit einem Eingabekontext von 300K Token kann es mehrere Bilder oder bis zu 30 Minuten Video in einer einzigen Eingabe analysieren."},"knowledge":"2024-12-05","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["max_tokens","stop","temperature","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":300000,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.24"},"eur":{"currency":"eur","input":"0.051762684","output":"0.207050736"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"nova-micro","aliases":["amazon/nova-micro","nova-micro"],"name":"Nova Micro","toolCalling":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.14"},"eur":{"currency":"eur","input":"0.034508456","output":"0.120779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.710Z"},{"id":"nova-micro-v1","aliases":["amazon/nova-micro-v1","nova-micro-v1"],"name":"Amazon: Nova Micro 1.0","description":{"en":"Amazon Nova Micro 1.0 is a text-only model that delivers the lowest latency responses in the Amazon Nova family of models at a very low cost. With a context length of 128K tokens and optimized for speed and cost, Amazon Nova Micro excels at tasks such as text summarization, translation, content classification, interactive chat, and brainstorming. It has simple mathematical reasoning and coding abilities.","de":"Amazon Nova Micro 1.0 ist ein reines Textmodell, das die niedrigsten Latenzzeiten in der Amazon Nova-Modellfamilie zu sehr geringen Kosten bietet. Mit einer Kontextlänge von 128K Token und optimiert für Geschwindigkeit und Kosten, eignet sich Amazon Nova Micro hervorragend für Aufgaben wie Textzusammenfassung, Übersetzung, Inhaltsklassifizierung, interaktiven Chat und Brainstorming. Es verfügt über einfache mathematische Argumentations- und Kodierungsfähigkeiten."},"knowledge":"2024-12-05","toolCalling":true,"input":["text"],"output":["text"],"parameters":["max_tokens","stop","temperature","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.035","output":"0.14"},"eur":{"currency":"eur","input":"0.030194899","output":"0.120779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"nova-premier-v1","aliases":["amazon/nova-premier-v1","nova-premier-v1"],"name":"Amazon: Nova Premier 1.0","description":{"en":"Amazon Nova Premier is the most capable of Amazon’s multimodal models for complex reasoning tasks and for use as the best teacher for distilling custom models.","de":"Amazon Nova Premier ist das leistungsfähigste der multimodalen Modelle von Amazon für komplexe Argumentationsaufgaben und für die Verwendung als bester Lehrmeister für die Destillation benutzerdefinierter Modelle."},"knowledge":"2025-10-31","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["max_tokens","stop","temperature","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":1000000,"price":{"usd":{"currency":"usd","input":"2.5","output":"12.5"},"eur":{"currency":"eur","input":"2.1567785","output":"10.7838925"}}}],"lastImportedAt":"2025-11-19T12:06:32.761Z"},{"id":"nova-pro","aliases":["amazon/nova-pro","nova-pro"],"name":"Nova Pro","toolCalling":true,"knowledge":"2024-10-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":300000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.80","output":"3.20"},"eur":{"currency":"eur","input":"0.69016912","output":"2.76067648"}}}],"lastImportedAt":"2025-11-19T12:06:32.710Z"},{"id":"nova-pro-v1","aliases":["amazon/nova-pro-v1","nova-pro-v1"],"description":{"en":"Amazon Nova Pro 1.0 is a capable multimodal model from Amazon focused on providing a combination of accuracy, speed, and cost for a wide range of tasks. As of December 2024, it achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX). Amazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and at analyzing financial documents. **NOTE**: Video input is not supported at this time.","de":"Amazon Nova Pro 1.0 ist ein leistungsfähiges multimodales Modell von Amazon, das sich darauf konzentriert, eine Kombination aus Genauigkeit, Geschwindigkeit und Kosten für eine breite Palette von Aufgaben zu bieten. Ab Dezember 2024 erreicht es bei wichtigen Benchmarks wie der Beantwortung visueller Fragen (TextVQA) und dem Verstehen von Videos (VATEX) die beste Leistung. Amazon Nova Pro zeigt starke Fähigkeiten bei der Verarbeitung sowohl visueller als auch textueller Informationen und bei der Analyse von Finanzdokumenten. **HINWEIS**: Die Videoeingabe wird derzeit nicht unterstützt."},"name":"Nova Pro 1.0","toolCalling":true,"knowledge":"2024-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","max_tokens","stop","top_k","top_p"],"providers":[{"providerId":"cortecs","contextLength":300000,"outputLimit":5000,"price":{"usd":{"currency":"usd","input":"1.02","output":"4.06"},"eur":{"currency":"eur","input":"0.879965628","output":"3.502608284"}}},{"providerId":"openrouter","contextLength":300000,"price":{"usd":{"currency":"usd","input":"0.8","output":"3.2"},"eur":{"currency":"eur","input":"0.69016912","output":"2.76067648"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.721Z"},{"id":"nvidia-nemotron-nano-9b-v2","aliases":["nvidia/nvidia-nemotron-nano-9b-v2","nvidia-nemotron-nano-9b-v2"],"description":{"en":"NVIDIA-Nemotron-Nano-9B-v2 is a multilingual large language model designed for reasoning and non-reasoning tasks, supporting various applications like chatbots and AI agents.","de":"NVIDIA-Nemotron-Nano-9B-v2 ist ein mehrsprachiges großes Sprachmodell, das für schlussfolgernde und nicht schlussfolgernde Aufgaben entwickelt wurde und verschiedene Anwendungen wie Chatbots und KI-Agenten unterstützt."},"name":"nvidia-nemotron-nano-9b-v2","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-09-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"nvidia","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.692Z"},{"id":"o1","aliases":["openai/o1-2024-12-17","o1-2024-12-17","openai/o1","o1"],"description":{"en":"The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding. The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. The o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).","de":"Die neueste und stärkste Modellfamilie von OpenAI, o1, wurde entwickelt, um mehr Zeit mit Denken zu verbringen, bevor es reagiert. Die o1-Modellreihe wird mit groß angelegtem Reinforcement Learning trainiert, um mit Hilfe von Gedankenketten zu denken. Die o1-Modelle sind für Mathematik, Wissenschaft, Programmierung und andere MINT-Aufgaben optimiert. Sie weisen bei Benchmarks in Physik, Chemie und Biologie durchweg eine Genauigkeit auf PhD-Niveau auf. Erfahren Sie mehr in der [Launch-Ankündigung] (https://openai.com/o1)."},"name":"o1","reasoning":true,"toolCalling":true,"knowledge":"2023-09-01","input":["text","image","file"],"output":["text"],"parameters":["tools","temperature","max_tokens","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"vercel","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"15.00","output":"60.00"},"eur":{"currency":"eur","input":"12.88","output":"51.51"}}},{"providerId":"github-models","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"15.00","output":"60.00"},"eur":{"currency":"eur","input":"12.88","output":"51.51"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"15.00","output":"60.00"},"eur":{"currency":"eur","input":"12.88","output":"51.51"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"15.00","output":"60.00"},"eur":{"currency":"eur","input":"12.88","output":"51.51"}}},{"providerId":"openai","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"15.00","output":"60.00"},"eur":{"currency":"eur","input":"12.88","output":"51.51"}}},{"providerId":"azure-cognitive-services","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"15.00","output":"60.00"},"eur":{"currency":"eur","input":"12.88","output":"51.51"}}},{"providerId":"poe","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"14.00","output":"54.00"},"eur":{"currency":"eur","input":"12.02","output":"46.36"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"15","output":"60"},"eur":{"currency":"eur","input":"12.88","output":"51.51"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.093Z","outputLimit":16384,"contextLength":128000},{"id":"o1-mini","aliases":["openai/o1-mini","o1-mini"],"name":"OpenAI o1-mini","reasoning":true,"toolCalling":true,"knowledge":"2023-09-01","input":["text"],"output":["text"],"parameters":["tools","temperature"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"helicone","contextLength":128000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"openai","contextLength":128000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}}],"lastImportedAt":"2025-12-09T00:20:58.410Z","outputLimit":16384,"contextLength":128000},{"id":"o1-pro","aliases":["openai/o1-pro","o1-pro"],"description":{"en":"The o1 series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o1-pro model uses more compute to think harder and provide consistently better answers.","de":"Die Modelle der o1-Serie werden mit Hilfe von Reinforcement Learning trainiert, um zu denken, bevor sie antworten, und komplexe Schlussfolgerungen zu ziehen. Das Modell o1-pro verwendet mehr Rechenleistung, um besser zu denken und durchgängig bessere Antworten zu liefern."},"name":"o1-pro","reasoning":true,"toolCalling":true,"knowledge":"2023-09-01","input":["text","image","file"],"output":["text"],"parameters":["tools","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs"],"providers":[{"providerId":"openai","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"150.00","output":"600.00"},"eur":{"currency":"eur","input":"130.15","output":"520.62"}}},{"providerId":"poe","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"140.00","output":"540.00"},"eur":{"currency":"eur","input":"121.48","output":"468.56"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"150","output":"600"},"eur":{"currency":"eur","input":"130.15","output":"520.62"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-21T07:05:34.351Z","outputLimit":100000,"contextLength":200000},{"id":"o3","aliases":["openai/o3-2025-04-16","o3-2025-04-16","openai/o3","o3"],"description":{"en":"o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.","de":"o3 ist ein vielseitiges und leistungsstarkes Modell für alle Bereiche. Es setzt neue Maßstäbe in den Bereichen Mathematik, Naturwissenschaften, Codierung und visuelles Denken. Es eignet sich auch hervorragend für technisches Schreiben und das Befolgen von Anweisungen. Verwenden Sie es, um mehrstufige Probleme zu durchdenken, die eine Analyse von Text, Code und Bildern erfordern."},"name":"o3 (Preview)","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text","image","file"],"output":["text"],"parameters":["tools","temperature","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"github-models","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"openai","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"azure-cognitive-services","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"poe","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.80","output":"7.20"},"eur":{"currency":"eur","input":"1.54","output":"6.18"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"2","output":"8"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.333Z","outputLimit":16384,"contextLength":128000},{"id":"o3-deep-research","aliases":["openai/o3-deep-research-2025-06-26","o3-deep-research-2025-06-26","openai/o3-deep-research","o3-deep-research"],"description":{"en":"o3-deep-research is OpenAI's advanced model for deep research, designed to tackle complex, multi-step research tasks. Note: This model always uses the 'web_search' tool which adds additional cost.","de":"o3-deep-research ist OpenAIs fortschrittliches Modell für Deep Research, das für die Bewältigung komplexer, mehrstufiger Forschungsaufgaben entwickelt wurde. Hinweis: Dieses Modell verwendet immer das Tool \"web_search\", das zusätzliche Kosten verursacht."},"name":"o3-deep-research","reasoning":true,"toolCalling":true,"knowledge":"2024-05-01","input":["text","image","file"],"output":["text"],"parameters":["tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","presence_penalty","reasoning","response_format","seed","stop","structured_outputs","temperature","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"openai","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"10.00","output":"40.00"},"eur":{"currency":"eur","input":"8.68","output":"34.71"}}},{"providerId":"poe","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"9.00","output":"36.00"},"eur":{"currency":"eur","input":"7.81","output":"31.24"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"10","output":"40"},"eur":{"currency":"eur","input":"8.68","output":"34.71"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-21T07:05:34.352Z","outputLimit":100000,"contextLength":200000},{"id":"o3-mini","aliases":["openai/o3-mini-2025-01-31","o3-mini-2025-01-31","openai/o3-mini","o3-mini"],"description":{"en":"OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding. This model supports the `reasoning_effort` parameter, which can be set to \"high\", \"medium\", or \"low\" to control the thinking time of the model. The default is \"medium\". OpenRouter also offers the model slug `openai/o3-mini-high` to default the parameter to \"high\". The model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities. The model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.","de":"OpenAI o3-mini ist ein kosteneffizientes Sprachmodell, das für MINT-Aufgaben optimiert ist und sich besonders in den Bereichen Wissenschaft, Mathematik und Programmierung auszeichnet. Dieses Modell unterstützt den Parameter `reasoning_effort`, der auf \"high\", \"medium\" oder \"low\" gesetzt werden kann, um die Denkzeit des Modells zu steuern. Die Voreinstellung ist \"mittel\". OpenRouter bietet auch den Modell-Slug `openai/o3-mini-high`, um den Parameter auf \"high\" zu setzen. Das Modell verfügt über drei einstellbare Denkaufwandstufen und unterstützt wichtige Entwicklerfähigkeiten wie Funktionsaufrufe, strukturierte Ausgaben und Streaming, obwohl es keine Bildverarbeitungsfähigkeiten enthält. Das Modell zeigt signifikante Verbesserungen gegenüber dem Vorgängermodell, wobei die Expertentester in 56 % der Fälle die Antworten des Modells bevorzugen und bei komplexen Fragen eine 39 %ige Verringerung der Hauptfehler feststellen. Bei mittlerem Reasoning-Aufwand erreicht o3-mini die Leistung des größeren o1-Modells bei anspruchsvollen Reasoning-Evaluierungen wie AIME und GPQA, bei gleichzeitig niedrigeren Latenzzeiten und Kosten."},"name":"o3-mini","reasoning":true,"toolCalling":true,"knowledge":"2023-10-01","input":["text","image","file"],"output":["text"],"parameters":["tools","temperature","max_tokens","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"github-models","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"openai","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"azure-cognitive-services","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"poe","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"0.99","output":"4.00"},"eur":{"currency":"eur","input":"0.85","output":"3.43"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"1.1","output":"4.4"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.329Z","outputLimit":16384,"contextLength":128000},{"id":"o3-mini-high","aliases":["openai/o3-mini-high-2025-01-31","o3-mini-high-2025-01-31","openai/o3-mini-high","o3-mini-high"],"description":{"en":"OpenAI o3-mini-high is the same model as [o3-mini](/openai/o3-mini) with reasoning_effort set to high. o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding. The model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities. The model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.","de":"OpenAI o3-mini-high ist das gleiche Modell wie [o3-mini](/openai/o3-mini), wobei reasoning_effort auf high gesetzt ist. o3-mini ist ein kosteneffizientes Sprachmodell, das für MINT-Aufgaben optimiert ist und sich besonders in den Bereichen Wissenschaft, Mathematik und Programmierung auszeichnet. Das Modell verfügt über drei einstellbare Reasoning-Aufwandsstufen und unterstützt wichtige Entwicklerfähigkeiten wie Funktionsaufrufe, strukturierte Ausgaben und Streaming, obwohl es keine Bildverarbeitungsfähigkeiten enthält. Das Modell zeigt signifikante Verbesserungen gegenüber dem Vorgängermodell, wobei die Expertentester in 56 % der Fälle die Antworten des Modells bevorzugen und bei komplexen Fragen eine 39 %ige Verringerung der Hauptfehler feststellen. Bei mittlerem Reasoning-Aufwand erreicht o3-mini die Leistung des größeren o1-Modells bei anspruchsvollen Reasoning-Evaluierungen wie AIME und GPQA, bei gleichzeitig niedrigeren Latenzzeiten und Kosten."},"name":"o3-mini-high","reasoning":true,"toolCalling":true,"knowledge":"2025-02-12","input":["text","image","file"],"output":["text"],"parameters":["tools","max_tokens","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"poe","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"0.99","output":"4.00"},"eur":{"currency":"eur","input":"0.86","output":"3.47"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"1.1","output":"4.4"},"eur":{"currency":"eur","input":"0.95","output":"3.82"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-21T07:05:37.808Z","outputLimit":100000,"contextLength":200000},{"id":"o3-pro","aliases":["openai/o3-pro-2025-06-10","o3-pro-2025-06-10","openai/o3-pro","o3-pro"],"description":{"en":"The o-series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o3-pro model uses more compute to think harder and provide consistently better answers. Note that BYOK is required for this model. Set up here: https://openrouter.ai/settings/integrations","de":"Die Modelle der o-Serie werden mit Hilfe von Reinforcement Learning darauf trainiert, zu denken, bevor sie antworten, und komplexe Schlussfolgerungen zu ziehen. Das Modell o3-pro verwendet mehr Rechenleistung, um mehr zu denken und durchgängig bessere Antworten zu liefern. Beachten Sie, dass BYOK für dieses Modell erforderlich ist. Hier einrichten: https://openrouter.ai/settings/integrations"},"name":"OpenAI o3 Pro","reasoning":true,"toolCalling":true,"knowledge":"2024-05-01","input":["text","image","file"],"output":["text"],"parameters":["tools","temperature","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"helicone","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"20.00","output":"80.00"},"eur":{"currency":"eur","input":"17.16","output":"68.66"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"20.00","output":"80.00"},"eur":{"currency":"eur","input":"17.16","output":"68.66"}}},{"providerId":"openai","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"20.00","output":"80.00"},"eur":{"currency":"eur","input":"17.16","output":"68.66"}}},{"providerId":"poe","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"18.00","output":"72.00"},"eur":{"currency":"eur","input":"15.45","output":"61.79"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"20","output":"80"},"eur":{"currency":"eur","input":"17.16","output":"68.66"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.768Z","outputLimit":16384,"contextLength":128000},{"id":"o4-mini","aliases":["openai/o4-mini-2025-04-16","o4-mini-2025-04-16","openai/o4-mini","o4-mini"],"description":{"en":"OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains. Despite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.","de":"OpenAI o4-mini ist ein kompaktes Reasoning-Modell der o-Serie, das für schnelle, kosteneffiziente Leistung optimiert ist und gleichzeitig starke multimodale und agentenbasierte Fähigkeiten aufweist. Es unterstützt den Einsatz von Werkzeugen und zeigt eine konkurrenzfähige Argumentations- und Codierungsleistung bei Benchmarks wie AIME (99,5% mit Python) und SWE-Bench, wobei es seinen Vorgänger o3-mini übertrifft und in einigen Bereichen sogar an o3 heranreicht. Trotz seiner geringeren Größe zeigt o4-mini eine hohe Genauigkeit bei MINT-Aufgaben, visuellem Problemlösen (z. B. MathVista, MMMU) und Codebearbeitung. Es eignet sich besonders gut für Szenarien mit hohem Durchsatz, bei denen Latenz oder Kosten kritisch sind. Dank seiner effizienten Architektur und seines ausgefeilten Reinforcement-Learning-Trainings kann o4-mini Werkzeuge verketten, strukturierte Ausgaben erzeugen und mehrstufige Aufgaben mit minimaler Verzögerung lösen - oft in weniger als einer Minute."},"name":"o4-mini (Preview)","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text","image","file"],"output":["text"],"parameters":["tools","temperature","include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice"],"providers":[{"providerId":"github-copilot","contextLength":128000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"github-models","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"openai","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"openrouter","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"requesty","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"aihubmix","contextLength":200000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.50","output":"6.00"},"eur":{"currency":"eur","input":"1.29","output":"5.15"}}},{"providerId":"azure-cognitive-services","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.10","output":"4.40"},"eur":{"currency":"eur","input":"0.94","output":"3.78"}}},{"providerId":"poe","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"0.99","output":"4.00"},"eur":{"currency":"eur","input":"0.85","output":"3.43"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.331Z","outputLimit":16384,"contextLength":128000},{"id":"o4-mini-deep-research","aliases":["openai/o4-mini-deep-research-2025-06-26","o4-mini-deep-research-2025-06-26","openai/o4-mini-deep-research","o4-mini-deep-research"],"description":{"en":"o4-mini-deep-research is OpenAI's faster, more affordable deep research model—ideal for tackling complex, multi-step research tasks. Note: This model always uses the 'web_search' tool which adds additional cost.","de":"o4-mini-deep-research ist das schnellere und kostengünstigere Deep-Research-Modell von OpenAI - ideal für die Bewältigung komplexer, mehrstufiger Forschungsaufgaben. Hinweis: Dieses Modell verwendet immer das Tool \"web_search\", das zusätzliche Kosten verursacht."},"name":"o4-mini-deep-research","reasoning":true,"toolCalling":true,"knowledge":"2024-05-01","input":["text","image","file"],"output":["text"],"parameters":["tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","presence_penalty","reasoning","response_format","seed","stop","structured_outputs","temperature","tool_choice","top_logprobs","top_p"],"providers":[{"providerId":"openai","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.74","output":"6.94"}}},{"providerId":"poe","contextLength":200000,"outputLimit":100000,"price":{"usd":{"currency":"usd","input":"1.80","output":"7.20"},"eur":{"currency":"eur","input":"1.56","output":"6.25"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"2","output":"8"},"eur":{"currency":"eur","input":"1.74","output":"6.94"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-21T07:05:34.352Z","outputLimit":100000,"contextLength":200000},{"id":"o4-mini-high","aliases":["openai/o4-mini-high-2025-04-16","o4-mini-high-2025-04-16","openai/o4-mini-high","o4-mini-high"],"name":"OpenAI: o4 Mini High","description":{"en":"OpenAI o4-mini-high is the same model as [o4-mini](/openai/o4-mini) with reasoning_effort set to high. OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains. Despite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.","de":"OpenAI o4-mini-high ist das gleiche Modell wie [o4-mini](/openai/o4-mini), wobei reasoning_effort auf high gesetzt ist. OpenAI o4-mini ist ein kompaktes Reasoning-Modell der o-Serie, optimiert für schnelle, kosteneffiziente Leistung unter Beibehaltung starker multimodaler und agenturischer Fähigkeiten. Es unterstützt den Einsatz von Werkzeugen und zeigt eine konkurrenzfähige Argumentations- und Codierungsleistung in Benchmarks wie AIME (99,5% mit Python) und SWE-bench, wobei es seinen Vorgänger o3-mini übertrifft und in einigen Bereichen sogar an o3 heranreicht. Trotz seiner geringeren Größe zeigt o4-mini eine hohe Genauigkeit bei MINT-Aufgaben, visuellem Problemlösen (z. B. MathVista, MMMU) und Codebearbeitung. Es eignet sich besonders gut für Szenarien mit hohem Durchsatz, bei denen Latenz oder Kosten kritisch sind. Dank seiner effizienten Architektur und seines ausgefeilten Reinforcement-Learning-Trainings kann o4-mini Werkzeuge verketten, strukturierte Ausgaben erzeugen und mehrstufige Aufgaben mit minimaler Verzögerung lösen - oft in weniger als einer Minute."},"knowledge":"2025-04-16","reasoning":true,"toolCalling":true,"input":["image","text","file"],"output":["text"],"parameters":["include_reasoning","max_tokens","reasoning","response_format","seed","structured_outputs","tool_choice","tools"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"1.1","output":"4.4"},"eur":{"currency":"eur","input":"0.94898254","output":"3.79593016"}}}],"lastImportedAt":"2025-11-19T12:06:32.765Z"},{"id":"olmo-2-0325-32b-instruct","aliases":["allenai/olmo-2-0325-32b-instruct","olmo-2-0325-32b-instruct"],"name":"AllenAI: Olmo 2 32B Instruct","description":{"en":"OLMo-2 32B Instruct is a supervised instruction-finetuned variant of the OLMo-2 32B March 2025 base model. It excels in complex reasoning and instruction-following tasks across diverse benchmarks such as GSM8K, MATH, IFEval, and general NLP evaluation. Developed by AI2, OLMo-2 32B is part of an open, research-oriented initiative, trained primarily on English-language datasets to advance the understanding and development of open-source language models.","de":"OLMo-2 32B Instruct ist eine auf überwachte Anweisungen abgestimmte Variante des Basismodells OLMo-2 32B March 2025. Es zeichnet sich durch komplexe Schlussfolgerungen und Aufgaben zur Befolgung von Anweisungen in verschiedenen Benchmarks wie GSM8K, MATH, IFEval und allgemeiner NLP-Bewertung aus. Das von AI2 entwickelte OLMo-2 32B ist Teil einer offenen, forschungsorientierten Initiative, die hauptsächlich auf englischsprachigen Datensätzen trainiert, um das Verständnis und die Entwicklung von Open-Source-Sprachmodellen zu fördern."},"knowledge":"2025-03-14","input":["text"],"output":["text"],"parameters":[],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.2"},"eur":{"currency":"eur","input":"0.04","output":"0.17"}}}],"lastImportedAt":"2025-11-28T12:08:40.538Z","contextLength":128000},{"id":"olmo-3-32b-think","aliases":["allenai/olmo-3-32b-think-20251121","allenai/olmo-3-32b-think:free","olmo-3-32b-think-20251121","allenai/olmo-3-32b-think","olmo-3-32b-think:free","olmo-3-32b-think","olmo-3-32b-think"],"name":"AllenAI: Olmo 3 32B Think (free)","description":{"en":"OLMo 3 32B Think is a large-scale, 32-billion-parameter model purpose-built for deep reasoning, complex logic chains and advanced instruction-following scenarios. Its capacity enables strong performance on demanding evaluation tasks and highly nuanced conversational reasoning. Developed by AI2 under the Apache 2.0 license, OLMo 3 32B Think embodies the OLMo initiative’s commitment to openness, offering full transparency across weights, code and training methodology.","de":"OLMo 3 32B Think ist ein groß angelegtes Modell mit 32 Milliarden Parametern, das speziell für tiefgreifende Schlussfolgerungen, komplexe Logikketten und fortgeschrittene Szenarien zur Befolgung von Anweisungen entwickelt wurde. Seine Kapazität ermöglicht eine starke Leistung bei anspruchsvollen Evaluierungsaufgaben und hochgradig nuanciertem Conversational Reasoning. OLMo 3 32B Think wurde von AI2 unter der Apache 2.0-Lizenz entwickelt und verkörpert das Engagement der OLMo-Initiative für Offenheit und bietet vollständige Transparenz in Bezug auf Gewichtung, Code und Trainingsmethoden."},"knowledge":"2025-11-21","reasoning":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":65536,"price":{"usd":{"currency":"usd","input":"0","output":"0"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-03T00:20:35.058Z","contextLength":65536},{"id":"olmo-3-7b-instruct","aliases":["allenai/olmo-3-7b-instruct-20251121","olmo-3-7b-instruct-20251121","allenai/olmo-3-7b-instruct","olmo-3-7b-instruct"],"name":"AllenAI: Olmo 3 7B Instruct","description":{"en":"OLMo 3 7B Instruct is a supervised instruction-fine-tuned variant of the OLMo 3 7B base model, optimized for instruction-following, question-answering, and natural conversational dialogue. By leveraging high-quality instruction data and an open training pipeline, it delivers strong performance across everyday NLP tasks while remaining accessible and easy to integrate. Developed by AI2 under the Apache 2.0 license, the model offers a transparent, community-friendly option for instruction-driven applications.","de":"OLMo 3 7B Instruct ist eine überwachte, auf Anweisungen abgestimmte Variante des OLMo 3 7B-Basismodells, die für das Befolgen von Anweisungen, das Beantworten von Fragen und natürliche Dialoge optimiert ist. Durch die Nutzung hochwertiger Instruktionsdaten und einer offenen Trainingspipeline liefert es eine starke Leistung bei alltäglichen NLP-Aufgaben und bleibt dabei zugänglich und einfach zu integrieren. Das von AI2 unter der Apache 2.0 Lizenz entwickelte Modell bietet eine transparente, Community-freundliche Option für anweisungsgesteuerte Anwendungen."},"knowledge":"2025-11-21","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":65536,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.2"},"eur":{"currency":"eur","input":"0.09","output":"0.17"}}}],"lastImportedAt":"2025-11-22T12:07:08.845Z","contextLength":65536},{"id":"olmo-3-7b-think","aliases":["allenai/olmo-3-7b-think-20251121","olmo-3-7b-think-20251121","allenai/olmo-3-7b-think","olmo-3-7b-think"],"name":"AllenAI: Olmo 3 7B Think","description":{"en":"OLMo 3 7B Think is a research-oriented language model in the OLMo family designed for advanced reasoning and instruction-driven tasks. It excels at multi-step problem solving, logical inference, and maintaining coherent conversational context. Developed by AI2 under the Apache 2.0 license, OLMo 3 7B Think supports transparent, fully open experimentation and provides a lightweight yet capable foundation for academic research and practical NLP workflows.","de":"OLMo 3 7B Think ist ein forschungsorientiertes Sprachmodell der OLMo-Familie, das für fortgeschrittenes logisches Denken und anweisungsgesteuerte Aufgaben entwickelt wurde. Es zeichnet sich durch mehrstufige Problemlösungen, logische Schlussfolgerungen und die Aufrechterhaltung eines kohärenten Gesprächskontextes aus. Entwickelt von AI2 unter der Apache 2.0 Lizenz, unterstützt OLMo 3 7B Think transparente, vollständig offene Experimente und bietet eine leichtgewichtige und dennoch leistungsfähige Grundlage für akademische Forschung und praktische NLP-Workflows."},"knowledge":"2025-11-21","reasoning":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":65536,"price":{"usd":{"currency":"usd","input":"0.12","output":"0.2"},"eur":{"currency":"eur","input":"0.1","output":"0.17"}}}],"lastImportedAt":"2025-11-22T12:07:08.845Z","contextLength":65536},{"id":"open-mistral-7b","aliases":["open-mistral-7b"],"name":"Mistral 7B","toolCalling":true,"openWeights":true,"knowledge":"2023-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"mistral","contextLength":8000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"0.25","output":"0.25"},"eur":{"currency":"eur","input":"0.21567785","output":"0.21567785"}}}],"lastImportedAt":"2025-11-19T12:06:32.703Z"},{"id":"open-mixtral-8x22b","aliases":["open-mixtral-8x22b"],"name":"Mixtral 8x22B","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"mistral","contextLength":64000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"2.00","output":"6.00"},"eur":{"currency":"eur","input":"1.7254228","output":"5.1762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.703Z"},{"id":"open-mixtral-8x7b","aliases":["open-mixtral-8x7b"],"name":"Mixtral 8x7B","toolCalling":true,"openWeights":true,"knowledge":"2024-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"mistral","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.70","output":"0.70"},"eur":{"currency":"eur","input":"0.60389798","output":"0.60389798"}}}],"lastImportedAt":"2025-11-19T12:06:32.703Z"},{"id":"openai-gpt-52","aliases":["openai-gpt-52"],"name":"GPT-5.2","reasoning":true,"toolCalling":true,"knowledge":"2025-08-31","input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"venice","contextLength":200000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"2.19","output":"17.50"},"eur":{"currency":"eur","input":"1.87","output":"14.97"}}}],"lastImportedAt":"2025-12-12T12:08:56.213Z","outputLimit":128000,"contextLength":200000},{"id":"openai-gpt-oss-120b","aliases":["openai-gpt-oss-120b"],"name":"OpenAI GPT OSS 120B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"venice","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}},{"providerId":"siliconflow","contextLength":131000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.45"},"eur":{"currency":"eur","input":"0.04","output":"0.39"}}}],"lastImportedAt":"2025-12-10T00:21:34.138Z","outputLimit":8000,"contextLength":131000},{"id":"openai-gpt-oss-20b","aliases":["openai-gpt-oss-20b"],"name":"openai/gpt-oss-20b","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.18"},"eur":{"currency":"eur","input":"0.03","output":"0.16"}}}],"lastImportedAt":"2025-11-30T00:23:20.896Z","outputLimit":8000,"contextLength":131000},{"id":"openchat-3.5-0106","aliases":["openchat-3.5-0106"],"name":"@cf/openchat/openchat-3.5-0106","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":8192,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.694Z","outputLimit":8192,"contextLength":8192},{"id":"openhermes-2.5-mistral-7b-awq","aliases":["openhermes-2.5-mistral-7b-awq"],"name":"@hf/thebloke/openhermes-2.5-mistral-7b-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.709Z","outputLimit":4096,"contextLength":4096},{"id":"osmosis-structure-0.6b","aliases":["osmosis/osmosis-structure-0.6b","osmosis-structure-0.6b"],"name":"Osmosis Structure 0.6B","toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"inference","contextLength":4000,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.50"},"eur":{"currency":"eur","input":"0.08627114","output":"0.4313557"}}}],"lastImportedAt":"2025-11-19T12:06:32.754Z"},{"id":"oswe-vscode-prime","aliases":["oswe-vscode-prime"],"name":"Raptor Mini (Preview)","reasoning":true,"toolCalling":true,"knowledge":"2024-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-copilot","contextLength":200000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.696Z"},{"id":"parakeet-tdt-0.6b-v2","aliases":["nvidia/parakeet-tdt-0.6b-v2","parakeet-tdt-0.6b-v2"],"description":{"en":"Parakeet TDT 0.6B V2 is a 600M parameter ASR model designed for English transcription, providing punctuation, capitalization, and timestamp predictions. Suitable for applications in speech-to-text services.","de":"Parakeet TDT 0.6B V2 ist ein ASR-Modell mit 600 Mio. Parametern, das für die englische Transkription entwickelt wurde und Vorhersagen zu Interpunktion, Großschreibung und Zeitstempeln liefert. Geeignet für Anwendungen in Sprache-zu-Text-Diensten."},"name":"Parakeet TDT 0.6B v2","knowledge":"2024-01-01","input":["audio"],"output":["text"],"parameters":[],"providers":[{"providerId":"nvidia","outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.693Z"},{"id":"phi-2","aliases":["phi-2"],"name":"@cf/microsoft/phi-2","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":2048,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.482Z","outputLimit":2048,"contextLength":2048},{"id":"phi-3-medium-128k-instruct","aliases":["microsoft/phi-3-medium-128k-instruct","phi-3-medium-128k-instruct"],"description":{"en":"Phi-3 128K Medium is a powerful 14-billion parameter model designed for advanced language understanding, reasoning, and instruction following. Optimized through supervised fine-tuning and preference adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing. At time of release, Phi-3 Medium demonstrated state-of-the-art performance among lightweight models. In the MMLU-Pro eval, the model even comes close to a Llama3 70B level of performance. For 4k context length, try [Phi-3 Medium 4K](/models/microsoft/phi-3-medium-4k-instruct).","de":"Phi-3 128K Medium ist ein leistungsstarkes Modell mit 14 Milliarden Parametern, das für fortgeschrittenes Sprachverständnis, logisches Denken und das Befolgen von Anweisungen entwickelt wurde. Optimiert durch überwachte Feinabstimmung und Präferenzanpassungen, zeichnet es sich bei Aufgaben aus, die den gesunden Menschenverstand, Mathematik, logisches Denken und Codeverarbeitung beinhalten. Zum Zeitpunkt der Veröffentlichung zeigte Phi-3 Medium die beste Leistung unter den leichtgewichtigen Modellen. Im MMLU-Pro-Test erreicht das Modell sogar annähernd das Leistungsniveau des Llama3 70B. Für 4k Kontextlänge, versuchen Sie [Phi-3 Medium 4K](/models/microsoft/phi-3-medium-4k-instruct)."},"name":"Phi-3-medium instruct (128k)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","max_tokens","tool_choice","top_p"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.17","output":"0.68"},"eur":{"currency":"eur","input":"0.15","output":"0.59"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.17","output":"0.68"},"eur":{"currency":"eur","input":"0.15","output":"0.59"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"1","output":"1"},"eur":{"currency":"eur","input":"0.86","output":"0.86"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-30T12:07:55.924Z","outputLimit":4096,"contextLength":128000},{"id":"phi-3-medium-4k-instruct","aliases":["microsoft/phi-3-medium-4k-instruct","phi-3-medium-4k-instruct"],"description":{"en":"Phi-3-Medium-4K-Instruct is a 14B parameter multilingual text generation model optimized for instruction following, reasoning, and code generation. It supports 4K token context length and is suitable for low-memory environments. Intended for research and commercial applications, it requires careful evaluation for accuracy and safety in high-risk scenarios.","de":"Phi-3-Medium-4K-Instruct ist ein mehrsprachiges Textgenerierungsmodell mit 14B-Parametern, das für Befehlsverfolgung, Schlussfolgerungen und Codegenerierung optimiert ist. Es unterstützt eine Token-Kontextlänge von 4K und ist für Umgebungen mit geringem Speicherplatz geeignet. Es ist für Forschung und kommerzielle Anwendungen gedacht und muss sorgfältig auf Genauigkeit und Sicherheit in Hochrisikoszenarien geprüft werden."},"name":"Phi-3-medium instruct (4k)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":4096,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":4096,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.17","output":"0.68"},"eur":{"currency":"eur","input":"0.15","output":"0.59"}}},{"providerId":"azure-cognitive-services","contextLength":4096,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.17","output":"0.68"},"eur":{"currency":"eur","input":"0.15","output":"0.59"}}}],"lastImportedAt":"2025-11-30T12:07:55.927Z","outputLimit":1024,"contextLength":4096},{"id":"phi-3-mini-128k-instruct","aliases":["microsoft/phi-3-mini-128k-instruct","phi-3-mini-128k-instruct"],"description":{"en":"Phi-3 Mini is a powerful 3.8B parameter model designed for advanced language understanding, reasoning, and instruction following. Optimized through supervised fine-tuning and preference adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing. At time of release, Phi-3 Medium demonstrated state-of-the-art performance among lightweight models. This model is static, trained on an offline dataset with an October 2023 cutoff date.","de":"Phi-3 Mini ist ein leistungsstarkes Modell mit 3,8 B Parametern, das für fortgeschrittenes Sprachverständnis, logisches Denken und das Befolgen von Anweisungen entwickelt wurde. Optimiert durch überwachte Feinabstimmung und Präferenzanpassungen, zeichnet es sich bei Aufgaben aus, die den gesunden Menschenverstand, Mathematik, logisches Denken und Codeverarbeitung beinhalten. Zum Zeitpunkt der Veröffentlichung zeigte Phi-3 Medium die beste Leistung unter den leichtgewichtigen Modellen. Dieses Modell ist statisch und wurde auf einem Offline-Datensatz mit einem Stichtag im Oktober 2023 trainiert."},"name":"Phi-3-mini instruct (128k)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","max_tokens","tool_choice","top_p"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.52"},"eur":{"currency":"eur","input":"0.11","output":"0.45"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.52"},"eur":{"currency":"eur","input":"0.11","output":"0.45"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.1"},"eur":{"currency":"eur","input":"0.09","output":"0.09"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-30T12:07:55.927Z","outputLimit":4096,"contextLength":128000},{"id":"phi-3-mini-4k-instruct","aliases":["microsoft/phi-3-mini-4k-instruct","phi-3-mini-4k-instruct"],"description":{"en":"Phi-3 Mini-4K-Instruct is a 3.8B parameter text generation model designed for English and French, optimized for instruction following in low-memory and low-latency environments. It excels in reasoning tasks, particularly in math and logic. Suitable for commercial and research applications, it emphasizes safety and alignment with human preferences.","de":"Phi-3 Mini-4K-Instruct ist ein 3,8B-Parameter-Textgenerierungsmodell für Englisch und Französisch, das für das Verfolgen von Anweisungen in Umgebungen mit geringem Speicherbedarf und niedriger Latenzzeit optimiert ist. Es eignet sich hervorragend für schlussfolgernde Aufgaben, insbesondere in Mathematik und Logik. Es eignet sich für kommerzielle und Forschungsanwendungen und legt Wert auf Sicherheit und Anpassung an menschliche Präferenzen."},"name":"Phi-3-mini instruct (4k)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":4096,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":4096,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.52"},"eur":{"currency":"eur","input":"0.11","output":"0.45"}}},{"providerId":"azure-cognitive-services","contextLength":4096,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.52"},"eur":{"currency":"eur","input":"0.11","output":"0.45"}}}],"lastImportedAt":"2025-11-30T12:07:55.925Z","outputLimit":1024,"contextLength":4096},{"id":"phi-3-small-128k-instruct","aliases":["microsoft/phi-3-small-128k-instruct","phi-3-small-128k-instruct"],"description":{"en":"Phi-3-Small-128K-Instruct is a 7B parameter multilingual text generation model, optimized for reasoning tasks including code and logic. It supports a context length of 128K tokens, ideal for memory-constrained and latency-sensitive applications. Designed for commercial and research use, it requires careful evaluation for accuracy and safety in high-risk scenarios.","de":"Phi-3-Small-128K-Instruct ist ein mehrsprachiges Textgenerierungsmodell mit 7B-Parametern, das für schlussfolgernde Aufgaben einschließlich Code und Logik optimiert ist. Es unterstützt eine Kontextlänge von 128K Token, ideal für speicherbeschränkte und latenzempfindliche Anwendungen. Es wurde für den kommerziellen und wissenschaftlichen Einsatz entwickelt und erfordert eine sorgfältige Bewertung der Genauigkeit und Sicherheit in Hochrisikoszenarien."},"name":"Phi-3-small instruct (128k)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.52"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.52"}}}],"lastImportedAt":"2025-11-30T12:07:55.925Z","outputLimit":4096,"contextLength":128000},{"id":"phi-3-small-8k-instruct","aliases":["microsoft/phi-3-small-8k-instruct","phi-3-small-8k-instruct"],"description":{"en":"Phi-3-Small-8K-Instruct is a 7B parameter multilingual text generation model optimized for instruction-following and reasoning tasks, particularly in code and mathematics. It supports 8K token context length, making it suitable for latency-sensitive applications and research. Caution is advised for high-risk scenarios due to potential biases and inaccuracies.","de":"Phi-3-Small-8K-Instruct ist ein mehrsprachiges Texterzeugungsmodell mit 7B-Parametern, das für das Verfolgen von Anweisungen und logische Schlussfolgerungen optimiert ist, insbesondere in den Bereichen Code und Mathematik. Es unterstützt 8K-Token-Kontextlängen und ist damit für latenzempfindliche Anwendungen und Forschung geeignet. Vorsicht ist geboten bei Szenarien mit hohem Risiko aufgrund möglicher Verzerrungen und Ungenauigkeiten."},"name":"Phi-3-small instruct (8k)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.52"}}},{"providerId":"azure-cognitive-services","contextLength":8192,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.13","output":"0.52"}}}],"lastImportedAt":"2025-11-30T12:07:55.926Z","outputLimit":2048,"contextLength":8192},{"id":"phi-3.5-mini-128k-instruct","aliases":["microsoft/phi-3.5-mini-128k-instruct","microsoft/phi-3.5-mini-instruct","phi-3.5-mini-128k-instruct","phi-3.5-mini-instruct"],"name":"Microsoft: Phi-3.5 Mini 128K Instruct","description":{"en":"Phi-3.5 models are lightweight, state-of-the-art open models. These models were trained with Phi-3 datasets that include both synthetic data and the filtered, publicly available websites data, with a focus on high quality and reasoning-dense properties. Phi-3.5 Mini uses 3.8B parameters, and is a dense decoder-only transformer model using the same tokenizer as [Phi-3 Mini](/models/microsoft/phi-3-mini-128k-instruct). The models underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. When assessed against benchmarks that test common sense, language understanding, math, code, long context and logical reasoning, Phi-3.5 models showcased robust and state-of-the-art performance among models with less than 13 billion parameters.","de":"Phi-3.5-Modelle sind leichtgewichtige, offene Modelle auf dem neuesten Stand der Technik. Diese Modelle wurden mit Phi-3-Datensätzen trainiert, die sowohl synthetische Daten als auch gefilterte, öffentlich verfügbare Webseitendaten enthalten, wobei der Schwerpunkt auf hoher Qualität und schlussfolgernden Eigenschaften lag. Phi-3.5 Mini verwendet 3,8B Parameter und ist ein dichtes Decoder-Only-Transformer-Modell, das denselben Tokenizer wie [Phi-3 Mini](/models/microsoft/phi-3-mini-128k-instruct) verwendet. Die Modelle wurden einem strengen Verbesserungsprozess unterzogen, der sowohl eine überwachte Feinabstimmung, eine proximale Richtlinienoptimierung als auch eine direkte Präferenzoptimierung umfasst, um eine präzise Befolgung der Anweisungen und robuste Sicherheitsmaßnahmen zu gewährleisten. Bei der Bewertung anhand von Benchmarks, die den gesunden Menschenverstand, Sprachverständnis, Mathematik, Code, langen Kontext und logisches Denken testen, zeigten die Phi-3.5-Modelle eine robuste und hochmoderne Leistung bei Modellen mit weniger als 13 Milliarden Parametern."},"knowledge":"2024-08-21","toolCalling":true,"input":["text"],"output":["text"],"parameters":["max_tokens","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.1"},"eur":{"currency":"eur","input":"0.08627114","output":"0.08627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"phi-3.5-mini-instruct","aliases":["microsoft/phi-3.5-mini-instruct","phi-3.5-mini-instruct"],"description":{"en":"Phi-3.5-mini is a multilingual text generation model with 3.8B parameters, designed for memory-constrained and latency-sensitive environments. It supports 128K token context length, enabling tasks like long document summarization and QA. Suitable for commercial and research applications, it emphasizes strong reasoning in code and logic while cautioning against potential biases and inaccuracies.","de":"Phi-3.5-mini ist ein mehrsprachiges Textgenerierungsmodell mit 3.8B Parametern, das für speicherbegrenzte und latenzempfindliche Umgebungen entwickelt wurde. Es unterstützt 128K Token-Kontextlänge und ermöglicht so Aufgaben wie die Zusammenfassung langer Dokumente und Qualitätssicherung. Es eignet sich sowohl für kommerzielle als auch für Forschungsanwendungen und zeichnet sich durch eine starke Argumentation in Code und Logik aus, während es gleichzeitig vor möglichen Verzerrungen und Ungenauigkeiten warnt."},"name":"Phi-3.5-mini instruct (128k)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.52"},"eur":{"currency":"eur","input":"0.11","output":"0.45"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.52"},"eur":{"currency":"eur","input":"0.11","output":"0.45"}}}],"lastImportedAt":"2025-11-30T12:07:55.926Z","outputLimit":4096,"contextLength":128000},{"id":"phi-3.5-moe-instruct","aliases":["microsoft/phi-3.5-moe-instruct","phi-3.5-moe-instruct"],"description":{"en":"Phi-3.5-MoE is a multilingual text generation model with 6.6B active parameters and a 128K token context length. It excels in reasoning tasks, including code and math, making it suitable for research and commercial applications. It has undergone extensive safety evaluations and fine-tuning but may produce factual inaccuracies.","de":"Phi-3.5-MoE ist ein mehrsprachiges Textgenerierungsmodell mit 6.6B aktiven Parametern und einer 128K Token-Kontextlänge. Es eignet sich hervorragend für schlussfolgernde Aufgaben, einschließlich Code und Mathematik, und ist damit für Forschung und kommerzielle Anwendungen geeignet. Es wurde umfangreichen Sicherheitsevaluierungen und Feinabstimmungen unterzogen, kann jedoch faktische Ungenauigkeiten aufweisen."},"name":"Phi-3.5-MoE instruct (128k)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.16","output":"0.64"},"eur":{"currency":"eur","input":"0.14","output":"0.55"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.16","output":"0.64"},"eur":{"currency":"eur","input":"0.14","output":"0.55"}}}],"lastImportedAt":"2025-11-30T12:07:55.927Z","outputLimit":4096,"contextLength":128000},{"id":"phi-3.5-vision-instruct","aliases":["microsoft/phi-3.5-vision-instruct","phi-3.5-vision-instruct"],"description":{"en":"Phi-3.5-vision is a multimodal model designed for image and text tasks, featuring 4.2B parameters and a 128K token context length. It supports applications like image understanding, optical character recognition, and multi-image summarization. The model is intended for research and commercial use, focusing on English language tasks.","de":"Phi-3.5-vision ist ein multimodales Modell für Bild- und Textaufgaben mit 4,2B Parametern und einer Kontextlänge von 128K Token. Es unterstützt Anwendungen wie Bildverständnis, optische Zeichenerkennung und die Zusammenfassung mehrerer Bilder. Das Modell ist für die Forschung und den kommerziellen Einsatz bestimmt und konzentriert sich auf englischsprachige Aufgaben."},"name":"Phi-3.5-vision instruct (128k)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.723Z"},{"id":"phi-4","aliases":["microsoft/phi-4","phi-4"],"description":{"en":"[Microsoft Research](/microsoft) Phi-4 is designed to perform well in complex reasoning tasks and can operate efficiently in situations with limited memory or where quick responses are needed. At 14 billion parameters, it was trained on a mix of high-quality synthetic datasets, data from curated websites, and academic materials. It has undergone careful improvement to follow instructions accurately and maintain strong safety standards. It works best with English language inputs. For more information, please see [Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)","de":"[Microsoft Research](/microsoft) Phi-4 ist so konzipiert, dass er bei komplexen Denkaufgaben gut abschneidet und in Situationen mit begrenztem Speicherplatz oder wenn schnelle Antworten erforderlich sind, effizient arbeitet. Mit 14 Milliarden Parametern wurde es auf einer Mischung aus hochwertigen synthetischen Datensätzen, Daten von kuratierten Websites und akademischen Materialien trainiert. Es wurde sorgfältig verbessert, um Anweisungen genau zu befolgen und strenge Sicherheitsstandards einzuhalten. Er funktioniert am besten mit englischsprachigen Eingaben. Weitere Informationen finden Sie im [Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)"},"name":"Phi-4","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"github-models","contextLength":16000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.50"},"eur":{"currency":"eur","input":"0.11","output":"0.43"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.50"},"eur":{"currency":"eur","input":"0.11","output":"0.43"}}},{"providerId":"openrouter","contextLength":16384,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.14"},"eur":{"currency":"eur","input":"0.05","output":"0.12"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-30T12:07:55.925Z","outputLimit":4096,"contextLength":16000},{"id":"phi-4-mini","aliases":["phi-4-mini"],"name":"Phi-4-mini","toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}}],"lastImportedAt":"2025-11-30T12:07:57.718Z","outputLimit":4096,"contextLength":128000},{"id":"phi-4-mini-instruct","aliases":["microsoft/phi-4-mini-instruct","phi-4-mini-instruct"],"description":{"en":"Phi-4-mini-instruct is a multilingual text-generation model with 3.8B parameters, designed for memory-constrained environments and strong reasoning tasks. It supports 128K token context length and is suitable for various applications in AI systems. Users should consider limitations in accuracy and safety, particularly in high-risk scenarios.","de":"Phi-4-mini-instruct ist ein mehrsprachiges Texterzeugungsmodell mit 3,8B Parametern, das für speicherbeschränkte Umgebungen und starke Argumentationsaufgaben entwickelt wurde. Es unterstützt 128K Token-Kontextlänge und ist für verschiedene Anwendungen in KI-Systemen geeignet. Die Benutzer sollten die Einschränkungen in Bezug auf Genauigkeit und Sicherheit berücksichtigen, insbesondere in Hochrisikoszenarien."},"name":"Phi-4-Mini","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text","image","audio"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"nvidia","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"wandb","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.08","output":"0.35"},"eur":{"currency":"eur","input":"0.069016912","output":"0.30194899"}}}],"lastImportedAt":"2025-11-19T12:06:32.693Z"},{"id":"phi-4-mini-reasoning","aliases":["microsoft/phi-4-mini-reasoning","phi-4-mini-reasoning"],"description":{"en":"Phi-4-mini-reasoning is a compact transformer model with 3.8B parameters, optimized for multi-step mathematical reasoning tasks. It supports 128K token context length and is suited for formal proof generation, symbolic computation, and advanced math problems. The model is designed for environments with memory constraints and provides reliable outputs in analytical scenarios.","de":"Phi-4-mini-reasoning ist ein kompaktes Transformatormodell mit 3,8B Parametern, das für mehrstufige mathematische Argumentationsaufgaben optimiert ist. Es unterstützt 128K Token-Kontextlänge und eignet sich für die Generierung formaler Beweise, symbolische Berechnungen und fortgeschrittene mathematische Probleme. Das Modell ist für Umgebungen mit Speicherbeschränkungen konzipiert und liefert zuverlässige Ergebnisse in analytischen Szenarien."},"name":"Phi-4-mini-reasoning","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.30"},"eur":{"currency":"eur","input":"0.06","output":"0.26"}}}],"lastImportedAt":"2025-11-30T12:07:55.926Z","outputLimit":4096,"contextLength":128000},{"id":"phi-4-multimodal","aliases":["phi-4-multimodal"],"name":"Phi-4-multimodal","openWeights":true,"knowledge":"2023-10-01","input":["text","image","audio"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"azure","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.08","output":"0.32"},"eur":{"currency":"eur","input":"0.07","output":"0.28"}}},{"providerId":"azure-cognitive-services","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.08","output":"0.32"},"eur":{"currency":"eur","input":"0.07","output":"0.28"}}}],"lastImportedAt":"2025-11-30T12:07:57.771Z","outputLimit":4096,"contextLength":128000},{"id":"phi-4-multimodal-instruct","aliases":["microsoft/phi-4-multimodal-instruct","phi-4-multimodal-instruct"],"description":{"en":"Phi-4 Multimodal Instruct is a versatile 5.6B parameter foundation model that combines advanced reasoning and instruction-following capabilities across both text and visual inputs, providing accurate text outputs. The unified architecture enables efficient, low-latency inference, suitable for edge and mobile deployments. Phi-4 Multimodal Instruct supports text inputs in multiple languages including Arabic, Chinese, English, French, German, Japanese, Spanish, and more, with visual input optimized primarily for English. It delivers impressive performance on multimodal tasks involving mathematical, scientific, and document reasoning, providing developers and enterprises a powerful yet compact model for sophisticated interactive applications. For more information, see the [Phi-4 Multimodal blog post](https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/).","de":"Phi-4 Multimodal Instruct ist ein vielseitiges Basismodell mit 5,6 B Parametern, das fortschrittliche Schlussfolgerungen und Funktionen zur Befolgung von Anweisungen sowohl für Text- als auch für visuelle Eingaben kombiniert und genaue Textausgaben liefert. Die einheitliche Architektur ermöglicht eine effiziente Inferenz mit geringer Latenz, die sich für Edge- und mobile Implementierungen eignet. Phi-4 Multimodal Instruct unterstützt Texteingaben in mehreren Sprachen, darunter Arabisch, Chinesisch, Englisch, Französisch, Deutsch, Japanisch und Spanisch, wobei die visuelle Eingabe hauptsächlich für Englisch optimiert ist. Es liefert eine beeindruckende Leistung bei multimodalen Aufgaben, die mathematische, wissenschaftliche und dokumentarische Argumente beinhalten, und bietet Entwicklern und Unternehmen ein leistungsstarkes und dennoch kompaktes Modell für anspruchsvolle interaktive Anwendungen. Weitere Informationen finden Sie im [Phi-4 Multimodal Blog Post] (https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/)."},"name":"Phi-4-multimodal-instruct","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text","image","audio"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","top_k","top_p"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.1"},"eur":{"currency":"eur","input":"0.04313557","output":"0.08627114"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.724Z"},{"id":"phi-4-reasoning","aliases":["microsoft/phi-4-reasoning","phi-4-reasoning"],"description":{"en":"Phi-4-reasoning is a 14B parameter model designed for text generation, particularly in math, science, and coding contexts. It excels in reasoning tasks, including algorithmic problem solving and instruction following. Safety measures are integrated, addressing biases and content reliability. Best suited for English-language applications.","de":"Phi-4-reasoning ist ein 14B-Parameter-Modell, das für die Textgenerierung, insbesondere in mathematischen, wissenschaftlichen und kodierenden Kontexten, entwickelt wurde. Es zeichnet sich durch schlussfolgernde Aufgaben aus, einschließlich algorithmischer Problemlösung und Befolgung von Anweisungen. Es sind Sicherheitsmaßnahmen integriert, die sich mit Verzerrungen und inhaltlicher Zuverlässigkeit befassen. Am besten geeignet für englischsprachige Anwendungen."},"name":"Phi-4-Reasoning","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"github-models","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"azure","contextLength":32000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.50"},"eur":{"currency":"eur","input":"0.11","output":"0.43"}}},{"providerId":"azure-cognitive-services","contextLength":32000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.50"},"eur":{"currency":"eur","input":"0.11","output":"0.43"}}}],"lastImportedAt":"2025-11-30T12:07:55.927Z","outputLimit":4096,"contextLength":32000},{"id":"phi-4-reasoning-plus","aliases":["microsoft/phi-4-reasoning-plus-04-30","microsoft/phi-4-reasoning-plus","phi-4-reasoning-plus-04-30","phi-4-reasoning-plus"],"description":{"en":"Phi-4-reasoning-plus is an enhanced 14B parameter model from Microsoft, fine-tuned from Phi-4 with additional reinforcement learning to boost accuracy on math, science, and code reasoning tasks. It uses the same dense decoder-only transformer architecture as Phi-4, but generates longer, more comprehensive outputs structured into a step-by-step reasoning trace and final answer. While it offers improved benchmark scores over Phi-4-reasoning across tasks like AIME, OmniMath, and HumanEvalPlus, its responses are typically ~50% longer, resulting in higher latency. Designed for English-only applications, it is well-suited for structured reasoning workflows where output quality takes priority over response speed.","de":"Phi-4-reasoning-plus ist ein erweitertes 14B-Parameter-Modell von Microsoft, das auf der Grundlage von Phi-4 mit zusätzlichem Reinforcement Learning abgestimmt wurde, um die Genauigkeit bei mathematischen, wissenschaftlichen und Code-Reasoning-Aufgaben zu erhöhen. Es verwendet die gleiche dichte Decoder-Only-Transformer-Architektur wie Phi-4, erzeugt aber längere, umfassendere Ausgaben, die in eine schrittweise Argumentation und eine endgültige Antwort strukturiert sind. Während er bei Aufgaben wie AIME, OmniMath und HumanEvalPlus bessere Benchmark-Ergebnisse als Phi-4-Reasoning bietet, sind seine Antworten typischerweise ~50 % länger, was zu einer höheren Latenz führt. Es wurde für rein englischsprachige Anwendungen entwickelt und eignet sich gut für strukturierte Argumentationsabläufe, bei denen die Qualität der Ausgabe Vorrang vor der Antwortgeschwindigkeit hat."},"name":"Phi-4-reasoning-plus","reasoning":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","top_k","top_p"],"providers":[{"providerId":"azure","contextLength":32000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.50"},"eur":{"currency":"eur","input":"0.11","output":"0.43"}}},{"providerId":"azure-cognitive-services","contextLength":32000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.50"},"eur":{"currency":"eur","input":"0.11","output":"0.43"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.35"},"eur":{"currency":"eur","input":"0.06","output":"0.3"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-30T12:07:57.824Z","outputLimit":4096,"contextLength":32000},{"id":"phoenix-1.0","aliases":["phoenix-1.0"],"name":"@cf/leonardo/phoenix-1.0","input":["text"],"output":["image"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.01","output":"0.01"},"eur":{"currency":"eur","input":"0.01","output":"0.01"}}}],"lastImportedAt":"2025-12-08T00:21:41.466Z"},{"id":"pixtral-12b","aliases":["mistralai/pixtral-12b-2409","mistralai/pixtral-12b","mistral/pixtral-12b","pixtral-12b-2409","pixtral-12b"],"description":{"en":"The first multi-modal, text+image-to-text model from Mistral AI. Its weights were launched via torrent: https://x.com/mistralai/status/1833758285167722836.","de":"Das erste multimodale, Text+Bild-zu-Text-Modell von Mistral AI. Seine Gewichte wurden über Torrent veröffentlicht: https://x.com/mistralai/status/1833758285167722836."},"name":"Pixtral 12B","toolCalling":true,"openWeights":true,"knowledge":"2024-09-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"mistral","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.12940671","output":"0.12940671"}}},{"providerId":"vercel","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.12940671","output":"0.12940671"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.1"},"eur":{"currency":"eur","input":"0.08627114","output":"0.08627114"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.703Z"},{"id":"pixtral-large","aliases":["mistral/pixtral-large","pixtral-large"],"name":"Pixtral Large","toolCalling":true,"openWeights":true,"knowledge":"2024-11-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"2.00","output":"6.00"},"eur":{"currency":"eur","input":"1.7254228","output":"5.1762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.705Z"},{"id":"plamo-embedding-1b","aliases":["workers-ai/plamo-embedding-1b","plamo-embedding-1b"],"name":"plamo emuedding 1b","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.088Z","outputLimit":16384,"contextLength":128000},{"id":"polaris-alpha","aliases":["openrouter/polaris-alpha","polaris-alpha"],"name":"Polaris Alpha","toolCalling":true,"knowledge":"2025-07-01","input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"openrouter","contextLength":256000,"outputLimit":128000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.751Z","deprecated":true},{"id":"qvq-max","aliases":["qvq-max"],"name":"QVQ Max","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.20","output":"4.80"},"eur":{"currency":"eur","input":"1.03525368","output":"4.14101472"}}}],"lastImportedAt":"2025-11-19T12:06:32.686Z"},{"id":"qwen-2.5-7b-vision-instruct","aliases":["qwen/qwen-2.5-7b-vision-instruct","qwen-2.5-7b-vision-instruct"],"name":"Qwen 2.5 7B Vision Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"inference","contextLength":125000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17254228","output":"0.17254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.754Z"},{"id":"qwen-2.5-coder-32b","aliases":["qwen-2.5-coder-32b"],"name":"Qwen 2.5 Coder 32B","openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"venice","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.4313557","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.717Z","deprecated":true},{"id":"qwen-2.5-qwq-32b","aliases":["qwen-2.5-qwq-32b"],"name":"Venice Reasoning","reasoning":true,"openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"venice","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.4313557","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.717Z","deprecated":true},{"id":"qwen-2.5-vl","aliases":["qwen-2.5-vl"],"name":"Qwen 2.5 VL 72B","openWeights":true,"knowledge":"2023-10-01","input":["text","image"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"venice","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.70","output":"2.80"},"eur":{"currency":"eur","input":"0.60389798","output":"2.41559192"}}}],"lastImportedAt":"2025-11-19T12:06:32.716Z","deprecated":true},{"id":"qwen-3-coder-480b","aliases":["qwen-3-coder-480b"],"name":"Qwen 3 Coder 480B","toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cerebras","contextLength":131000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"2.00","output":"2.00"},"eur":{"currency":"eur","input":"1.7254228","output":"1.7254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.761Z","deprecated":true},{"id":"qwen-deep-research","aliases":["qwen-deep-research"],"name":"Qwen Deep Research","toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":1000000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"7.74","output":"23.37"},"eur":{"currency":"eur","input":"6.677386236","output":"20.161565418"}}}],"lastImportedAt":"2025-11-19T12:06:32.715Z"},{"id":"qwen-doc-turbo","aliases":["qwen-doc-turbo"],"name":"Qwen Doc Turbo","toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.14"},"eur":{"currency":"eur","input":"0.077644026","output":"0.120779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.715Z"},{"id":"qwen-flash","aliases":["qwen-flash"],"name":"Qwen Flash","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":1000000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.40"},"eur":{"currency":"eur","input":"0.04313557","output":"0.34508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.687Z"},{"id":"qwen-long","aliases":["qwen-long"],"name":"Qwen Long","toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":10000000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.29"},"eur":{"currency":"eur","input":"0.060389798","output":"0.250186306"}}}],"lastImportedAt":"2025-11-19T12:06:32.715Z"},{"id":"qwen-math-plus","aliases":["qwen-math-plus"],"name":"Qwen Math Plus","toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":4096,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.57","output":"1.72"},"eur":{"currency":"eur","input":"0.491745498","output":"1.483863608"}}}],"lastImportedAt":"2025-11-19T12:06:32.715Z"},{"id":"qwen-math-turbo","aliases":["qwen-math-turbo"],"name":"Qwen Math Turbo","toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":4096,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.29","output":"0.86"},"eur":{"currency":"eur","input":"0.250186306","output":"0.741931804"}}}],"lastImportedAt":"2025-11-19T12:06:32.715Z"},{"id":"qwen-max","aliases":["qwen/qwen-max-2025-01-25","qwen-max-2025-01-25","qwen/qwen-max","qwen-max"],"description":{"en":"Qwen-Max, based on Qwen2.5, provides the best inference performance among [Qwen models](/qwen), especially for complex multi-step tasks. It's a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. The parameter count is unknown.","de":"Qwen-Max, basierend auf Qwen2.5, bietet die beste Inferenzleistung unter den [Qwen-Modellen](/qwen), insbesondere bei komplexen mehrstufigen Aufgaben. Es handelt sich um ein groß angelegtes MoE-Modell, das mit über 20 Billionen Token vortrainiert und mit kuratierten Supervised Fine-Tuning (SFT)- und Reinforcement Learning from Human Feedback (RLHF)-Methoden weiter nachtrainiert wurde. Die Anzahl der Parameter ist unbekannt."},"name":"Qwen Max","toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","max_tokens","presence_penalty","response_format","seed","tool_choice","top_p"],"providers":[{"providerId":"alibaba","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.60","output":"6.40"},"eur":{"currency":"eur","input":"1.38033824","output":"5.52135296"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"1.6","output":"6.4"},"eur":{"currency":"eur","input":"1.38033824","output":"5.52135296"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.688Z"},{"id":"qwen-mt-plus","aliases":["qwen-mt-plus"],"name":"Qwen-MT Plus","knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"alibaba","contextLength":16384,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.46","output":"7.37"},"eur":{"currency":"eur","input":"2.122270044","output":"6.358183018"}}}],"lastImportedAt":"2025-11-19T12:06:32.689Z"},{"id":"qwen-mt-turbo","aliases":["qwen-mt-turbo"],"name":"Qwen-MT Turbo","knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"alibaba","contextLength":16384,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.16","output":"0.49"},"eur":{"currency":"eur","input":"0.138033824","output":"0.422728586"}}}],"lastImportedAt":"2025-11-19T12:06:32.688Z"},{"id":"qwen-omni-turbo","aliases":["qwen-omni-turbo"],"name":"Qwen-Omni Turbo","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":32768,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.27"},"eur":{"currency":"eur","input":"0.060389798","output":"0.232932078"}}}],"lastImportedAt":"2025-11-19T12:06:32.684Z"},{"id":"qwen-omni-turbo-realtime","aliases":["qwen-omni-turbo-realtime"],"name":"Qwen-Omni Turbo Realtime","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","audio"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":32768,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.27","output":"1.07"},"eur":{"currency":"eur","input":"0.232932078","output":"0.923101198"}}}],"lastImportedAt":"2025-11-19T12:06:32.688Z"},{"id":"qwen-plus","aliases":["qwen/qwen-plus-2025-01-25","qwen-plus-2025-01-25","qwen/qwen-plus","qwen-plus"],"description":{"en":"Qwen-Plus, based on the Qwen2.5 foundation model, is a 131K context model with a balanced performance, speed, and cost combination.","de":"Qwen-Plus, das auf dem Qwen2.5-Grundmodell basiert, ist ein 131K-Kontextmodell mit einer ausgewogenen Kombination aus Leistung, Geschwindigkeit und Kosten."},"name":"Qwen Plus","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","max_tokens","presence_penalty","response_format","seed","tool_choice","top_p"],"providers":[{"providerId":"alibaba","contextLength":1000000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.20"},"eur":{"currency":"eur","input":"0.34508456","output":"1.03525368"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.4","output":"1.2"},"eur":{"currency":"eur","input":"0.34508456","output":"1.03525368"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.688Z"},{"id":"qwen-plus-character","aliases":["qwen-plus-character"],"name":"Qwen Plus Character","toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":32768,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.12","output":"0.29"},"eur":{"currency":"eur","input":"0.103525368","output":"0.250186306"}}}],"lastImportedAt":"2025-11-19T12:06:32.714Z"},{"id":"qwen-plus-character-ja","aliases":["qwen-plus-character-ja"],"name":"Qwen Plus Character (Japanese)","toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":8192,"outputLimit":512,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.40"},"eur":{"currency":"eur","input":"0.4313557","output":"1.20779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.686Z"},{"id":"qwen-qwen2.5-14b-instruct","aliases":["qwen-qwen2.5-14b-instruct"],"name":"Qwen/Qwen2.5-14B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.10"},"eur":{"currency":"eur","input":"0.09","output":"0.09"}}}],"lastImportedAt":"2025-11-26T00:20:51.149Z","outputLimit":4000,"contextLength":33000},{"id":"qwen-qwen2.5-32b-instruct","aliases":["qwen-qwen2.5-32b-instruct"],"name":"Qwen/Qwen2.5-32B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.18","output":"0.18"},"eur":{"currency":"eur","input":"0.16","output":"0.16"}}}],"lastImportedAt":"2025-11-26T00:20:49.167Z","outputLimit":4000,"contextLength":33000},{"id":"qwen-qwen2.5-72b-instruct","aliases":["qwen-qwen2.5-72b-instruct"],"name":"Qwen/Qwen2.5-72B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.59","output":"0.59"},"eur":{"currency":"eur","input":"0.51","output":"0.51"}}}],"lastImportedAt":"2025-11-26T00:20:50.132Z","outputLimit":4000,"contextLength":33000},{"id":"qwen-qwen2.5-72b-instruct-128k","aliases":["qwen-qwen2.5-72b-instruct-128k"],"name":"Qwen/Qwen2.5-72B-Instruct-128K","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.59","output":"0.59"},"eur":{"currency":"eur","input":"0.51","output":"0.51"}}}],"lastImportedAt":"2025-11-26T00:20:45.260Z","outputLimit":4000,"contextLength":131000},{"id":"qwen-qwen2.5-7b-instruct","aliases":["qwen-qwen2.5-7b-instruct"],"name":"Qwen/Qwen2.5-7B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.05"},"eur":{"currency":"eur","input":"0.04","output":"0.04"}}}],"lastImportedAt":"2025-11-26T00:20:49.080Z","outputLimit":4000,"contextLength":33000},{"id":"qwen-qwen2.5-coder-32b-instruct","aliases":["qwen-qwen2.5-coder-32b-instruct"],"name":"Qwen/Qwen2.5-Coder-32B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.18","output":"0.18"},"eur":{"currency":"eur","input":"0.16","output":"0.16"}}}],"lastImportedAt":"2025-11-26T00:20:45.561Z","outputLimit":4000,"contextLength":33000},{"id":"qwen-qwen2.5-vl-32b-instruct","aliases":["qwen-qwen2.5-vl-32b-instruct"],"name":"Qwen/Qwen2.5-VL-32B-Instruct","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.27"},"eur":{"currency":"eur","input":"0.23","output":"0.23"}}}],"lastImportedAt":"2025-11-30T00:23:20.814Z","outputLimit":131000,"contextLength":131000},{"id":"qwen-qwen2.5-vl-72b-instruct","aliases":["qwen-qwen2.5-vl-72b-instruct"],"name":"Qwen/Qwen2.5-VL-72B-Instruct","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.59","output":"0.59"},"eur":{"currency":"eur","input":"0.51","output":"0.51"}}}],"lastImportedAt":"2025-11-26T00:20:47.338Z","outputLimit":4000,"contextLength":131000},{"id":"qwen-qwen2.5-vl-7b-instruct","aliases":["qwen-qwen2.5-vl-7b-instruct"],"name":"Qwen/Qwen2.5-VL-7B-Instruct","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":4000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.05"},"eur":{"currency":"eur","input":"0.04","output":"0.04"}}}],"lastImportedAt":"2025-11-26T00:20:49.574Z","outputLimit":4000,"contextLength":33000},{"id":"qwen-qwen3-14b","aliases":["qwen-qwen3-14b"],"name":"Qwen/Qwen3-14B","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.28"},"eur":{"currency":"eur","input":"0.06","output":"0.24"}}}],"lastImportedAt":"2025-11-26T00:20:46.738Z","outputLimit":131000,"contextLength":131000},{"id":"qwen-qwen3-235b-a22b","aliases":["qwen-qwen3-235b-a22b"],"name":"Qwen/Qwen3-235B-A22B","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.35","output":"1.42"},"eur":{"currency":"eur","input":"0.3","output":"1.23"}}}],"lastImportedAt":"2025-11-26T00:20:50.214Z","outputLimit":131000,"contextLength":131000},{"id":"qwen-qwen3-30b-a3b","aliases":["qwen-qwen3-30b-a3b"],"name":"Qwen/Qwen3-30B-A3B","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.45"},"eur":{"currency":"eur","input":"0.08","output":"0.39"}}}],"lastImportedAt":"2025-11-26T00:20:47.588Z","outputLimit":131000,"contextLength":131000},{"id":"qwen-qwen3-32b","aliases":["qwen-qwen3-32b"],"name":"Qwen/Qwen3-32B","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.57"},"eur":{"currency":"eur","input":"0.12","output":"0.5"}}}],"lastImportedAt":"2025-11-26T00:20:50.551Z","outputLimit":131000,"contextLength":131000},{"id":"qwen-qwen3-8b","aliases":["qwen-qwen3-8b"],"name":"Qwen/Qwen3-8B","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.06"},"eur":{"currency":"eur","input":"0.05","output":"0.05"}}}],"lastImportedAt":"2025-11-26T00:20:49.905Z","outputLimit":131000,"contextLength":131000},{"id":"qwen-qwen3-coder-30b-a3b-instruct","aliases":["qwen-qwen3-coder-30b-a3b-instruct"],"name":"Qwen/Qwen3-Coder-30B-A3B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.28"},"eur":{"currency":"eur","input":"0.06","output":"0.24"}}}],"lastImportedAt":"2025-11-26T00:20:48.319Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-coder-480b-a35b-instruct","aliases":["qwen-qwen3-coder-480b-a35b-instruct"],"name":"Qwen/Qwen3-Coder-480B-A35B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.00"},"eur":{"currency":"eur","input":"0.22","output":"0.87"}}}],"lastImportedAt":"2025-11-26T00:20:46.491Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-next-80b-a3b-instruct","aliases":["qwen-qwen3-next-80b-a3b-instruct"],"name":"Qwen/Qwen3-Next-80B-A3B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.14","output":"1.40"},"eur":{"currency":"eur","input":"0.12","output":"1.21"}}}],"lastImportedAt":"2025-11-30T00:23:20.929Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-next-80b-a3b-thinking","aliases":["qwen-qwen3-next-80b-a3b-thinking"],"name":"Qwen/Qwen3-Next-80B-A3B-Thinking","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.57"},"eur":{"currency":"eur","input":"0.12","output":"0.5"}}}],"lastImportedAt":"2025-11-26T00:20:48.570Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-omni-30b-a3b-captioner","aliases":["qwen-qwen3-omni-30b-a3b-captioner"],"name":"Qwen/Qwen3-Omni-30B-A3B-Captioner","toolCalling":true,"input":["audio"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":66000,"outputLimit":66000,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.35"}}}],"lastImportedAt":"2025-11-27T00:20:23.020Z","outputLimit":66000,"contextLength":66000},{"id":"qwen-qwen3-omni-30b-a3b-instruct","aliases":["qwen-qwen3-omni-30b-a3b-instruct"],"name":"Qwen/Qwen3-Omni-30B-A3B-Instruct","toolCalling":true,"input":["text","image","audio"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":66000,"outputLimit":66000,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.35"}}}],"lastImportedAt":"2025-11-26T00:20:46.572Z","outputLimit":66000,"contextLength":66000},{"id":"qwen-qwen3-omni-30b-a3b-thinking","aliases":["qwen-qwen3-omni-30b-a3b-thinking"],"name":"Qwen/Qwen3-Omni-30B-A3B-Thinking","reasoning":true,"toolCalling":true,"input":["text","image","audio"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":66000,"outputLimit":66000,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.09","output":"0.35"}}}],"lastImportedAt":"2025-11-26T00:20:50.467Z","outputLimit":66000,"contextLength":66000},{"id":"qwen-qwen3-vl-235b-a22b-instruct","aliases":["qwen-qwen3-vl-235b-a22b-instruct"],"name":"Qwen/Qwen3-VL-235B-A22B-Instruct","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.50"},"eur":{"currency":"eur","input":"0.26","output":"1.3"}}}],"lastImportedAt":"2025-11-26T00:20:48.655Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-vl-235b-a22b-thinking","aliases":["qwen-qwen3-vl-235b-a22b-thinking"],"name":"Qwen/Qwen3-VL-235B-A22B-Thinking","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.45","output":"3.50"},"eur":{"currency":"eur","input":"0.39","output":"3.04"}}}],"lastImportedAt":"2025-11-26T00:20:45.729Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-vl-30b-a3b-instruct","aliases":["qwen-qwen3-vl-30b-a3b-instruct"],"name":"Qwen/Qwen3-VL-30B-A3B-Instruct","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.29","output":"1.00"},"eur":{"currency":"eur","input":"0.25","output":"0.87"}}}],"lastImportedAt":"2025-11-26T00:20:50.978Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-vl-30b-a3b-thinking","aliases":["qwen-qwen3-vl-30b-a3b-thinking"],"name":"Qwen/Qwen3-VL-30B-A3B-Thinking","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.29","output":"1.00"},"eur":{"currency":"eur","input":"0.25","output":"0.87"}}}],"lastImportedAt":"2025-11-26T00:20:45.902Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-vl-32b-instruct","aliases":["qwen-qwen3-vl-32b-instruct"],"name":"Qwen/Qwen3-VL-32B-Instruct","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.60"},"eur":{"currency":"eur","input":"0.17","output":"0.52"}}}],"lastImportedAt":"2025-11-26T00:20:46.153Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-vl-32b-thinking","aliases":["qwen-qwen3-vl-32b-thinking"],"name":"Qwen/Qwen3-VL-32B-Thinking","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.50"},"eur":{"currency":"eur","input":"0.17","output":"1.3"}}}],"lastImportedAt":"2025-11-26T00:20:47.421Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-vl-8b-instruct","aliases":["qwen-qwen3-vl-8b-instruct"],"name":"Qwen/Qwen3-VL-8B-Instruct","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.18","output":"0.68"},"eur":{"currency":"eur","input":"0.16","output":"0.59"}}}],"lastImportedAt":"2025-11-26T00:20:49.739Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwen3-vl-8b-thinking","aliases":["qwen-qwen3-vl-8b-thinking"],"name":"Qwen/Qwen3-VL-8B-Thinking","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.18","output":"2.00"},"eur":{"currency":"eur","input":"0.16","output":"1.74"}}}],"lastImportedAt":"2025-11-26T00:20:49.654Z","outputLimit":262000,"contextLength":262000},{"id":"qwen-qwq-32b","aliases":["qwen-qwq-32b"],"name":"Qwen QwQ 32B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-09-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"groq","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.29","output":"0.39"},"eur":{"currency":"eur","input":"0.25","output":"0.33"}}},{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.58"},"eur":{"currency":"eur","input":"0.13","output":"0.5"}}}],"lastImportedAt":"2025-12-08T00:21:39.745Z","outputLimit":16384,"contextLength":131000},{"id":"qwen-turbo","aliases":["qwen/qwen-turbo-2024-11-01","qwen-turbo-2024-11-01","qwen/qwen-turbo","qwen-turbo"],"description":{"en":"Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and low cost, suitable for simple tasks.","de":"Qwen-Turbo, basierend auf Qwen2.5, ist ein 1M-Kontext-Modell, das sich durch hohe Geschwindigkeit und geringe Kosten auszeichnet und für einfache Aufgaben geeignet ist."},"name":"Qwen Turbo","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","max_tokens","presence_penalty","response_format","seed","tool_choice","top_p"],"providers":[{"providerId":"alibaba","contextLength":1000000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.20"},"eur":{"currency":"eur","input":"0.04313557","output":"0.17254228"}}},{"providerId":"openrouter","contextLength":1000000,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.2"},"eur":{"currency":"eur","input":"0.04313557","output":"0.17254228"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.685Z"},{"id":"qwen-vl-max","aliases":["qwen/qwen-vl-max-2025-01-25","qwen-vl-max-2025-01-25","qwen/qwen-vl-max","qwen-vl-max"],"description":{"en":"Qwen VL Max is a visual understanding model with 7500 tokens context length. It excels in delivering optimal performance for a broader spectrum of complex tasks.","de":"Qwen VL Max ist ein visuelles Verstehensmodell mit einer Kontextlänge von 7500 Token. Es zeichnet sich durch eine optimale Leistung für ein breiteres Spektrum komplexer Aufgaben aus."},"name":"Qwen-VL Max","toolCalling":true,"knowledge":"2024-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","max_tokens","presence_penalty","response_format","seed","tool_choice","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.80","output":"3.20"},"eur":{"currency":"eur","input":"0.69016912","output":"2.76067648"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.8","output":"3.2"},"eur":{"currency":"eur","input":"0.69016912","output":"2.76067648"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.685Z"},{"id":"qwen-vl-ocr","aliases":["qwen-vl-ocr"],"name":"Qwen-VL OCR","knowledge":"2024-04-01","input":["text","image"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"alibaba","contextLength":34096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.72","output":"0.72"},"eur":{"currency":"eur","input":"0.621152208","output":"0.621152208"}}}],"lastImportedAt":"2025-11-19T12:06:32.687Z"},{"id":"qwen-vl-plus","aliases":["qwen/qwen-vl-plus","qwen-vl-plus"],"description":{"en":"Qwen's Enhanced Large Visual Language Model. Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and extreme aspect ratios for image input. It delivers significant performance across a broad range of visual tasks.","de":"Qwens verbessertes großes visuelles Sprachmodell. Deutlich verbessert für detaillierte Erkennungsfähigkeiten und Texterkennungsfähigkeiten, unterstützt ultrahohe Pixelauflösungen bis zu Millionen von Pixeln und extreme Seitenverhältnisse für die Bildeingabe. Es bietet eine beachtliche Leistung für eine breite Palette visueller Aufgaben."},"name":"Qwen-VL Plus","toolCalling":true,"knowledge":"2024-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","max_tokens","presence_penalty","response_format","seed","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.21","output":"0.63"},"eur":{"currency":"eur","input":"0.181169394","output":"0.543508182"}}},{"providerId":"openrouter","contextLength":7500,"price":{"usd":{"currency":"usd","input":"0.21","output":"0.63"},"eur":{"currency":"eur","input":"0.181169394","output":"0.543508182"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.689Z"},{"id":"qwen.qwen3-32b-v1:0","aliases":["qwen.qwen3-32b-v1:0"],"name":"Qwen3 32B (dense)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.12940671","output":"0.51762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.758Z"},{"id":"qwen.qwen3-coder-30b-a3b-v1:0","aliases":["qwen.qwen3-coder-30b-a3b-v1:0"],"name":"Qwen3 Coder 30B A3B Instruct","toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":262144,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.60"},"eur":{"currency":"eur","input":"0.12940671","output":"0.51762684"}}}],"lastImportedAt":"2025-11-19T12:06:32.757Z"},{"id":"qwen.qwen3-coder-480b-a35b-v1:0","aliases":["qwen.qwen3-coder-480b-a35b-v1:0"],"name":"Qwen3 Coder 480B A35B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"amazon-bedrock","contextLength":131072,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.22","output":"1.80"},"eur":{"currency":"eur","input":"0.189796508","output":"1.55288052"}}}],"lastImportedAt":"2025-11-19T12:06:32.759Z"},{"id":"qwen1.5-0.5b-chat","aliases":["qwen1.5-0.5b-chat"],"name":"@cf/qwen/qwen1.5-0.5b-chat","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.442Z","outputLimit":32000,"contextLength":32000},{"id":"qwen1.5-1.8b-chat","aliases":["qwen1.5-1.8b-chat"],"name":"@cf/qwen/qwen1.5-1.8b-chat","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.090Z","outputLimit":32000,"contextLength":32000},{"id":"qwen1.5-14b-chat-awq","aliases":["qwen1.5-14b-chat-awq"],"name":"@cf/qwen/qwen1.5-14b-chat-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":7500,"outputLimit":7500,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.069Z","outputLimit":7500,"contextLength":7500},{"id":"qwen1.5-7b-chat-awq","aliases":["qwen1.5-7b-chat-awq"],"name":"@cf/qwen/qwen1.5-7b-chat-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":20000,"outputLimit":20000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.784Z","outputLimit":20000,"contextLength":20000},{"id":"qwen2-5-14b-instruct","aliases":["qwen2-5-14b-instruct"],"name":"Qwen2.5 14B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.35","output":"1.40"},"eur":{"currency":"eur","input":"0.30194899","output":"1.20779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.686Z"},{"id":"qwen2-5-32b-instruct","aliases":["qwen2-5-32b-instruct"],"name":"Qwen2.5 32B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.70","output":"2.80"},"eur":{"currency":"eur","input":"0.60389798","output":"2.41559192"}}}],"lastImportedAt":"2025-11-19T12:06:32.688Z"},{"id":"qwen2-5-72b-instruct","aliases":["qwen/qwen-2.5-72b-instruct","qwen/qwen2.5-72b-instruct","qwen-2.5-72b-instruct","qwen2-5-72b-instruct","qwen2.5-72b-instruct"],"description":{"en":"Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2: - Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains. - Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. - Long-context Support up to 128K tokens and can generate up to 8K tokens. - Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).","de":"Qwen2.5 72B ist die neueste Serie der großen Sprachmodelle von Qwen. Qwen2.5 bringt die folgenden Verbesserungen gegenüber Qwen2: - Erheblich mehr Wissen und stark verbesserte Fähigkeiten in Codierung und Mathematik, dank unserer spezialisierten Expertenmodelle in diesen Bereichen. - Erhebliche Verbesserungen beim Verfolgen von Anweisungen, beim Erzeugen langer Texte (über 8K Token), beim Verstehen strukturierter Daten (z.B. Tabellen) und beim Erzeugen strukturierter Ausgaben, insbesondere JSON. Höhere Widerstandsfähigkeit gegenüber der Vielfalt von Systemaufforderungen, Verbesserung der Umsetzung von Rollenspielen und der Festlegung von Bedingungen für Chatbots. - Langer Kontext Unterstützt bis zu 128K Token und kann bis zu 8K Token erzeugen. - Mehrsprachige Unterstützung für über 29 Sprachen, darunter Chinesisch, Englisch, Französisch, Spanisch, Portugiesisch, Deutsch, Italienisch, Russisch, Japanisch, Koreanisch, Vietnamesisch, Thailändisch, Arabisch und mehr. Die Nutzung dieses Modells unterliegt der [Tongyi Qianwen LIZENZVEREINBARUNG] (https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)."},"name":"Qwen2.5 72B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"1.40","output":"5.60"},"eur":{"currency":"eur","input":"1.21","output":"4.86"}}},{"providerId":"chutes","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.26"},"eur":{"currency":"eur","input":"0.06","output":"0.23"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.26"},"eur":{"currency":"eur","input":"0.06","output":"0.23"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-27T00:20:22.885Z","outputLimit":8192,"contextLength":32768},{"id":"qwen2-5-7b-instruct","aliases":["qwen/qwen-2.5-7b-instruct","qwen/qwen2.5-7b-instruct","qwen-2.5-7b-instruct","qwen2-5-7b-instruct","qwen2.5-7b-instruct"],"description":{"en":"Qwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2: - Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains. - Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. - Long-context Support up to 128K tokens and can generate up to 8K tokens. - Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).","de":"Qwen2.5 7B ist die neueste Serie der großen Sprachmodelle von Qwen. Qwen2.5 bringt die folgenden Verbesserungen gegenüber Qwen2: - Erheblich mehr Wissen und stark verbesserte Fähigkeiten in Codierung und Mathematik, dank unserer spezialisierten Expertenmodelle in diesen Bereichen. - Erhebliche Verbesserungen beim Verfolgen von Anweisungen, beim Erzeugen langer Texte (über 8K Token), beim Verstehen strukturierter Daten (z.B. Tabellen) und beim Erzeugen strukturierter Ausgaben, insbesondere JSON. Höhere Widerstandsfähigkeit gegenüber der Vielfalt von Systemaufforderungen, Verbesserung der Umsetzung von Rollenspielen und der Festlegung von Bedingungen für Chatbots. - Langer Kontext Unterstützt bis zu 128K Token und kann bis zu 8K Token erzeugen. - Mehrsprachige Unterstützung für über 29 Sprachen, darunter Chinesisch, Englisch, Französisch, Spanisch, Portugiesisch, Deutsch, Italienisch, Russisch, Japanisch, Koreanisch, Vietnamesisch, Thailändisch, Arabisch und mehr. Die Nutzung dieses Modells unterliegt der [Tongyi Qianwen LIZENZVEREINBARUNG] (https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)."},"name":"Qwen2.5 7B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","top_k","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.17","output":"0.70"},"eur":{"currency":"eur","input":"0.15","output":"0.61"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.1"},"eur":{"currency":"eur","input":"0.03","output":"0.09"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-26T00:20:44.832Z","outputLimit":8192,"contextLength":32768},{"id":"qwen2-5-coder-7b-instruct","aliases":["qwen/qwen2.5-coder-7b-instruct","qwen2-5-coder-7b-instruct","qwen2.5-coder-7b-instruct"],"description":{"en":"Qwen2.5-Coder-7B-Instruct is a 7B parameter instruction-tuned language model optimized for code-related tasks such as code generation, reasoning, and bug fixing. Based on the Qwen2.5 architecture, it incorporates enhancements like RoPE, SwiGLU, RMSNorm, and GQA attention with support for up to 128K tokens using YaRN-based extrapolation. It is trained on a large corpus of source code, synthetic data, and text-code grounding, providing robust performance across programming languages and agentic coding workflows. This model is part of the Qwen2.5-Coder family and offers strong compatibility with tools like vLLM for efficient deployment. Released under the Apache 2.0 license.","de":"Qwen2.5-Coder-7B-Instruct ist ein auf 7B-Parameter-Befehle abgestimmtes Sprachmodell, das für codebezogene Aufgaben wie Codegenerierung, Schlussfolgerungen und Fehlerbehebung optimiert ist. Es basiert auf der Qwen2.5-Architektur und beinhaltet Erweiterungen wie RoPE, SwiGLU, RMSNorm und GQA-Aufmerksamkeit mit Unterstützung für bis zu 128K Token unter Verwendung von YaRN-basierter Extrapolation. Es wurde auf einem großen Korpus von Quellcode, synthetischen Daten und Textcode-Grounding trainiert und bietet eine robuste Leistung in allen Programmiersprachen und agentenbasierten Codierungsworkflows. Dieses Modell ist Teil der Qwen2.5-Coder-Familie und bietet eine hohe Kompatibilität mit Tools wie vLLM für einen effizienten Einsatz. Freigegeben unter der Apache 2.0 Lizenz."},"name":"Qwen2.5-Coder 7B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","response_format","structured_outputs","top_k","top_p"],"providers":[{"providerId":"alibaba-cn","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.29"},"eur":{"currency":"eur","input":"0.120779596","output":"0.250186306"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.09"},"eur":{"currency":"eur","input":"0.025881342","output":"0.077644026"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.715Z"},{"id":"qwen2-5-math-72b-instruct","aliases":["qwen2-5-math-72b-instruct"],"name":"Qwen2.5-Math 72B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":4096,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.57","output":"1.72"},"eur":{"currency":"eur","input":"0.491745498","output":"1.483863608"}}}],"lastImportedAt":"2025-11-19T12:06:32.715Z"},{"id":"qwen2-5-math-7b-instruct","aliases":["qwen2-5-math-7b-instruct"],"name":"Qwen2.5-Math 7B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba-cn","contextLength":4096,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.29"},"eur":{"currency":"eur","input":"0.120779596","output":"0.250186306"}}}],"lastImportedAt":"2025-11-19T12:06:32.716Z"},{"id":"qwen2-5-omni-7b","aliases":["qwen2-5-omni-7b"],"name":"Qwen2.5-Omni 7B","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":32768,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.40"},"eur":{"currency":"eur","input":"0.08627114","output":"0.34508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.688Z"},{"id":"qwen2-5-vl-72b-instruct","aliases":["qwen/qwen2.5-vl-72b-instruct:free","qwen/qwen2.5-vl-72b-instruct","qwen2.5-vl-72b-instruct:free","qwen2-5-vl-72b-instruct","qwen2.5-vl-72b-instruct"],"description":{"en":"Qwen2.5-VL is proficient in recognizing common objects such as flowers, birds, fish, and insects. It is also highly capable of analyzing texts, charts, icons, graphics, and layouts within images.","de":"Qwen2.5-VL ist in der Lage, gängige Objekte wie Blumen, Vögel, Fische und Insekten zu erkennen. Es ist auch sehr gut in der Lage, Texte, Diagramme, Symbole, Grafiken und Layouts in Bildern zu analysieren."},"name":"Qwen2.5-VL 72B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"2.80","output":"8.40"},"eur":{"currency":"eur","input":"2.41","output":"7.24"}}},{"providerId":"chutes","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.13"},"eur":{"currency":"eur","input":"0.03","output":"0.11"}}},{"providerId":"openrouter","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.13"},"eur":{"currency":"eur","input":"0.03","output":"0.11"}}},{"providerId":"ovhcloud","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"1.01","output":"1.01"},"eur":{"currency":"eur","input":"0.87","output":"0.87"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":32768,"outputLimit":32768,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-03T12:09:06.554Z","outputLimit":8192,"contextLength":32000},{"id":"qwen2-5-vl-7b-instruct","aliases":["qwen/qwen-2.5-vl-7b-instruct","qwen/qwen2.5-vl-7b-instruct","qwen/qwen-2-vl-7b-instruct","qwen-2.5-vl-7b-instruct","qwen2-5-vl-7b-instruct","qwen2.5-vl-7b-instruct","qwen-2-vl-7b-instruct"],"description":{"en":"Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements: - SoTA understanding of images of various resolution & ratio: Qwen2.5-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc. - Understanding videos of 20min+: Qwen2.5-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc. - Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2.5-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions. - Multilingual Support: to serve global users, besides English and Chinese, Qwen2.5-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc. For more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL). Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).","de":"Qwen2.5 VL 7B ist ein multimodaler LLM des Qwen-Teams mit den folgenden wesentlichen Verbesserungen: - SoTA-Verständnis von Bildern mit unterschiedlicher Auflösung und unterschiedlichem Verhältnis: Qwen2.5-VL erreicht Spitzenleistungen bei Benchmarks für visuelles Verstehen, einschließlich MathVista, DocVQA, RealWorldQA, MTVQA, etc. - Verstehen von Videos von 20min+: Qwen2.5-VL kann Videos mit einer Länge von mehr als 20 Minuten verstehen, um hochwertige videobasierte Fragen zu beantworten, Dialoge zu führen, Inhalte zu erstellen, usw. - Agenten, die Handys, Roboter usw. bedienen können: Qwen2.5-VL ist in der Lage, komplexe Schlussfolgerungen zu ziehen und Entscheidungen zu treffen und kann in Geräte wie Mobiltelefone, Roboter usw. integriert werden, um diese auf der Grundlage von visuellen Umgebungen und Textanweisungen automatisch zu bedienen. - Mehrsprachige Unterstützung: Um globale Benutzer zu bedienen, unterstützt Qwen2.5-VL neben Englisch und Chinesisch nun auch das Verstehen von Texten in verschiedenen Sprachen innerhalb von Bildern, einschließlich der meisten europäischen Sprachen, Japanisch, Koreanisch, Arabisch, Vietnamesisch, etc. Weitere Einzelheiten finden Sie in diesem [Blogpost] (https://qwenlm.github.io/blog/qwen2-vl/) und [GitHub Repo] (https://github.com/QwenLM/Qwen2-VL). Die Nutzung dieses Modells unterliegt der [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)."},"name":"Qwen2.5-VL 7B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","top_k","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.35","output":"1.05"},"eur":{"currency":"eur","input":"0.30194899","output":"0.90584697"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.2","output":"0.2"},"eur":{"currency":"eur","input":"0.17254228","output":"0.17254228"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.688Z"},{"id":"qwen2.5-coder-32b-instruct","aliases":["workers-ai/qwen2.5-coder-32b-instruct","hf:qwen/qwen2.5-coder-32b-instruct","qwen/qwen-2.5-coder-32b-instruct","qwen/qwen2.5-coder-32b-instruct","qwen-2.5-coder-32b-instruct","qwen2.5-coder-32b-instruct","qwen2-5-coder-32b-instruct"],"description":{"en":"Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Qwen2.5-Coder brings the following improvements upon CodeQwen1.5: - Significantly improvements in **code generation**, **code reasoning** and **code fixing**. - A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies. To read more about its evaluation results, check out [Qwen 2.5 Coder's blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).","de":"Qwen2.5-Coder ist die neueste Serie von Code-Specific Qwen large language models (früher bekannt als CodeQwen). Qwen2.5-Coder bringt die folgenden Verbesserungen gegenüber CodeQwen1.5: - Signifikante Verbesserungen in **Code-Generierung**, **Code-Reasoning** und **Code-Fixing**. - Eine umfassendere Grundlage für reale Anwendungen wie **Code-Agenten**. Nicht nur die Verbesserung der Codierfähigkeiten, sondern auch die Beibehaltung der Stärken in Mathematik und allgemeinen Kompetenzen. Weitere Informationen zu den Evaluierungsergebnissen finden Sie im [Qwen 2.5 Coder's blog] (https://qwenlm.github.io/blog/qwen2.5-coder-family/)."},"name":"Qwen2.5 Coder 32B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"vultr","contextLength":12952,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.20"},"eur":{"currency":"eur","input":"0.17","output":"0.17"}}},{"providerId":"alibaba-cn","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.29","output":"0.86"},"eur":{"currency":"eur","input":"0.25","output":"0.74"}}},{"providerId":"chutes","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.11"},"eur":{"currency":"eur","input":"0.03","output":"0.09"}}},{"providerId":"cloudflare-workers-ai","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.66","output":"1.00"},"eur":{"currency":"eur","input":"0.57","output":"0.86"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"1.00"},"eur":{"currency":"eur","input":"0","output":"0.86"}}},{"providerId":"openrouter","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.11"},"eur":{"currency":"eur","input":"0.03","output":"0.09"}}},{"providerId":"ovhcloud","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.96","output":"0.96"},"eur":{"currency":"eur","input":"0.82","output":"0.82"}}},{"providerId":"synthetic","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.80","output":"0.80"},"eur":{"currency":"eur","input":"0.69","output":"0.69"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.628Z","outputLimit":2048,"contextLength":12952},{"id":"qwen2.5-coder-7b-fast","aliases":["qwen2.5-coder-7b-fast"],"name":"Qwen2.5 Coder 7B fast","knowledge":"2024-09-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"helicone","contextLength":32000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.09"},"eur":{"currency":"eur","input":"0.03","output":"0.08"}}}],"lastImportedAt":"2025-12-09T00:20:58.768Z","outputLimit":8192,"contextLength":32000},{"id":"qwen2.5-vl-32b-instruct","aliases":["qwen/qwen2.5-vl-32b-instruct:free","qwen/qwen2.5-vl-32b-instruct","qwen2.5-vl-32b-instruct:free","qwen2.5-vl-32b-instruct"],"description":{"en":"Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for enhanced mathematical reasoning, structured outputs, and visual problem-solving capabilities. It excels at visual analysis tasks, including object recognition, textual interpretation within images, and precise event localization in extended videos. Qwen2.5-VL-32B demonstrates state-of-the-art performance across multimodal benchmarks such as MMMU, MathVista, and VideoMME, while maintaining strong reasoning and clarity in text-based tasks like MMLU, mathematical problem-solving, and code generation.","de":"Qwen2.5-VL-32B ist ein multimodales Sprachmodell für das Sehen, das durch Verstärkungslernen für verbesserte mathematische Schlussfolgerungen, strukturierte Ausgaben und visuelle Problemlösungsfähigkeiten fein abgestimmt wurde. Es eignet sich hervorragend für visuelle Analyseaufgaben, einschließlich Objekterkennung, Textinterpretation in Bildern und präziser Ereignislokalisierung in ausgedehnten Videos. Qwen2.5-VL-32B demonstriert den neuesten Stand der Technik bei multimodalen Benchmarks wie MMMU, MathVista und VideoMME, während es bei textbasierten Aufgaben wie MMLU, mathematischen Problemlösungen und Codegenerierung eine starke Argumentation und Klarheit beibehält."},"name":"Qwen2.5 VL 32B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-09-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_logprobs","top_p"],"providers":[{"providerId":"chutes","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.22"},"eur":{"currency":"eur","input":"0.04","output":"0.19"}}},{"providerId":"io-net","contextLength":32000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.22"},"eur":{"currency":"eur","input":"0.04","output":"0.19"}}},{"providerId":"openrouter","contextLength":16384,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.22"},"eur":{"currency":"eur","input":"0.04","output":"0.19"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":8192,"outputLimit":8192,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-11-30T00:23:18.845Z","outputLimit":4096,"contextLength":16384},{"id":"qwen3-14b","aliases":["qwen/qwen3-14b-04-28","qwen/qwen3-14b:free","qwen3-14b-04-28","qwen/qwen3-14b","qwen3-14b:free","qwen3-14b"],"description":{"en":"Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series, designed for both complex reasoning and efficient dialogue. It supports seamless switching between a \"thinking\" mode for tasks like math, programming, and logical inference, and a \"non-thinking\" mode for general-purpose conversation. The model is fine-tuned for instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling.","de":"Qwen3-14B ist ein dichtes kausales Sprachmodell mit 14,8 B Parametern aus der Qwen3-Reihe, das sowohl für komplexe Schlussfolgerungen als auch für effiziente Dialoge entwickelt wurde. Es unterstützt den nahtlosen Wechsel zwischen einem \"denkenden\" Modus für Aufgaben wie Mathematik, Programmierung und logische Schlussfolgerungen und einem \"nicht denkenden\" Modus für allgemeine Konversation. Das Modell ist auf das Befolgen von Anweisungen, die Verwendung von Agententools, kreatives Schreiben und mehrsprachige Aufgaben in über 100 Sprachen und Dialekten abgestimmt. Es kann nativ 32K Token-Kontexte verarbeiten und lässt sich durch YaRN-basierte Skalierung auf 131K Token erweitern."},"name":"Qwen3 14B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.35","output":"1.40"},"eur":{"currency":"eur","input":"0.3","output":"1.21"}}},{"providerId":"chutes","contextLength":40960,"outputLimit":40960,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.22"},"eur":{"currency":"eur","input":"0.04","output":"0.19"}}},{"providerId":"openrouter","contextLength":40960,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.22"},"eur":{"currency":"eur","input":"0.04","output":"0.19"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":40960,"outputLimit":40960,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-11-27T00:20:22.822Z","outputLimit":8192,"contextLength":40960},{"id":"qwen3-235b","aliases":["qwen3-235b"],"name":"Venice Large 1.1","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"venice","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.45","output":"3.50"},"eur":{"currency":"eur","input":"0.39","output":"3.01"}}},{"providerId":"iflowcn","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-10T00:21:34.118Z","outputLimit":8192,"contextLength":128000},{"id":"qwen3-235b-a22b","aliases":["accounts/fireworks/models/qwen3-235b-a22b","qwen/qwen3-235b-a22b-04-28","qwen/qwen3-235b-a22b:free","qwen3-235b-a22b-04-28","qwen/qwen3-235b-a22b","qwen3-235b-a22b:free","qwen3-235b-a22b"],"description":{"en":"Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a \"thinking\" mode for complex reasoning, math, and code tasks, and a \"non-thinking\" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling.","de":"Qwen3-235B-A22B ist ein von Qwen entwickeltes Mixed-of-Experts (MoE)-Modell mit 235B Parametern, das 22B Parameter pro Vorwärtsdurchlauf aktiviert. Es unterstützt den nahtlosen Wechsel zwischen einem \"denkenden\" Modus für komplexe Denk-, Mathematik- und Codeaufgaben und einem \"nicht denkenden\" Modus für allgemeine Konversationseffizienz. Das Modell zeichnet sich durch starke Argumentationsfähigkeit, mehrsprachige Unterstützung (mehr als 100 Sprachen und Dialekte), fortgeschrittenes Befolgen von Anweisungen und die Fähigkeit zum Aufrufen von Agententools aus. Es kann von Haus aus mit einem Kontextfenster von 32K Token umgehen und lässt sich durch YaRN-basierte Skalierung auf bis zu 131K Token erweitern."},"name":"Qwen3 235B-A22B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","response_format","stop","structured_outputs","tool_choice","top_k","top_p","logit_bias","logprobs","min_p","repetition_penalty","seed","top_logprobs"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.70","output":"2.80"},"eur":{"currency":"eur","input":"0.61","output":"2.43"}}},{"providerId":"nvidia","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"chutes","contextLength":40960,"outputLimit":40960,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.04"}}},{"providerId":"fireworks-ai","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.22","output":"0.88"},"eur":{"currency":"eur","input":"0.19","output":"0.76"}}},{"providerId":"openrouter","contextLength":40960,"price":{"usd":{"currency":"usd","input":"0.18","output":"0.54"},"eur":{"currency":"eur","input":"0.16","output":"0.47"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":131072,"outputLimit":131072,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-11-27T00:20:22.886Z","outputLimit":8192,"contextLength":40960},{"id":"qwen3-235b-a22b-instruct","aliases":["qwen3-235b-a22b-instruct"],"name":"Qwen3-235B-A22B-Instruct","toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"iflowcn","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.754Z"},{"id":"qwen3-235b-a22b-thinking","aliases":["qwen3-235b-a22b-thinking"],"name":"Qwen3 235B A22B Thinking","reasoning":true,"knowledge":"2025-07-01","input":["text","image","video"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"helicone","contextLength":262144,"outputLimit":81920,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.90"},"eur":{"currency":"eur","input":"0.26","output":"2.49"}}}],"lastImportedAt":"2025-12-09T00:20:58.752Z","outputLimit":81920,"contextLength":262144},{"id":"qwen3-30b-a3b","aliases":["qwen/qwen3-30b-a3b-04-28","qwen/qwen3-30b-a3b:free","qwen3-30b-a3b-04-28","qwen/qwen3-30b-a3b","qwen3-30b-a3b:free","qwen3-30b-a3b"],"description":{"en":"Qwen3, the latest generation in the Qwen large language model series, features both dense and mixture-of-experts (MoE) architectures to excel in reasoning, multilingual support, and advanced agent tasks. Its unique ability to switch seamlessly between a thinking mode for complex reasoning and a non-thinking mode for efficient dialogue ensures versatile, high-quality performance. Significantly outperforming prior models like QwQ and Qwen2.5, Qwen3 delivers superior mathematics, coding, commonsense reasoning, creative writing, and interactive dialogue capabilities. The Qwen3-30B-A3B variant includes 30.5 billion parameters (3.3 billion activated), 48 layers, 128 experts (8 activated per task), and supports up to 131K token contexts with YaRN, setting a new standard among open-source models.","de":"Qwen3, die neueste Generation der großen Qwen-Sprachmodellserie, verfügt sowohl über eine dichte als auch über eine Mixed-of-Experts (MoE)-Architektur und zeichnet sich durch logisches Denken, mehrsprachige Unterstützung und fortgeschrittene Agentenaufgaben aus. Seine einzigartige Fähigkeit, nahtlos zwischen einem denkenden Modus für komplexe Schlussfolgerungen und einem nicht denkenden Modus für effiziente Dialoge zu wechseln, sorgt für eine vielseitige, hochwertige Leistung. Qwen3 übertrifft Vorgängermodelle wie QwQ und Qwen2.5 deutlich und bietet überlegene Fähigkeiten in den Bereichen Mathematik, Codierung, logisches Denken, kreatives Schreiben und interaktive Dialoge. Die Qwen3-30B-A3B-Variante umfasst 30,5 Milliarden Parameter (3,3 Milliarden aktiviert), 48 Schichten, 128 Experten (8 pro Aufgabe aktiviert) und unterstützt bis zu 131K Token-Kontexte mit YaRN und setzt damit einen neuen Standard unter den Open-Source-Modellen."},"name":"Qwen3 30B A3B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"chutes","contextLength":40960,"outputLimit":40960,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.22"},"eur":{"currency":"eur","input":"0.05","output":"0.19"}}},{"providerId":"helicone","contextLength":41000,"outputLimit":41000,"price":{"usd":{"currency":"usd","input":"0.08","output":"0.29"},"eur":{"currency":"eur","input":"0.07","output":"0.25"}}},{"providerId":"openrouter","contextLength":40960,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.22"},"eur":{"currency":"eur","input":"0.05","output":"0.19"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":40960,"outputLimit":40960,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.408Z","outputLimit":40960,"contextLength":40960},{"id":"qwen3-30b-a3b-fp8","aliases":["workers-ai/qwen3-30b-a3b-fp8","qwen3-30b-a3b-fp8"],"name":"@cf/qwen/qwen3-30b-a3b-fp8","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.34"},"eur":{"currency":"eur","input":"0.04","output":"0.29"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.228Z","outputLimit":16384,"contextLength":32768},{"id":"qwen3-32b","aliases":["qwen/qwen3-32b-04-28","qwen/qwen3-32b:free","qwen3-32b-04-28","qwen/qwen3-32b","qwen3-32b:free","qwen3-32b"],"description":{"en":"Qwen3-32B is a dense 32.8B parameter causal language model from the Qwen3 series, optimized for both complex reasoning and efficient dialogue. It supports seamless switching between a \"thinking\" mode for tasks like math, coding, and logical inference, and a \"non-thinking\" mode for faster, general-purpose conversation. The model demonstrates strong performance in instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling.","de":"Qwen3-32B ist ein dichtes kausales Sprachmodell mit 32,8 B Parametern aus der Qwen3-Serie, das sowohl für komplexe Schlussfolgerungen als auch für effiziente Dialoge optimiert ist. Es unterstützt den nahtlosen Wechsel zwischen einem \"denkenden\" Modus für Aufgaben wie Mathematik, Codierung und logische Schlussfolgerungen und einem \"nicht denkenden\" Modus für schnellere, allgemeine Konversation. Das Modell zeigt starke Leistungen beim Befolgen von Anweisungen, bei der Verwendung von Agentenwerkzeugen, beim kreativen Schreiben und bei mehrsprachigen Aufgaben in über 100 Sprachen und Dialekten. Es kann nativ 32K Token-Kontexte verarbeiten und lässt sich durch YaRN-basierte Skalierung auf 131K Token erweitern."},"name":"Qwen3 32B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.70","output":"2.80"},"eur":{"currency":"eur","input":"0.6","output":"2.4"}}},{"providerId":"groq","contextLength":131072,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.29","output":"0.59"},"eur":{"currency":"eur","input":"0.25","output":"0.51"}}},{"providerId":"chutes","contextLength":40960,"outputLimit":40960,"price":{"usd":{"currency":"usd","input":"0.08","output":"0.24"},"eur":{"currency":"eur","input":"0.07","output":"0.21"}}},{"providerId":"cortecs","contextLength":16384,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.33"},"eur":{"currency":"eur","input":"0.09","output":"0.28"}}},{"providerId":"helicone","contextLength":131072,"outputLimit":40960,"price":{"usd":{"currency":"usd","input":"0.29","output":"0.59"},"eur":{"currency":"eur","input":"0.25","output":"0.51"}}},{"providerId":"ovhcloud","contextLength":32000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.25"},"eur":{"currency":"eur","input":"0.08","output":"0.21"}}},{"providerId":"iflowcn","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":40960,"price":{"usd":{"currency":"usd","input":"0.08","output":"0.24"},"eur":{"currency":"eur","input":"0.07","output":"0.21"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":40960,"outputLimit":40960,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.135Z","outputLimit":16384,"contextLength":16384},{"id":"qwen3-4b","aliases":["qwen/qwen3-4b-04-28","qwen/qwen3-4b:free","qwen3-4b-04-28","qwen3-4b:free","qwen/qwen3-4b","qwen3-4b"],"description":{"en":"Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support both general-purpose and reasoning-intensive tasks. It introduces a dual-mode architecture—thinking and non-thinking—allowing dynamic switching between high-precision logical reasoning and efficient dialogue generation. This makes it well-suited for multi-turn chat, instruction following, and complex agent workflows.","de":"Qwen3-4B ist ein dichtes Sprachmodell mit 4 Milliarden Parametern aus der Qwen3-Serie, das sowohl allgemeine als auch schlussfolgernde Aufgaben unterstützt. Es führt eine Dual-Mode-Architektur ein - Denken und Nicht-Denken -, die ein dynamisches Umschalten zwischen hochpräzisem logischem Denken und effizienter Dialoggenerierung ermöglicht. Dadurch eignet er sich gut für Multi-Turn-Chat, das Befolgen von Anweisungen und komplexe Agenten-Workflows."},"name":"Venice Small","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","response_format","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"venice","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.05","output":"0.15"},"eur":{"currency":"eur","input":"0.04","output":"0.13"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":40960,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-12-10T00:21:34.138Z","outputLimit":8192,"contextLength":32768},{"id":"qwen3-8b","aliases":["qwen/qwen3-8b-04-28","qwen/qwen3-8b:free","qwen3-8b-04-28","qwen3-8b:free","qwen/qwen3-8b","qwen3-8b"],"description":{"en":"Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both reasoning-heavy tasks and efficient dialogue. It supports seamless switching between \"thinking\" mode for math, coding, and logical inference, and \"non-thinking\" mode for general conversation. The model is fine-tuned for instruction-following, agent integration, creative writing, and multilingual use across 100+ languages and dialects. It natively supports a 32K token context window and can extend to 131K tokens with YaRN scaling.","de":"Qwen3-8B ist ein dichtes kausales Sprachmodell mit 8,2B Parametern aus der Qwen3-Reihe, das sowohl für schlussfolgernde Aufgaben als auch für effiziente Dialoge konzipiert ist. Es unterstützt den nahtlosen Wechsel zwischen dem \"denkenden\" Modus für Mathematik, Codierung und logische Schlussfolgerungen und dem \"nicht denkenden\" Modus für allgemeine Konversation. Das Modell ist auf das Befolgen von Anweisungen, die Integration von Agenten, kreatives Schreiben und die mehrsprachige Verwendung in über 100 Sprachen und Dialekten abgestimmt. Es unterstützt von Haus aus ein Kontextfenster mit 32K Token und kann mit YaRN-Skalierung auf 131K Token erweitert werden."},"name":"Qwen3 8B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.18","output":"0.70"},"eur":{"currency":"eur","input":"0.16","output":"0.61"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.028","output":"0.1104"},"eur":{"currency":"eur","input":"0.02","output":"0.1"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":40960,"outputLimit":40960,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-11-25T00:19:20.552Z","outputLimit":8192,"contextLength":128000},{"id":"qwen3-asr-flash","aliases":["qwen3-asr-flash"],"name":"Qwen3-ASR Flash","knowledge":"2024-04-01","input":["audio"],"output":["text"],"parameters":[],"providers":[{"providerId":"alibaba","contextLength":53248,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.04"},"eur":{"currency":"eur","input":"0.034508456","output":"0.034508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.684Z"},{"id":"qwen3-coder","aliases":["qwen/qwen3-coder-480b-a35b-instruct","qwen/qwen3-coder-480b-a35b-07-25","qwen3-coder-480b-a35b-instruct","qwen3-coder-480b-a35b-07-25","qwen/qwen3-coder:free","qwen/qwen3-coder","qwen3-coder:free","qwen3-coder"],"description":{"en":"Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts). Pricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.","de":"Qwen3-Coder-480B-A35B-Instruct ist ein Mixture-of-Experts (MoE) Codegenerierungsmodell, das vom Qwen-Team entwickelt wurde. Es ist für agentenbasierte Kodierungsaufgaben wie Funktionsaufrufe, Werkzeugnutzung und Long-Context-Reasoning über Repositories optimiert. Das Modell hat insgesamt 480 Milliarden Parameter, wobei 35 Milliarden pro Vorwärtsdurchlauf aktiv sind (8 von 160 Experten). Die Preise für die Alibaba-Endpunkte variieren je nach Kontextlänge. Sobald eine Anfrage mehr als 128k Eingabe-Token umfasst, wird der höhere Preis verwendet."},"name":"Qwen3 Coder 480B A35B Instruct Turbo","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","seed","stop","tool_choice","top_k","top_p","logit_bias","logprobs","min_p","reasoning","response_format","structured_outputs","top_logprobs"],"providers":[{"providerId":"helicone","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.22","output":"0.95"},"eur":{"currency":"eur","input":"0.19","output":"0.82"}}},{"providerId":"opencode","contextLength":262144,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.45","output":"1.80"},"eur":{"currency":"eur","input":"0.39","output":"1.54"}}},{"providerId":"fastrouter","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}},{"providerId":"openrouter","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}},{"providerId":"iflowcn","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":262000,"outputLimit":66536,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.486Z","outputLimit":16384,"contextLength":256000},{"id":"qwen3-coder-30b","aliases":["qwen/qwen3-coder-30b","qwen3-coder-30b"],"name":"Qwen3 Coder 30B","toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"lmstudio","contextLength":262144,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.755Z"},{"id":"qwen3-coder-30b-a3b-instruct","aliases":["qwen/qwen3-coder-30b-a3b-instruct","qwen3-coder-30b-a3b-instruct"],"description":{"en":"Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, and agentic tool use. Built on the Qwen3 architecture, it supports a native context length of 256K tokens (extendable to 1M with Yarn) and performs strongly in tasks involving function calls, browser use, and structured code completion. This model is optimized for instruction-following without “thinking mode”, and integrates well with OpenAI-compatible tool-use formats.","de":"Qwen3-Coder-30B-A3B-Instruct ist ein 30,5B-Parameter Mixture-of-Experts (MoE)-Modell mit 128 Experten (8 aktive pro Vorwärtsdurchlauf), das für fortgeschrittene Codegenerierung, Repository-Scale-Verständnis und den Einsatz von Agententools entwickelt wurde. Es basiert auf der Qwen3-Architektur, unterstützt eine native Kontextlänge von 256K Token (erweiterbar auf 1M mit Yarn) und zeigt starke Leistungen bei Aufgaben, die Funktionsaufrufe, Browser-Nutzung und strukturierte Code-Vervollständigung beinhalten. Dieses Modell ist für das Befolgen von Anweisungen ohne \"Denkmodus\" optimiert und lässt sich gut in OpenAI-kompatible Formate für die Werkzeugnutzung integrieren."},"name":"Qwen3-Coder 30B-A3B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"alibaba","contextLength":262144,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.45","output":"2.25"},"eur":{"currency":"eur","input":"0.39","output":"1.93"}}},{"providerId":"chutes","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.25"},"eur":{"currency":"eur","input":"0.05","output":"0.21"}}},{"providerId":"helicone","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.30"},"eur":{"currency":"eur","input":"0.09","output":"0.26"}}},{"providerId":"ovhcloud","contextLength":256000,"outputLimit":256000,"price":{"usd":{"currency":"usd","input":"0.07","output":"0.26"},"eur":{"currency":"eur","input":"0.06","output":"0.22"}}},{"providerId":"modelscope","contextLength":262144,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"scaleway","contextLength":128000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.80"},"eur":{"currency":"eur","input":"0.17","output":"0.69"}}},{"providerId":"openrouter","contextLength":262144,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.25"},"eur":{"currency":"eur","input":"0.05","output":"0.21"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.133Z","outputLimit":8192,"contextLength":128000},{"id":"qwen3-coder-480b","aliases":["qwen-3-coder-480b","qwen3-coder-480b"],"name":"Qwen3 Coder 480B","toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}},{"providerId":"cerebras","contextLength":131000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"2.00","output":"2.00"},"eur":{"currency":"eur","input":"1.74","output":"1.74"}}}],"lastImportedAt":"2025-11-23T00:23:30.628Z","outputLimit":8192,"contextLength":131000,"deprecated":true},{"id":"qwen3-coder-480b-a35b-instruct","aliases":["accounts/fireworks/models/qwen3-coder-480b-a35b-instruct","hf:qwen/qwen3-coder-480b-a35b-instruct","qwen/qwen3-coder-480b-a35b-instruct","qwen3-coder-480b-a35b-instruct"],"description":{"en":"Qwen3-Coder-480B-A35B-Instruct is a causal language model optimized for agentic coding and long-context tasks, supporting 256K tokens natively, extendable to 1M, with high performance across coding platforms.","de":"Qwen3-Coder-480B-A35B-Instruct ist ein kausales Sprachmodell, das für die Kodierung von Agenten und Aufgaben mit langem Kontext optimiert ist. Es unterstützt nativ 256K Token, erweiterbar auf 1M, mit hoher Leistung auf allen Kodierungsplattformen."},"name":"Qwen3-Coder 480B-A35B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":262144,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.50","output":"7.50"},"eur":{"currency":"eur","input":"1.29","output":"6.44"}}},{"providerId":"nvidia","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"nebius","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.80"},"eur":{"currency":"eur","input":"0.34","output":"1.55"}}},{"providerId":"venice","contextLength":262144,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.75","output":"3.00"},"eur":{"currency":"eur","input":"0.64","output":"2.58"}}},{"providerId":"cortecs","contextLength":262000,"outputLimit":262000,"price":{"usd":{"currency":"usd","input":"0.44","output":"1.98"},"eur":{"currency":"eur","input":"0.38","output":"1.7"}}},{"providerId":"baseten","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.38","output":"1.53"},"eur":{"currency":"eur","input":"0.33","output":"1.31"}}},{"providerId":"huggingface","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"2.00","output":"2.00"},"eur":{"currency":"eur","input":"1.72","output":"1.72"}}},{"providerId":"wandb","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"1.00","output":"1.50"},"eur":{"currency":"eur","input":"0.86","output":"1.29"}}},{"providerId":"synthetic","contextLength":256000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"2.00","output":"2.00"},"eur":{"currency":"eur","input":"1.72","output":"1.72"}}},{"providerId":"deepinfra","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.40","output":"1.60"},"eur":{"currency":"eur","input":"0.34","output":"1.37"}}},{"providerId":"aihubmix","contextLength":262144,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.82","output":"3.29"},"eur":{"currency":"eur","input":"0.7","output":"2.82"}}},{"providerId":"fireworks-ai","contextLength":256000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.45","output":"1.80"},"eur":{"currency":"eur","input":"0.39","output":"1.55"}}}],"lastImportedAt":"2025-12-10T00:21:33.625Z","outputLimit":8192,"contextLength":256000},{"id":"qwen3-coder-480b-a35b-instruct-fp8","aliases":["qwen/qwen3-coder-480b-a35b-instruct-fp8","qwen3-coder-480b-a35b-instruct-fp8"],"description":{"en":"Qwen3-Coder-480B-A35B-Instruct is a 480B parameter causal language model optimized for agentic coding and long-context tasks, supporting up to 256K tokens natively. It features advanced tool calling capabilities, performs well in coding-related tasks, and is suitable for various inference frameworks.","de":"Qwen3-Coder-480B-A35B-Instruct ist ein 480B-Parameter-Kausal-Sprachmodell, das für agenturische Kodierung und Aufgaben mit langem Kontext optimiert ist und nativ bis zu 256K Token unterstützt. Es verfügt über fortschrittliche Funktionen zum Aufrufen von Werkzeugen, erbringt gute Leistungen bei kodierungsbezogenen Aufgaben und ist für verschiedene Inferenz-Frameworks geeignet."},"name":"Qwen3 Coder 480B A35B Instruct (FP8)","toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"chutes","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.22","output":"0.95"},"eur":{"currency":"eur","input":"0.189796508","output":"0.81957583"}}},{"providerId":"togetherai","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"2.00","output":"2.00"},"eur":{"currency":"eur","input":"1.7254228","output":"1.7254228"}}},{"providerId":"submodel","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.80"},"eur":{"currency":"eur","input":"0.17254228","output":"0.69016912"}}}],"lastImportedAt":"2025-11-19T12:06:32.720Z"},{"id":"qwen3-coder-480b-a35b-instruct-int4-mixed-ar","aliases":["intel/qwen3-coder-480b-a35b-instruct-int4-mixed-ar","qwen3-coder-480b-a35b-instruct-int4-mixed-ar"],"description":{"en":"This model is an int4 mixed quantization version of Qwen/Qwen3-Coder-480B-A35B-Instruct designed for text generation tasks. It supports vLLM and can be used on CPU, Intel GPU, or CUDA. Users should be aware of potential biases and inaccuracies in generated content and perform safety testing before deployment.","de":"Dieses Modell ist eine Version von Qwen/Qwen3-Coder-480B-A35B-Instruct mit gemischter Quantisierung (int4), die für Aufgaben der Texterzeugung entwickelt wurde. Es unterstützt vLLM und kann auf CPU, Intel GPU oder CUDA verwendet werden. Benutzer sollten sich möglicher Verzerrungen und Ungenauigkeiten in den generierten Inhalten bewusst sein und vor dem Einsatz Sicherheitstests durchführen."},"name":"Qwen 3 Coder 480B","toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"io-net","contextLength":106000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.22","output":"0.95"},"eur":{"currency":"eur","input":"0.19","output":"0.82"}}}],"lastImportedAt":"2025-11-30T00:23:20.998Z","outputLimit":4096,"contextLength":106000},{"id":"qwen3-coder-480b-a35b-instruct-turbo","aliases":["qwen/qwen3-coder-480b-a35b-instruct-turbo","qwen3-coder-480b-a35b-instruct-turbo"],"name":"Qwen3 Coder 480B A35B Instruct Turbo","toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"deepinfra","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}}],"lastImportedAt":"2025-11-19T12:06:32.754Z"},{"id":"qwen3-coder-480b-cloud","aliases":["qwen3-coder-480b-cloud"],"name":"Qwen3 Coder 480B","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.507Z","outputLimit":8192,"contextLength":200000,"deprecated":true},{"id":"qwen3-coder-flash","aliases":["qwen/qwen3-coder-flash","qwen3-coder-flash"],"description":{"en":"Qwen3 Coder Flash is Alibaba's fast and cost efficient version of their proprietary Qwen3 Coder Plus. It is a powerful coding agent model specializing in autonomous programming via tool calling and environment interaction, combining coding proficiency with versatile general-purpose abilities.","de":"Qwen3 Coder Flash ist die schnelle und kostengünstige Version von Alibabas eigenem Qwen3 Coder Plus. Es handelt sich um ein leistungsfähiges Coding-Agent-Modell, das auf autonomes Programmieren durch Tool-Aufrufe und Interaktion mit der Umgebung spezialisiert ist und Programmierkenntnisse mit vielseitigen Allzweckfähigkeiten kombiniert."},"name":"Qwen3 Coder Flash","toolCalling":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","max_tokens","presence_penalty","response_format","seed","tool_choice","top_p"],"providers":[{"providerId":"alibaba","contextLength":1000000,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.50"},"eur":{"currency":"eur","input":"0.26","output":"1.3"}}},{"providerId":"openrouter","contextLength":128000,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.50"},"eur":{"currency":"eur","input":"0.26","output":"1.3"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-26T12:09:02.614Z","outputLimit":65536,"contextLength":128000},{"id":"qwen3-coder-plus","aliases":["alibaba/qwen3-coder-plus","qwen/qwen3-coder-plus","qwen3-coder-plus"],"description":{"en":"Qwen3 Coder Plus is Alibaba's proprietary version of the Open Source Qwen3 Coder 480B A35B. It is a powerful coding agent model specializing in autonomous programming via tool calling and environment interaction, combining coding proficiency with versatile general-purpose abilities.","de":"Qwen3 Coder Plus ist die von Alibaba entwickelte Version des Open-Source-Systems Qwen3 Coder 480B A35B. Es handelt sich um ein leistungsfähiges Coding Agent-Modell, das auf autonomes Programmieren durch Tool-Aufrufe und Interaktion mit der Umgebung spezialisiert ist und Coding-Kenntnisse mit vielseitigen Allzweckfähigkeiten kombiniert."},"name":"Qwen3 Coder Plus","toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","max_tokens","presence_penalty","response_format","seed","structured_outputs","tool_choice","top_p"],"providers":[{"providerId":"alibaba","contextLength":1048576,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.8627114","output":"4.313557"}}},{"providerId":"vercel","contextLength":1000000,"outputLimit":1000000,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.8627114","output":"4.313557"}}},{"providerId":"zenmux","contextLength":1000000,"outputLimit":66540,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.8627114","output":"4.313557"}}},{"providerId":"iflowcn","contextLength":256000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"1","output":"5"},"eur":{"currency":"eur","input":"0.8627114","output":"4.313557"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.689Z"},{"id":"qwen3-coder:480b-cloud","aliases":["qwen3-coder:480b-cloud"],"name":"Qwen3 Coder 480B","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-12-04T00:21:43.577Z","outputLimit":8192,"contextLength":200000},{"id":"qwen3-coder:exacto","aliases":["qwen/qwen3-coder-480b-a35b-instruct","qwen/qwen3-coder-480b-a35b-07-25","qwen3-coder-480b-a35b-instruct","qwen3-coder-480b-a35b-07-25","qwen/qwen3-coder:exacto","qwen3-coder:exacto"],"description":{"en":"Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts). Pricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.","de":"Qwen3-Coder-480B-A35B-Instruct ist ein Mixture-of-Experts (MoE) Codegenerierungsmodell, das vom Qwen-Team entwickelt wurde. Es ist für agentenbasierte Kodierungsaufgaben wie Funktionsaufrufe, Werkzeugnutzung und Long-Context-Reasoning über Repositories optimiert. Das Modell hat insgesamt 480 Milliarden Parameter, wobei 35 Milliarden pro Vorwärtsdurchlauf aktiv sind (8 von 160 Experten). Die Preise für die Alibaba-Endpunkte variieren je nach Kontextlänge. Sobald eine Anfrage mehr als 128k Eingabe-Token umfasst, wird der höhere Preis verwendet."},"name":"Qwen3 Coder (exacto)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"openrouter","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.38","output":"1.8"},"eur":{"currency":"eur","input":"0.33","output":"1.54"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T12:08:56.341Z","outputLimit":32768,"contextLength":131072},{"id":"qwen3-embedding-0.6b","aliases":["workers-ai/qwen3-embedding-0.6b","qwen3-embedding-0.6b"],"name":"qwen3 emuedding 0.6b","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:42.294Z","outputLimit":16384,"contextLength":128000},{"id":"qwen3-embedding-4b","aliases":["qwen/qwen3-embedding-4b","qwen3-embedding-4b"],"description":{"en":"The Qwen3-Embedding-4B model is designed for text embedding and ranking tasks, supporting over 100 languages. With 4 billion parameters, it excels in text retrieval, classification, and clustering. It offers flexible output dimensions (32 to 2560) and customizable instructions for enhanced performance in specific applications.","de":"Das Qwen3-Embedding-4B-Modell wurde für die Einbettung von Text und für Ranking-Aufgaben entwickelt und unterstützt über 100 Sprachen. Mit 4 Milliarden Parametern zeichnet es sich bei der Textsuche, Klassifizierung und beim Clustering aus. Es bietet flexible Ausgabedimensionen (32 bis 2560) und anpassbare Anweisungen für eine verbesserte Leistung in spezifischen Anwendungen."},"name":"Qwen 3 Embedding 4B","openWeights":true,"knowledge":"2024-12-01","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"huggingface","contextLength":32000,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.01","output":"0.00"},"eur":{"currency":"eur","input":"0.008627114","output":"0"}}},{"providerId":"inference","contextLength":32000,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.01","output":"0.00"},"eur":{"currency":"eur","input":"0.008627114","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.728Z"},{"id":"qwen3-embedding-8b","aliases":["qwen/qwen3-embedding-8b","qwen3-embedding-8b"],"description":{"en":"The Qwen3-Embedding-8B model is a text embedding model designed for tasks like text retrieval, classification, and clustering. It offers multilingual support for 100+ languages, with 8 billion parameters and a context length of 32k. It excels in performance and flexibility, allowing user-defined dimensions and instructions for tailored applications.","de":"Das Qwen3-Embedding-8B-Modell ist ein Text-Embedding-Modell, das für Aufgaben wie Textsuche, Klassifizierung und Clustering entwickelt wurde. Es bietet mehrsprachige Unterstützung für über 100 Sprachen, mit 8 Milliarden Parametern und einer Kontextlänge von 32k. Es zeichnet sich durch seine Leistung und Flexibilität aus und ermöglicht benutzerdefinierte Dimensionen und Anweisungen für maßgeschneiderte Anwendungen."},"name":"Qwen 3 Embedding 4B","openWeights":true,"knowledge":"2024-12-01","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"huggingface","contextLength":32000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.01","output":"0.00"},"eur":{"currency":"eur","input":"0.008627114","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.728Z"},{"id":"qwen3-livetranslate-flash-realtime","aliases":["qwen3-livetranslate-flash-realtime"],"name":"Qwen3-LiveTranslate Flash Realtime","knowledge":"2024-04-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"alibaba","contextLength":53248,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"10.00","output":"10.00"},"eur":{"currency":"eur","input":"8.627114","output":"8.627114"}}}],"lastImportedAt":"2025-11-19T12:06:32.684Z"},{"id":"qwen3-max","aliases":["alibaba/qwen3-max","qwen/qwen3-max","qwen3-max"],"description":{"en":"Qwen3-Max is an updated release built on the Qwen3 series, offering major improvements in reasoning, instruction following, multilingual support, and long-tail knowledge coverage compared to the January 2025 version. It delivers higher accuracy in math, coding, logic, and science tasks, follows complex instructions in Chinese and English more reliably, reduces hallucinations, and produces higher-quality responses for open-ended Q&A, writing, and conversation. The model supports over 100 languages with stronger translation and commonsense reasoning, and is optimized for retrieval-augmented generation (RAG) and tool calling, though it does not include a dedicated “thinking” mode.","de":"Qwen3-Max ist eine aktualisierte Version, die auf der Qwen3-Reihe aufbaut und im Vergleich zur Version vom Januar 2025 wesentliche Verbesserungen in den Bereichen logisches Denken, Befolgung von Anweisungen, mehrsprachige Unterstützung und Abdeckung von Long-Tail-Wissen bietet. Es bietet eine höhere Genauigkeit bei Mathematik-, Codierungs-, Logik- und Wissenschaftsaufgaben, befolgt komplexe Anweisungen auf Chinesisch und Englisch zuverlässiger, reduziert Halluzinationen und erzeugt qualitativ hochwertigere Antworten bei offenen Fragen und Antworten, beim Schreiben und bei Gesprächen. Das Modell unterstützt über 100 Sprachen mit stärkerer Übersetzung und Commonsense Reasoning und ist für Retrieval-Augmented Generation (RAG) und Tool Calling optimiert, obwohl es keinen speziellen \"Denk\"-Modus enthält."},"name":"Qwen3 Max","reasoning":true,"toolCalling":true,"knowledge":"2024-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","max_tokens","presence_penalty","response_format","seed","tool_choice","top_p"],"providers":[{"providerId":"alibaba","contextLength":262144,"outputLimit":65536,"price":{"usd":{"currency":"usd","input":"1.20","output":"6.00"},"eur":{"currency":"eur","input":"1.03525368","output":"5.1762684"}}},{"providerId":"vercel","contextLength":262144,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"1.20","output":"6.00"},"eur":{"currency":"eur","input":"1.03525368","output":"5.1762684"}}},{"providerId":"openrouter","contextLength":256000,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"1.20","output":"6.00"},"eur":{"currency":"eur","input":"1.03525368","output":"5.1762684"}}},{"providerId":"iflowcn","contextLength":256000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.689Z"},{"id":"qwen3-next-80b","aliases":["qwen3-next-80b"],"name":"Qwen 3 Next 80b","toolCalling":true,"openWeights":true,"knowledge":"2025-07-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"venice","contextLength":262144,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.35","output":"1.90"},"eur":{"currency":"eur","input":"0.3","output":"1.63"}}}],"lastImportedAt":"2025-12-10T00:21:34.179Z","outputLimit":8192,"contextLength":262144},{"id":"qwen3-next-80b-a3b-instruct","aliases":["qwen/qwen3-next-80b-a3b-instruct-2509","alibaba/qwen3-next-80b-a3b-instruct","qwen/qwen3-next-80b-a3b-instruct","qwen3-next-80b-a3b-instruct-2509","qwen3-next-80b-a3b-instruct"],"description":{"en":"Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code generation, knowledge QA, and multilingual use, while remaining robust on alignment and formatting. Compared with prior Qwen3 instruct variants, it focuses on higher throughput and stability on ultra-long inputs and multi-turn dialogues, making it well-suited for RAG, tool use, and agentic workflows that require consistent final answers rather than visible chain-of-thought. The model employs scaling-efficient training and decoding to improve parameter efficiency and inference speed, and has been validated on a broad set of public benchmarks where it reaches or approaches larger Qwen3 systems in several categories while outperforming earlier mid-sized baselines. It is best used as a general assistant, code helper, and long-context task solver in production settings where deterministic, instruction-following outputs are preferred.","de":"Qwen3-Next-80B-A3B-Instruct ist ein auf Anweisungen abgestimmtes Chat-Modell der Qwen3-Next-Serie, das für schnelle, stabile Antworten ohne \"Denkspuren\" optimiert ist. Es zielt auf komplexe Aufgaben in den Bereichen Argumentation, Codegenerierung, Wissens-QA und mehrsprachige Nutzung ab und bleibt dabei robust in Bezug auf Ausrichtung und Formatierung. Im Vergleich zu früheren Qwen3-Instruct-Varianten liegt der Schwerpunkt auf höherem Durchsatz und Stabilität bei ultralangen Eingaben und Multi-Turn-Dialogen, wodurch es sich gut für RAG, den Einsatz von Werkzeugen und agentenbasierte Workflows eignet, die konsistente endgültige Antworten und keine sichtbaren Gedankenketten erfordern. Das Modell nutzt skalierungseffizientes Training und Dekodierung, um die Parametereffizienz und die Inferenzgeschwindigkeit zu verbessern. Es wurde an einer Vielzahl öffentlicher Benchmarks validiert, bei denen es in mehreren Kategorien größere Qwen3-Systeme erreicht oder an diese herankommt, während es frühere mittelgroße Basissysteme übertrifft. Er eignet sich am besten als allgemeiner Assistent, Code-Helfer und Löser von Aufgaben mit langem Kontext in Produktionsumgebungen, in denen deterministische, anweisungsgetreue Ausgaben bevorzugt werden."},"name":"Qwen3-Next 80B-A3B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.43","output":"1.72"}}},{"providerId":"nvidia","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.50","output":"2.00"},"eur":{"currency":"eur","input":"0.43","output":"1.72"}}},{"providerId":"chutes","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.80"},"eur":{"currency":"eur","input":"0.09","output":"0.69"}}},{"providerId":"helicone","contextLength":262000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.14","output":"1.40"},"eur":{"currency":"eur","input":"0.12","output":"1.2"}}},{"providerId":"huggingface","contextLength":262144,"outputLimit":66536,"price":{"usd":{"currency":"usd","input":"0.25","output":"1.00"},"eur":{"currency":"eur","input":"0.21","output":"0.86"}}},{"providerId":"openrouter","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.14","output":"1.40"},"eur":{"currency":"eur","input":"0.12","output":"1.2"}}},{"providerId":"io-net","contextLength":262144,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.80"},"eur":{"currency":"eur","input":"0.09","output":"0.69"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.131Z","outputLimit":4096,"contextLength":131072},{"id":"qwen3-next-80b-a3b-thinking","aliases":["qwen/qwen3-next-80b-a3b-thinking-2509","alibaba/qwen3-next-80b-a3b-thinking","qwen/qwen3-next-80b-a3b-thinking","qwen3-next-80b-a3b-thinking-2509","qwen3-next-80b-a3b-thinking"],"description":{"en":"Qwen3-Next-80B-A3B-Thinking is a reasoning-first chat model in the Qwen3-Next line that outputs structured “thinking” traces by default. It’s designed for hard multi-step problems; math proofs, code synthesis/debugging, logic, and agentic planning, and reports strong results across knowledge, reasoning, coding, alignment, and multilingual evaluations. Compared with prior Qwen3 variants, it emphasizes stability under long chains of thought and efficient scaling during inference, and it is tuned to follow complex instructions while reducing repetitive or off-task behavior. The model is suitable for agent frameworks and tool use (function calling), retrieval-heavy workflows, and standardized benchmarking where step-by-step solutions are required. It supports long, detailed completions and leverages throughput-oriented techniques (e.g., multi-token prediction) for faster generation. Note that it operates in thinking-only mode.","de":"Qwen3-Next-80B-A3B-Thinking ist ein Reasoning-First-Chat-Modell der Qwen3-Next-Reihe, das standardmäßig strukturierte \"Denk\"-Spuren ausgibt. Es wurde für schwierige, mehrstufige Probleme entwickelt: mathematische Beweise, Codesynthese/Debugging, Logik und agenturische Planung. Es liefert starke Ergebnisse in den Bereichen Wissen, Argumentation, Codierung, Ausrichtung und mehrsprachige Evaluierungen. Im Vergleich zu früheren Qwen3-Varianten liegt der Schwerpunkt auf der Stabilität bei langen Gedankenketten und der effizienten Skalierung während der Inferenz, und es ist darauf abgestimmt, komplexen Anweisungen zu folgen und gleichzeitig repetitives oder Off-Task-Verhalten zu reduzieren. Das Modell eignet sich für Agenten-Frameworks und den Einsatz von Tools (Funktionsaufrufe), abruflastige Workflows und standardisiertes Benchmarking, bei dem Schritt-für-Schritt-Lösungen erforderlich sind. Es unterstützt lange, detaillierte Vervollständigungen und nutzt durchsatzorientierte Techniken (z.B. Multi-Token-Vorhersage) für eine schnellere Generierung. Beachten Sie, dass es im reinen Denkmodus arbeitet."},"name":"Qwen3-Next 80B-A3B (Thinking)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-12-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.50","output":"6.00"},"eur":{"currency":"eur","input":"0.43","output":"5.21"}}},{"providerId":"nvidia","contextLength":262144,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"vercel","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.50","output":"6.00"},"eur":{"currency":"eur","input":"0.43","output":"5.21"}}},{"providerId":"huggingface","contextLength":262144,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.30","output":"2.00"},"eur":{"currency":"eur","input":"0.26","output":"1.74"}}},{"providerId":"openrouter","contextLength":131072,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.14","output":"1.40"},"eur":{"currency":"eur","input":"0.12","output":"1.22"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-25T00:19:20.553Z","outputLimit":16384,"contextLength":131072},{"id":"qwen3-omni-flash","aliases":["qwen3-omni-flash"],"name":"Qwen3-Omni Flash","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":65536,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.43","output":"1.66"},"eur":{"currency":"eur","input":"0.370965902","output":"1.432100924"}}}],"lastImportedAt":"2025-11-19T12:06:32.687Z"},{"id":"qwen3-omni-flash-realtime","aliases":["qwen3-omni-flash-realtime"],"name":"Qwen3-Omni Flash Realtime","toolCalling":true,"knowledge":"2024-04-01","input":["text","image","audio","video"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":65536,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.52","output":"1.99"},"eur":{"currency":"eur","input":"0.448609928","output":"1.716795686"}}}],"lastImportedAt":"2025-11-19T12:06:32.687Z"},{"id":"qwen3-vl-235b-a22b","aliases":["qwen3-vl-235b-a22b"],"name":"Qwen3-VL 235B-A22B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.70","output":"2.80"},"eur":{"currency":"eur","input":"0.60389798","output":"2.41559192"}}}],"lastImportedAt":"2025-11-19T12:06:32.686Z"},{"id":"qwen3-vl-235b-a22b-instruct","aliases":["qwen/qwen3-vl-235b-a22b-instruct","qwen3-vl-235b-a22b-instruct"],"description":{"en":"Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies strong text generation with visual understanding across images and video. The Instruct model targets general vision-language use (VQA, document parsing, chart/table extraction, multilingual OCR). The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning. Beyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows—turning sketches or mockups into code and assisting with UI debugging—while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents.","de":"Qwen3-VL-235B-A22B Instruct ist ein offenes multimodales Modell, das starke Texterzeugung mit visuellem Verständnis über Bilder und Videos hinweg vereint. Das Instruct-Modell zielt auf die allgemeine Verwendung von Bildsprache ab (VQA, Dokumentenparsing, Extraktion von Diagrammen/Tabellen, mehrsprachige OCR). Die Serie legt den Schwerpunkt auf robuste Wahrnehmung (Erkennung verschiedener realer und synthetischer Kategorien), räumliches Verständnis (2D/3D-Erdung) und visuelles Langzeitverständnis, mit konkurrenzfähigen Ergebnissen bei öffentlichen multimodalen Benchmarks für Wahrnehmung und logisches Denken. Über die Analyse hinaus unterstützt Qwen3-VL die Interaktion mit Agenten und die Nutzung von Werkzeugen: Es kann komplexen Anweisungen in Dialogen mit mehreren Bildern und Drehungen folgen, Text an Videozeitlinien für präzise zeitliche Abfragen anpassen und GUI-Elemente für Automatisierungsaufgaben bedienen. Die Modelle ermöglichen auch visuelle Coding-Workflows - die Umwandlung von Skizzen oder Mockups in Code und die Unterstützung beim UI-Debugging - und bieten gleichzeitig eine starke reine Textleistung, die mit den Flaggschiff-Sprachmodellen von Qwen3 vergleichbar ist. Damit eignet sich Qwen3-VL für Produktionsszenarien, die KI für Dokumente, mehrsprachige OCR, Software/UI-Assistenz, räumliche/verkörperte Aufgaben und Forschung zu Vision-Language-Agents umfassen."},"name":"Qwen3 VL 235B A22B Instruct","toolCalling":true,"openWeights":true,"knowledge":"2025-09-01","input":["text","image","video"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_logprobs","top_p"],"providers":[{"providerId":"chutes","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}},{"providerId":"helicone","contextLength":256000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.50"},"eur":{"currency":"eur","input":"0.26","output":"1.29"}}},{"providerId":"openrouter","contextLength":262144,"price":{"usd":{"currency":"usd","input":"0.2","output":"1.2"},"eur":{"currency":"eur","input":"0.17","output":"1.03"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.408Z","outputLimit":16384,"contextLength":256000},{"id":"qwen3-vl-235b-a22b-thinking","aliases":["qwen/qwen3-vl-235b-a22b-thinking","qwen3-vl-235b-a22b-thinking"],"description":{"en":"Qwen3-VL-235B-A22B Thinking is a multimodal model that unifies strong text generation with visual understanding across images and video. The Thinking model is optimized for multimodal reasoning in STEM and math. The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning. Beyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows, turning sketches or mockups into code and assisting with UI debugging, while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents.","de":"Qwen3-VL-235B-A22B Thinking ist ein multimodales Modell, das eine starke Texterzeugung mit visuellem Verständnis über Bilder und Videos verbindet. Das Thinking-Modell ist für multimodales Denken in MINT und Mathematik optimiert. Die Serie legt den Schwerpunkt auf robuste Wahrnehmung (Erkennung verschiedener realer und synthetischer Kategorien), räumliches Verständnis (2D/3D-Erdung) und langes visuelles Verstehen, mit konkurrenzfähigen Ergebnissen bei öffentlichen multimodalen Benchmarks für Wahrnehmung und logisches Denken. Über die Analyse hinaus unterstützt Qwen3-VL die Interaktion mit Agenten und die Nutzung von Werkzeugen: Es kann komplexen Anweisungen in Dialogen mit mehreren Bildern und Drehungen folgen, Text an Videozeitlinien für präzise zeitliche Abfragen anpassen und GUI-Elemente für Automatisierungsaufgaben bedienen. Die Modelle ermöglichen auch visuelle Coding-Workflows, indem sie Skizzen oder Mockups in Code umwandeln und beim UI-Debugging helfen, während sie eine starke reine Textleistung beibehalten, die mit den Flaggschiff-Sprachmodellen von Qwen3 vergleichbar ist. Damit eignet sich Qwen3-VL für Produktionsszenarien, die KI für Dokumente, mehrsprachige OCR, Software/UI-Assistenz, räumliche/verkörperte Aufgaben und Forschung zu Vision-Language-Agents umfassen."},"name":"Qwen3 VL 235B A22B Thinking","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-09-23","input":["text","image"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"chutes","contextLength":262144,"outputLimit":262144,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}},{"providerId":"openrouter","contextLength":262144,"price":{"usd":{"currency":"usd","input":"0.3","output":"1.2"},"eur":{"currency":"eur","input":"0.25881342","output":"1.03525368"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.720Z"},{"id":"qwen3-vl-235b-cloud","aliases":["qwen3-vl-235b-cloud"],"name":"Qwen3-VL 235B Instruct","toolCalling":true,"openWeights":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.424Z","outputLimit":8192,"contextLength":200000},{"id":"qwen3-vl-235b-instruct","aliases":["qwen3-vl-235b-instruct"],"name":"Qwen3-VL 235B Instruct","toolCalling":true,"openWeights":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-23T00:23:30.529Z","outputLimit":8192,"contextLength":200000,"deprecated":true},{"id":"qwen3-vl-235b-instruct-cloud","aliases":["qwen3-vl-235b-instruct-cloud"],"name":"Qwen3-VL 235B Instruct","toolCalling":true,"openWeights":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"ollama-cloud","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-24T00:22:13.767Z","outputLimit":8192,"contextLength":200000},{"id":"qwen3-vl-30b-a3b","aliases":["qwen3-vl-30b-a3b"],"name":"Qwen3-VL 30B-A3B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.80"},"eur":{"currency":"eur","input":"0.17254228","output":"0.69016912"}}}],"lastImportedAt":"2025-11-19T12:06:32.686Z"},{"id":"qwen3-vl-30b-a3b-instruct","aliases":["qwen/qwen3-vl-30b-a3b-instruct","qwen3-vl-30b-a3b-instruct"],"name":"Qwen: Qwen3 VL 30B A3B Instruct","description":{"en":"Qwen3-VL-30B-A3B-Instruct is a multimodal model that unifies strong text generation with visual understanding for images and videos. Its Instruct variant optimizes instruction-following for general multimodal tasks. It excels in perception of real-world/synthetic categories, 2D/3D spatial grounding, and long-form visual comprehension, achieving competitive multimodal benchmark results. For agentic use, it handles multi-image multi-turn instructions, video timeline alignments, GUI automation, and visual coding from sketches to debugged UI. Text performance matches flagship Qwen3 models, suiting document AI, OCR, UI assistance, spatial tasks, and agent research.","de":"Qwen3-VL-30B-A3B-Instruct ist ein multimodales Modell, das eine starke Texterzeugung mit visuellem Verständnis für Bilder und Videos vereint. Seine Instruct-Variante optimiert das Befolgen von Anweisungen für allgemeine multimodale Aufgaben. Es zeichnet sich durch die Wahrnehmung realer/synthetischer Kategorien, räumliches 2D/3D-Verständnis und langes visuelles Verstehen aus und erzielt konkurrenzfähige multimodale Benchmark-Ergebnisse. Für den agenturischen Einsatz bewältigt es Multibild-Multiturn-Anweisungen, Video-Timeline-Ausrichtungen, GUI-Automatisierung und visuelle Kodierung von Skizzen bis hin zur fehlerfreien Benutzeroberfläche. Die Textleistung entspricht den Flaggschiffmodellen von Qwen3 und eignet sich für Dokument-KI, OCR, UI-Assistenz, räumliche Aufgaben und Agentenforschung."},"knowledge":"2025-10-06","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.14","output":"1"},"eur":{"currency":"eur","input":"0.12","output":"0.86"}}}],"lastImportedAt":"2025-12-08T00:21:42.510Z","contextLength":131072},{"id":"qwen3-vl-30b-a3b-thinking","aliases":["qwen/qwen3-vl-30b-a3b-thinking","qwen3-vl-30b-a3b-thinking"],"name":"Qwen: Qwen3 VL 30B A3B Thinking","description":{"en":"Qwen3-VL-30B-A3B-Thinking is a multimodal model that unifies strong text generation with visual understanding for images and videos. Its Thinking variant enhances reasoning in STEM, math, and complex tasks. It excels in perception of real-world/synthetic categories, 2D/3D spatial grounding, and long-form visual comprehension, achieving competitive multimodal benchmark results. For agentic use, it handles multi-image multi-turn instructions, video timeline alignments, GUI automation, and visual coding from sketches to debugged UI. Text performance matches flagship Qwen3 models, suiting document AI, OCR, UI assistance, spatial tasks, and agent research.","de":"Qwen3-VL-30B-A3B-Thinking ist ein multimodales Modell, das eine starke Texterzeugung mit visuellem Verständnis für Bilder und Videos verbindet. Seine Thinking-Variante verbessert das logische Denken in MINT-, Mathematik- und komplexen Aufgaben. Es zeichnet sich durch die Wahrnehmung realer/synthetischer Kategorien, räumliches 2D/3D-Verständnis und langes visuelles Verstehen aus und erzielt konkurrenzfähige multimodale Benchmark-Ergebnisse. Für den agenturischen Einsatz bewältigt er Multibild-Multiturn-Anweisungen, Video-Timeline-Ausrichtungen, GUI-Automatisierung und visuelle Kodierung von Skizzen bis hin zur fehlerfreien Benutzeroberfläche. Die Textleistung entspricht den Flaggschiffmodellen von Qwen3 und eignet sich für Dokument-KI, OCR, UI-Assistenz, räumliche Aufgaben und Agentenforschung."},"knowledge":"2025-10-06","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.16","output":"0.8"},"eur":{"currency":"eur","input":"0.14","output":"0.69"}}}],"lastImportedAt":"2025-11-25T00:19:20.579Z","contextLength":131072},{"id":"qwen3-vl-8b-instruct","aliases":["qwen/qwen3-vl-8b-instruct","qwen3-vl-8b-instruct"],"name":"Qwen: Qwen3 VL 8B Instruct","description":{"en":"Qwen3-VL-8B-Instruct is a multimodal vision-language model from the Qwen3-VL series, built for high-fidelity understanding and reasoning across text, images, and video. It features improved multimodal fusion with Interleaved-MRoPE for long-horizon temporal reasoning, DeepStack for fine-grained visual-text alignment, and text-timestamp alignment for precise event localization. The model supports a native 256K-token context window, extensible to 1M tokens, and handles both static and dynamic media inputs for tasks like document parsing, visual question answering, spatial reasoning, and GUI control. It achieves text understanding comparable to leading LLMs while expanding OCR coverage to 32 languages and enhancing robustness under varied visual conditions.","de":"Qwen3-VL-8B-Instruct ist ein multimodales Vision-Language-Modell aus der Qwen3-VL-Reihe, das für ein realitätsnahes Verstehen und Schließen von Texten, Bildern und Videos entwickelt wurde. Es verfügt über eine verbesserte multimodale Fusion mit Interleaved-MRoPE für zeitliche Schlussfolgerungen mit langem Horizont, DeepStack für feinkörniges Bild-Text-Alignment und Text-Zeitstempel-Alignment für präzise Ereignislokalisierung. Das Modell unterstützt ein natives Kontextfenster mit 256K Token, das auf 1M Token erweiterbar ist, und verarbeitet sowohl statische als auch dynamische Medieneingaben für Aufgaben wie Dokumenten-Parsing, visuelle Fragebeantwortung, räumliche Schlussfolgerungen und GUI-Steuerung. Es erreicht ein Textverständnis, das mit dem führender LLMs vergleichbar ist, während die OCR-Abdeckung auf 32 Sprachen erweitert und die Robustheit unter verschiedenen visuellen Bedingungen verbessert wird."},"knowledge":"2025-10-14","toolCalling":true,"input":["image","text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.064","output":"0.4"},"eur":{"currency":"eur","input":"0.06","output":"0.35"}}}],"lastImportedAt":"2025-11-25T00:19:20.579Z","contextLength":131072},{"id":"qwen3-vl-8b-thinking","aliases":["qwen/qwen3-vl-8b-thinking","qwen3-vl-8b-thinking"],"name":"Qwen: Qwen3 VL 8B Thinking","description":{"en":"Qwen3-VL-8B-Thinking is the reasoning-optimized variant of the Qwen3-VL-8B multimodal model, designed for advanced visual and textual reasoning across complex scenes, documents, and temporal sequences. It integrates enhanced multimodal alignment and long-context processing (native 256K, expandable to 1M tokens) for tasks such as scientific visual analysis, causal inference, and mathematical reasoning over image or video inputs. Compared to the Instruct edition, the Thinking version introduces deeper visual-language fusion and deliberate reasoning pathways that improve performance on long-chain logic tasks, STEM problem-solving, and multi-step video understanding. It achieves stronger temporal grounding via Interleaved-MRoPE and timestamp-aware embeddings, while maintaining robust OCR, multilingual comprehension, and text generation on par with large text-only LLMs.","de":"Qwen3-VL-8B-Thinking ist die für schlussfolgerndes Denken optimierte Variante des multimodalen Qwen3-VL-8B-Modells, das für fortgeschrittenes visuelles und textuelles Schlussfolgern über komplexe Szenen, Dokumente und zeitliche Sequenzen entwickelt wurde. Es integriert erweiterte multimodale Ausrichtung und Verarbeitung langer Kontexte (nativ 256K, erweiterbar auf 1M Token) für Aufgaben wie wissenschaftliche visuelle Analyse, kausale Inferenz und mathematische Schlussfolgerungen über Bild- oder Videoeingaben. Im Vergleich zur Instruct-Edition führt die Thinking-Version eine tiefere visuelle Sprachfusion und gezielte Argumentationswege ein, die die Leistung bei langkettigen Logikaufgaben, beim Lösen von MINT-Problemen und beim mehrstufigen Verstehen von Videos verbessern. Durch Interleaved-MRoPE und zeitstempelbasierte Einbettungen wird eine stärkere zeitliche Verankerung erreicht, während gleichzeitig eine robuste OCR, ein mehrsprachiges Verständnis und eine Texterzeugung auf dem Niveau großer reiner Text-LLMs beibehalten werden."},"knowledge":"2025-10-14","reasoning":true,"toolCalling":true,"input":["image","text"],"output":["text"],"parameters":["include_reasoning","max_tokens","presence_penalty","reasoning","response_format","seed","structured_outputs","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":256000,"price":{"usd":{"currency":"usd","input":"0.18","output":"2.1"},"eur":{"currency":"eur","input":"0.155288052","output":"1.81169394"}}}],"lastImportedAt":"2025-11-19T12:06:32.762Z"},{"id":"qwen3-vl-instruct","aliases":["alibaba/qwen3-vl-instruct","qwen3-vl-instruct"],"name":"Qwen3 VL Instruct","toolCalling":true,"openWeights":true,"knowledge":"2025-04-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":131072,"outputLimit":129024,"price":{"usd":{"currency":"usd","input":"0.70","output":"2.80"},"eur":{"currency":"eur","input":"0.60389798","output":"2.41559192"}}}],"lastImportedAt":"2025-11-19T12:06:32.704Z"},{"id":"qwen3-vl-plus","aliases":["qwen3-vl-plus"],"name":"Qwen3-VL Plus","reasoning":true,"toolCalling":true,"knowledge":"2024-12-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":262144,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.20","output":"1.60"},"eur":{"currency":"eur","input":"0.17254228","output":"1.38033824"}}},{"providerId":"iflowcn","contextLength":256000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.688Z"},{"id":"qwen3-vl-thinking","aliases":["alibaba/qwen3-vl-thinking","qwen3-vl-thinking"],"name":"Qwen3 VL Thinking","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-09-01","input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":131072,"outputLimit":129024,"price":{"usd":{"currency":"usd","input":"0.70","output":"8.40"},"eur":{"currency":"eur","input":"0.60389798","output":"7.24677576"}}}],"lastImportedAt":"2025-11-19T12:06:32.704Z"},{"id":"qwerky-72b","aliases":["featherless/qwerky-72b","qwerky-72b"],"name":"Qwerky 72B","openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"openrouter","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.750Z"},{"id":"qwq-32b","aliases":["workers-ai/qwq-32b","qwen/qwq-32b:free","qwq-32b:free","qwen/qwq-32b","qwq-32b"],"description":{"en":"QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.","de":"QwQ ist das Denkmodell der Qwen-Reihe. Im Vergleich zu herkömmlichen, auf Befehle abgestimmten Modellen kann QwQ, das in der Lage ist, zu denken und zu schlussfolgern, bei nachgelagerten Aufgaben, insbesondere bei schwierigen Problemen, eine deutlich bessere Leistung erzielen. QwQ-32B ist ein mittelgroßes Denkmodell, das in der Lage ist, eine konkurrenzfähige Leistung gegenüber den modernsten Denkmodellen zu erzielen, z. B. DeepSeek-R1, o1-mini."},"name":"QwQ 32B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p"],"providers":[{"providerId":"alibaba-cn","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.29","output":"0.86"},"eur":{"currency":"eur","input":"0.25","output":"0.74"}}},{"providerId":"cloudflare-workers-ai","contextLength":24000,"outputLimit":24000,"price":{"usd":{"currency":"usd","input":"0.66","output":"1.00"},"eur":{"currency":"eur","input":"0.57","output":"0.86"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"1.00"},"eur":{"currency":"eur","input":"0","output":"0.86"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.4"},"eur":{"currency":"eur","input":"0.13","output":"0.34"}}}],"freeProviders":[{"providerId":"openrouter","providerName":"OpenRouter","contextLength":32768,"outputLimit":32768,"price":{"currency":"usd","input":"0.00","output":"0.00"}}],"defaultParameters":{},"lastImportedAt":"2025-12-08T00:21:39.961Z","outputLimit":8192,"contextLength":24000},{"id":"qwq-32b-arliai-rpr-v1","aliases":["arliai/qwq-32b-arliai-rpr-v1","qwq-32b-arliai-rpr-v1"],"description":{"en":"QwQ-32B-ArliAI-RpR-v1 is a 32B parameter model fine-tuned from Qwen/QwQ-32B using a curated creative writing and roleplay dataset originally developed for the RPMax series. It is designed to maintain coherence and reasoning across long multi-turn conversations by introducing explicit reasoning steps per dialogue turn, generated and refined using the base model itself. The model was trained using RS-QLORA+ on 8K sequence lengths and supports up to 128K context windows (with practical performance around 32K). It is optimized for creative roleplay and dialogue generation, with an emphasis on minimizing cross-context repetition while preserving stylistic diversity.","de":"QwQ-32B-ArliAI-RpR-v1 ist ein 32B-Parameter-Modell, das aus Qwen/QwQ-32B unter Verwendung eines kuratierten Datensatzes für kreatives Schreiben und Rollenspiele, der ursprünglich für die RPMax-Serie entwickelt wurde, feinabgestimmt wurde. Es wurde entwickelt, um Kohärenz und Argumentation über lange Gespräche mit mehreren Runden hinweg aufrechtzuerhalten, indem explizite Argumentationsschritte pro Dialogrunde eingeführt werden, die mit dem Basismodell selbst generiert und verfeinert werden. Das Modell wurde mit RS-QLORA+ auf 8K Sequenzlängen trainiert und unterstützt bis zu 128K Kontextfenster (mit praktischer Leistung um 32K). Es ist für kreatives Rollenspiel und Dialoggenerierung optimiert, wobei der Schwerpunkt auf der Minimierung von kontextübergreifenden Wiederholungen bei gleichzeitiger Wahrung der stilistischen Vielfalt liegt."},"name":"QwQ 32B ArliAI RpR V1","reasoning":true,"openWeights":true,"knowledge":"2025-04-13","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"chutes","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.11"},"eur":{"currency":"eur","input":"0.03","output":"0.1"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.03","output":"0.11"},"eur":{"currency":"eur","input":"0.03","output":"0.1"}}}],"defaultParameters":{},"lastImportedAt":"2025-11-27T00:20:23.016Z","outputLimit":32768,"contextLength":32768},{"id":"qwq-plus","aliases":["qwq-plus"],"name":"QwQ Plus","reasoning":true,"toolCalling":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"alibaba","contextLength":131072,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.80","output":"2.40"},"eur":{"currency":"eur","input":"0.69016912","output":"2.07050736"}}}],"lastImportedAt":"2025-11-19T12:06:32.687Z"},{"id":"ray2","aliases":["lumalabs/ray2","ray2"],"name":"Ray2","toolCalling":true,"input":["text","image"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":5000,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:43.512Z","contextLength":5000},{"id":"reka-flash-3","aliases":["rekaai/reka-flash-3","reka-flash-3"],"description":{"en":"Reka Flash 3 is a 21B general-purpose reasoning model designed for low latency and on-device deployment. Trained on synthetic and public datasets, it excels in tasks requiring reasoning but is less suited for knowledge-intensive tasks. Primarily an English model, it supports basic interactions in other languages.","de":"Reka Flash 3 ist ein 21B Allzweck-Schlussfolgermodell, das für niedrige Latenzzeiten und den Einsatz auf dem Gerät entwickelt wurde. Es wurde auf synthetischen und öffentlichen Datensätzen trainiert und eignet sich hervorragend für Aufgaben, die logisches Denken erfordern, aber weniger für wissensintensive Aufgaben. Es ist in erster Linie ein englisches Modell, unterstützt aber auch grundlegende Interaktionen in anderen Sprachen."},"name":"Reka Flash 3","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"openrouter","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.752Z"},{"id":"relace-apply-3","aliases":["relace/relace-apply-3","relace-apply-3"],"name":"Relace: Relace Apply 3","description":{"en":"Relace Apply 3 is a specialized code-patching LLM that merges AI-suggested edits straight into your source files. It can apply updates from GPT-4o, Claude, and others into your files at 10,000 tokens/sec on average. The model requires the prompt to be in the following format: <instruction>{instruction}</instruction> <code>{initial_code}</code> <update>{edit_snippet}</update> Zero Data Retention is enabled for Relace. Learn more about this model in their [documentation](https://docs.relace.ai/api-reference/instant-apply/apply)","de":"Relace Apply 3 ist ein spezieller Code-Patching-LLM, der von der KI vorgeschlagene Änderungen direkt in Ihre Quelldateien einfügt. Es kann Aktualisierungen von GPT-4o, Claude und anderen in Ihre Dateien mit durchschnittlich 10.000 Token/Sek. einfügen. Das Modell verlangt, dass die Eingabeaufforderung das folgende Format hat: <Anweisung>{Anweisung}</Anweisung> <code>{Einführungscode}</code> <update>{edit_snippet}</update> Zero Data Retention ist für Relace aktiviert. Erfahren Sie mehr über dieses Modell in der [Dokumentation] (https://docs.relace.ai/api-reference/instant-apply/apply)"},"knowledge":"2025-09-26","input":["text"],"output":["text"],"parameters":["max_tokens","seed","stop"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":256000,"price":{"usd":{"currency":"usd","input":"0.85","output":"1.25"},"eur":{"currency":"eur","input":"0.73330469","output":"1.07838925"}}}],"lastImportedAt":"2025-11-19T12:06:32.763Z"},{"id":"relace-search","aliases":["relace/relace-search-20251208","relace-search-20251208","relace/relace-search","relace-search"],"name":"Relace: Relace Search","description":{"en":"The relace-search model uses 4-12 `view_file` and `grep` tools in parallel to explore a codebase and return relevant files to the user request. In contrast to RAG, relace-search performs agentic multi-step reasoning to produce highly precise results 4x faster than any frontier model. It's designed to serve as a subagent that passes its findings to an \"oracle\" coding agent, who orchestrates/performs the rest of the coding task. To use relace-search you need to build an appropriate agent harness, and parse the response for relevant information to hand off to the oracle. Read more about it in the [Relace documentation](https://docs.relace.ai/docs/fast-agentic-search/agent).","de":"Das relace-search-Modell verwendet 4-12 `view_file` und `grep` Werkzeuge parallel, um eine Codebasis zu erforschen und relevante Dateien auf die Benutzeranfrage hin zurückzugeben. Im Gegensatz zu RAG führt relace-search agentenbasiertes mehrstufiges Reasoning durch, um hochpräzise Ergebnisse 4x schneller als jedes Frontier-Modell zu liefern. Es ist so konzipiert, dass es als Subagent dient, der seine Ergebnisse an einen \"Orakel\"-Codieragenten weitergibt, der den Rest der Codieraufgabe orchestriert/ausführt. Um relace-search zu verwenden, müssen Sie ein entsprechendes Agentenkonzept erstellen und die Antwort auf relevante Informationen analysieren, um sie an das Orakel weiterzugeben. Lesen Sie mehr darüber in der [Relace-Dokumentation] (https://docs.relace.ai/docs/fast-agentic-search/agent)."},"knowledge":"2025-12-08","toolCalling":true,"input":["text"],"output":["text"],"parameters":["max_tokens","seed","stop","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":256000,"price":{"usd":{"currency":"usd","input":"1","output":"3"},"eur":{"currency":"eur","input":"0.86","output":"2.57"}}}],"lastImportedAt":"2025-12-09T00:20:58.793Z","contextLength":256000},{"id":"remm-slerp-l2-13b","aliases":["undi95/remm-slerp-l2-13b","remm-slerp-l2-13b"],"name":"ReMM SLERP 13B","description":{"en":"A recreation trial of the original MythoMax-L2-B13 but with updated models. #merge","de":"Ein Nachbauversuch des ursprünglichen MythoMax-L2-B13, aber mit aktualisierten Modellen. #verschmelzen"},"knowledge":"2023-07-22","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_a","top_k","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":6144,"price":{"usd":{"currency":"usd","input":"0.45","output":"0.65"},"eur":{"currency":"eur","input":"0.38822013","output":"0.56076241"}}}],"lastImportedAt":"2025-11-19T12:06:32.768Z"},{"id":"resnet-50","aliases":["resnet-50"],"name":"@cf/microsoft/resnet-50","openWeights":true,"input":["image"],"output":["text"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.626Z"},{"id":"ring-1t","aliases":["inclusionai/ring-1t","ring-1t"],"description":{"en":"Ring-1T is a trillion-parameter open-source text generation model designed for deep reasoning and natural language inference. It supports a context window of up to 128K tokens and demonstrates strong performance in tasks such as math competitions, code generation, and logical reasoning. It utilizes advanced reinforcement learning techniques for stability and efficiency, making it suitable for developers and researchers in AI applications.","de":"Ring-1T ist ein Open-Source-Textgenerierungsmodell mit Billionen von Parametern, das für Deep Reasoning und natürlichsprachliche Inferenz entwickelt wurde. Es unterstützt ein Kontextfenster von bis zu 128K Token und zeigt eine starke Leistung bei Aufgaben wie Mathematikwettbewerben, Codegenerierung und logischen Schlussfolgerungen. Es nutzt fortschrittliche Reinforcement-Learning-Techniken für Stabilität und Effizienz und eignet sich daher für Entwickler und Forscher von KI-Anwendungen."},"name":"Ring-1T","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2024-06-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"bailing","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.57","output":"2.29"},"eur":{"currency":"eur","input":"0.49","output":"1.99"}}},{"providerId":"zenmux","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"0.56","output":"2.24"},"eur":{"currency":"eur","input":"0.49","output":"1.94"}}}],"lastImportedAt":"2025-11-27T00:20:23.008Z","outputLimit":32000,"contextLength":128000},{"id":"rnj-1-instruct","aliases":["essentialai/rnj-1-instruct","rnj-1-instruct"],"description":{"en":"Rnj-1 is an 8B-parameter, dense, open-weight model family developed by Essential AI and trained from scratch with a focus on programming, math, and scientific reasoning. The model demonstrates strong performance across multiple programming languages, tool-use workflows, and agentic execution environments (e.g., mini-SWE-agent).","de":"Rnj-1 ist eine dichte, offen gewichtete Modellfamilie mit 8B-Parametern, die von Essential AI entwickelt und von Grund auf mit Schwerpunkt auf Programmierung, Mathematik und wissenschaftlicher Argumentation trainiert wurde. Das Modell zeigt eine starke Leistung in verschiedenen Programmiersprachen, Tool-Use-Workflows und agentenbasierten Ausführungsumgebungen (z. B. Mini-SWE-Agent)."},"name":"Rnj-1 Instruct","toolCalling":true,"openWeights":true,"knowledge":"2024-10-01","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","stop","structured_outputs","top_k","top_p"],"providers":[{"providerId":"togetherai","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.13","output":"0.13"}}},{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.13","output":"0.13"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-12T12:08:56.327Z","outputLimit":32768,"contextLength":32768},{"id":"rocinante-12b","aliases":["thedrummer/rocinante-12b-v1.1","thedrummer/rocinante-12b","rocinante-12b-v1.1","rocinante-12b"],"name":"TheDrummer: Rocinante 12B","description":{"en":"Rocinante 12B is designed for engaging storytelling and rich prose. Early testers have reported: - Expanded vocabulary with unique and expressive word choices - Enhanced creativity for vivid narratives - Adventure-filled and captivating stories","de":"Rocinante 12B ist für fesselnde Erzählungen und reichhaltige Prosa konzipiert. Erste Tester haben berichtet: - Erweitertes Vokabular mit einzigartiger und ausdrucksstarker Wortwahl - Gesteigerte Kreativität für lebendige Erzählungen - Abenteuerreiche und fesselnde Geschichten"},"knowledge":"2024-09-30","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.17","output":"0.43"},"eur":{"currency":"eur","input":"0.146660938","output":"0.370965902"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"router","aliases":["switchpoint/router","router"],"name":"Switchpoint Router","description":{"en":"Switchpoint AI's router instantly analyzes your request and directs it to the optimal AI from an ever-evolving library. As the world of LLMs advances, our router gets smarter, ensuring you always benefit from the industry's newest models without changing your workflow. This model is configured for a simple, flat rate per response here on OpenRouter. It's powered by the full routing engine from [Switchpoint AI](https://www.switchpoint.dev).","de":"Der Router von Switchpoint AI analysiert Ihre Anfrage sofort und leitet sie an die optimale KI aus einer sich ständig weiterentwickelnden Bibliothek weiter. Mit den Fortschritten in der Welt der LLMs wird unser Router immer intelligenter und stellt sicher, dass Sie immer von den neuesten Modellen der Branche profitieren, ohne Ihren Arbeitsablauf zu ändern. Dieses Modell ist für eine einfache Flatrate pro Antwort hier auf OpenRouter konfiguriert. Es wird von der vollständigen Routing-Engine von [Switchpoint AI](https://www.switchpoint.dev) unterstützt."},"knowledge":"2025-07-11","reasoning":true,"input":["text"],"output":["text"],"parameters":["include_reasoning","max_tokens","reasoning","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.85","output":"3.4"},"eur":{"currency":"eur","input":"0.73330469","output":"2.93321876"}}}],"lastImportedAt":"2025-11-19T12:06:32.764Z"},{"id":"runway","aliases":["runwayml/runway","runway"],"name":"Runway","toolCalling":true,"input":["text","image"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":256,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:36.086Z","contextLength":256},{"id":"runway-gen-4-turbo","aliases":["runwayml/runway-gen-4-turbo","runway-gen-4-turbo"],"name":"Runway-Gen-4-Turbo","toolCalling":true,"input":["text","image"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":256,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:36.331Z","contextLength":256},{"id":"sarvam-m","aliases":["sarvamai/sarvam-m:free","sarvam-m:free","sarvam-m"],"name":"Sarvam-M (free)","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-05-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"openrouter","contextLength":32768,"outputLimit":32768,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.752Z"},{"id":"sherlock-dash-alpha","aliases":["openrouter/sherlock-dash-alpha","sherlock-dash-alpha"],"description":{"en":"This is a cloaked model provided to the community to gather feedback. A frontier non-reasoning model that excels at tool calling, with a 1.8M context window and multimodal support. **Note:** All prompts and completions for this model are logged by the provider and may be used to improve the model.","de":"Dies ist ein getarntes Modell, das der Gemeinschaft zur Verfügung gestellt wird, um Feedback zu sammeln. Ein Grenzmodell, das sich durch ein 1,8-Meter-Kontextfenster und multimodale Unterstützung auszeichnet. **Hinweis:** Alle Eingabeaufforderungen und Vervollständigungen für dieses Modell werden vom Anbieter protokolliert und können zur Verbesserung des Modells verwendet werden."},"name":"Sherlock Dash Alpha","toolCalling":true,"knowledge":"2025-11-01","input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"openrouter","contextLength":1840000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-20T00:20:31.553Z","outputLimit":64000,"contextLength":1840000,"deprecated":true},{"id":"sherlock-think-alpha","aliases":["openrouter/sherlock-think-alpha","sherlock-think-alpha"],"description":{"en":"This is a cloaked model provided to the community to gather feedback. A frontier reasoning model that excels at tool calling, with a 1.8M context window and multimodal support. **Note:** All prompts and completions for this model are logged by the provider and may be used to improve the model.","de":"Dies ist ein getarntes Modell, das der Gemeinschaft zur Verfügung gestellt wird, um Feedback zu sammeln. Ein Grenzüberschreitungsmodell, das sich durch den Aufruf von Werkzeugen auszeichnet, mit einem Kontextfenster von 1,8 Millionen und multimodaler Unterstützung. **Hinweis:** Alle Eingabeaufforderungen und Vervollständigungen für dieses Modell werden vom Anbieter protokolliert und können zur Verbesserung des Modells verwendet werden."},"name":"Sherlock Think Alpha","reasoning":true,"toolCalling":true,"knowledge":"2025-11-01","input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"openrouter","contextLength":1840000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-20T00:20:31.553Z","outputLimit":64000,"contextLength":1840000,"deprecated":true},{"id":"skyfall-36b-v2","aliases":["thedrummer/skyfall-36b-v2","skyfall-36b-v2"],"name":"TheDrummer: Skyfall 36B V2","description":{"en":"Skyfall 36B v2 is an enhanced iteration of Mistral Small 2501, specifically fine-tuned for improved creativity, nuanced writing, role-playing, and coherent storytelling.","de":"Skyfall 36B v2 ist eine verbesserte Iteration von Mistral Small 2501, die speziell auf verbesserte Kreativität, nuanciertes Schreiben, Rollenspiel und kohärentes Geschichtenerzählen abgestimmt ist."},"knowledge":"2025-03-10","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.55","output":"0.8"},"eur":{"currency":"eur","input":"0.47","output":"0.69"}}}],"lastImportedAt":"2025-12-08T00:21:42.511Z","contextLength":32768},{"id":"smart-turn-v2","aliases":["workers-ai/smart-turn-v2","smart-turn-v2"],"name":"smart turn v2","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.980Z","outputLimit":16384,"contextLength":128000},{"id":"solar-mini","aliases":["solar-mini"],"name":"solar-mini","toolCalling":true,"knowledge":"2024-09-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"upstage","contextLength":32768,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.15","output":"0.15"},"eur":{"currency":"eur","input":"0.12940671","output":"0.12940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.694Z"},{"id":"solar-pro2","aliases":["solar-pro2"],"name":"solar-pro2","reasoning":true,"toolCalling":true,"knowledge":"2025-03-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"upstage","contextLength":65536,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.25","output":"0.25"},"eur":{"currency":"eur","input":"0.21567785","output":"0.21567785"}}}],"lastImportedAt":"2025-11-19T12:06:32.694Z"},{"id":"sonar","aliases":["perplexity/sonar","sonar"],"description":{"en":"Sonar is lightweight, affordable, fast, and simple to use — now featuring citations and the ability to customize sources. It is designed for companies seeking to integrate lightweight question-and-answer features optimized for speed.","de":"Sonar ist leichtgewichtig, erschwinglich, schnell und einfach zu bedienen - jetzt mit Zitaten und der Möglichkeit, Quellen anzupassen. Es wurde für Unternehmen entwickelt, die leichtgewichtige, auf Geschwindigkeit optimierte Frage-und-Antwort-Funktionen integrieren möchten."},"name":"Sonar","knowledge":"2025-01-01","input":["text","image"],"output":["text"],"parameters":["temperature","frequency_penalty","max_tokens","presence_penalty","top_k","top_p","web_search_options"],"providers":[{"providerId":"vercel","contextLength":127000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"1.00","output":"1.00"},"eur":{"currency":"eur","input":"0.86","output":"0.86"}}},{"providerId":"helicone","contextLength":127000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"1.00","output":"1.00"},"eur":{"currency":"eur","input":"0.86","output":"0.86"}}},{"providerId":"perplexity","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"1.00","output":"1.00"},"eur":{"currency":"eur","input":"0.86","output":"0.86"}}},{"providerId":"openrouter","contextLength":127072,"price":{"usd":{"currency":"usd","input":"1","output":"1"},"eur":{"currency":"eur","input":"0.86","output":"0.86"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.338Z","outputLimit":4096,"contextLength":127000},{"id":"sonar-deep-research","aliases":["perplexity/sonar-deep-research","sonar-deep-research"],"description":{"en":"Sonar Deep Research is a research-focused model designed for multi-step retrieval, synthesis, and reasoning across complex topics. It autonomously searches, reads, and evaluates sources, refining its approach as it gathers information. This enables comprehensive report generation across domains like finance, technology, health, and current events. Notes on Pricing ([Source](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-deep-research)) - Input tokens comprise of Prompt tokens (user prompt) + Citation tokens (these are processed tokens from running searches) - Deep Research runs multiple searches to conduct exhaustive research. Searches are priced at $5/1000 searches. A request that does 30 searches will cost $0.15 in this step. - Reasoning is a distinct step in Deep Research since it does extensive automated reasoning through all the material it gathers during its research phase. Reasoning tokens here are a bit different than the CoTs in the answer - these are tokens that we use to reason through the research material prior to generating the outputs via the CoTs. Reasoning tokens are priced at $3/1M tokens","de":"Sonar Deep Research ist ein forschungsorientiertes Modell, das für die mehrstufige Suche, Synthese und Argumentation in komplexen Themenbereichen entwickelt wurde. Es sucht, liest und bewertet selbstständig Quellen und verfeinert seinen Ansatz, während es Informationen sammelt. Dies ermöglicht die Erstellung umfassender Berichte in Bereichen wie Finanzen, Technologie, Gesundheit und aktuelle Ereignisse. Hinweise zur Preisgestaltung ([Quelle](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-deep-research)) - Eingabe-Token bestehen aus Prompt-Token (Benutzer-Prompt) + Citation-Token (das sind verarbeitete Token aus laufenden Suchen) - Deep Research führt mehrere Suchen durch, um eine umfassende Recherche durchzuführen. Der Preis für Recherchen beträgt 5 $/1000 Recherchen. Eine Anfrage, die 30 Suchvorgänge durchführt, kostet in diesem Schritt $0,15. - Reasoning ist ein separater Schritt in Deep Research, da es umfangreiche automatisierte Schlussfolgerungen aus dem gesamten Material zieht, das es während seiner Recherchephase gesammelt hat. Reasoning-Tokens unterscheiden sich ein wenig von den CoTs in der Antwort - es handelt sich um Tokens, die wir zum Reasoning des Forschungsmaterials verwenden, bevor wir die Ausgaben über die CoTs generieren. Der Preis für Reasoning-Token liegt bei $3/1M Token"},"name":"Perplexity Sonar Deep Research","reasoning":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","top_k","top_p","web_search_options"],"providers":[{"providerId":"helicone","contextLength":127000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"2","output":"8"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.722Z","outputLimit":4096,"contextLength":127000},{"id":"sonar-pro","aliases":["perplexity/sonar-pro","sonar-pro"],"description":{"en":"Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro) For enterprises seeking more advanced capabilities, the Sonar Pro API can handle in-depth, multi-step queries with added extensibility, like double the number of citations per search as Sonar on average. Plus, with a larger context window, it can handle longer and more nuanced searches and follow-up questions.","de":"Hinweis: Der Preis für Sonar Pro beinhaltet den Preis für die Perplexity-Suche. Siehe [Details hier] (https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro) Für Unternehmen, die fortschrittlichere Funktionen suchen, kann die Sonar Pro-API tiefgreifende, mehrstufige Abfragen mit zusätzlicher Erweiterbarkeit verarbeiten, z. B. doppelt so viele Zitate pro Suche wie Sonar im Durchschnitt. Außerdem können dank eines größeren Kontextfensters längere und differenziertere Suchen und Folgefragen bearbeitet werden."},"name":"Sonar Pro","knowledge":"2025-01-01","input":["text","image"],"output":["text"],"parameters":["temperature","frequency_penalty","max_tokens","presence_penalty","top_k","top_p","web_search_options"],"providers":[{"providerId":"vercel","contextLength":200000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"helicone","contextLength":200000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"perplexity","contextLength":200000,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}},{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"3","output":"15"},"eur":{"currency":"eur","input":"2.57","output":"12.87"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.339Z","outputLimit":4096,"contextLength":200000},{"id":"sonar-pro-search","aliases":["perplexity/sonar-pro-search","sonar-pro-search"],"name":"Perplexity: Sonar Pro Search","description":{"en":"Exclusively available on the OpenRouter API, Sonar Pro's new Pro Search mode is Perplexity's most advanced agentic search system. It is designed for deeper reasoning and analysis. Pricing is based on tokens plus $18 per thousand requests. This model powers the Pro Search mode on the Perplexity platform. Sonar Pro Search adds autonomous, multi-step reasoning to Sonar Pro. So, instead of just one query + synthesis, it plans and executes entire research workflows using tools.","de":"Der neue Pro-Suchmodus von Sonar Pro, der exklusiv über die OpenRouter-API verfügbar ist, ist das fortschrittlichste agentische Suchsystem von Perplexity. Es ist für tiefergehende Schlussfolgerungen und Analysen konzipiert. Die Preisgestaltung basiert auf Token plus $18 pro tausend Anfragen. Mit diesem Modell wird der Pro-Search-Modus auf der Perplexity-Plattform betrieben. Sonar Pro Search erweitert Sonar Pro um autonomes, mehrstufiges Reasoning. Anstelle einer einzigen Abfrage + Synthese werden ganze Recherche-Workflows mithilfe von Tools geplant und ausgeführt."},"knowledge":"2025-10-30","reasoning":true,"input":["text","image"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","structured_outputs","temperature","top_k","top_p","web_search_options"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":200000,"price":{"usd":{"currency":"usd","input":"3","output":"15"},"eur":{"currency":"eur","input":"2.5881342","output":"12.940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.761Z"},{"id":"sonar-reasoning","aliases":["perplexity/sonar-reasoning","sonar-reasoning"],"description":{"en":"Sonar Reasoning is a reasoning model provided by Perplexity based on [DeepSeek R1](/deepseek/deepseek-r1). It allows developers to utilize long chain of thought with built-in web search. Sonar Reasoning is uncensored and hosted in US datacenters.","de":"Sonar Reasoning ist ein von Perplexity bereitgestelltes Schlussfolgerungsmodell, das auf [DeepSeek R1](/deepseek/deepseek-r1) basiert. Es erlaubt Entwicklern, lange Gedankenketten mit eingebauter Websuche zu nutzen. Sonar Reasoning ist unzensiert und wird in US-Rechenzentren gehostet."},"name":"Sonar Reasoning","reasoning":true,"knowledge":"2025-01-01","input":["text"],"output":["text"],"parameters":["temperature","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","top_k","top_p","web_search_options"],"providers":[{"providerId":"vercel","contextLength":127000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"helicone","contextLength":127000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"perplexity","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"1.00","output":"5.00"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}},{"providerId":"openrouter","contextLength":127000,"price":{"usd":{"currency":"usd","input":"1","output":"5"},"eur":{"currency":"eur","input":"0.86","output":"4.29"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.338Z","outputLimit":4096,"contextLength":127000},{"id":"sonar-reasoning-pro","aliases":["perplexity/sonar-reasoning-pro","sonar-reasoning-pro"],"description":{"en":"Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro) Sonar Reasoning Pro is a premier reasoning model powered by DeepSeek R1 with Chain of Thought (CoT). Designed for advanced use cases, it supports in-depth, multi-step queries with a larger context window and can surface more citations per search, enabling more comprehensive and extensible responses.","de":"Hinweis: Der Preis für Sonar Pro beinhaltet den Preis für die Perplexity-Suche. Siehe [Details hier] (https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro) Sonar Reasoning Pro ist ein erstklassiges Reasoning-Modell auf Basis von DeepSeek R1 mit Chain of Thought (CoT). Es wurde für fortgeschrittene Anwendungsfälle entwickelt und unterstützt tiefgreifende, mehrstufige Abfragen mit einem größeren Kontextfenster und kann mehr Zitate pro Suche anzeigen, was umfassendere und erweiterbare Antworten ermöglicht."},"name":"Sonar Reasoning Pro","reasoning":true,"knowledge":"2025-01-01","input":["text","image"],"output":["text"],"parameters":["temperature","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","top_k","top_p","web_search_options"],"providers":[{"providerId":"vercel","contextLength":127000,"outputLimit":8000,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"helicone","contextLength":127000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"perplexity","contextLength":128000,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"2.00","output":"8.00"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}},{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"2","output":"8"},"eur":{"currency":"eur","input":"1.72","output":"6.87"}}}],"defaultParameters":{},"lastImportedAt":"2025-12-09T00:20:58.339Z","outputLimit":4096,"contextLength":127000},{"id":"sonoma-dusk-alpha","aliases":["openrouter/sonoma-dusk-alpha","sonoma-dusk-alpha"],"name":"Sonoma Dusk Alpha","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"openrouter","contextLength":2000000,"outputLimit":2000000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.751Z","deprecated":true},{"id":"sonoma-sky-alpha","aliases":["openrouter/sonoma-sky-alpha","sonoma-sky-alpha"],"name":"Sonoma Sky Alpha","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"openrouter","contextLength":2000000,"outputLimit":2000000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.751Z","deprecated":true},{"id":"sora-2","aliases":["openai/sora-2","sora-2"],"name":"Sora-2","toolCalling":true,"input":["text","image"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:36.563Z"},{"id":"sora-2-pro","aliases":["openai/sora-2-pro","sora-2-pro"],"name":"Sora-2-Pro","toolCalling":true,"input":["text","image"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:38.724Z"},{"id":"sorcererlm-8x22b","aliases":["raifle/sorcererlm-8x22b-bf16","raifle/sorcererlm-8x22b","sorcererlm-8x22b-bf16","sorcererlm-8x22b"],"name":"SorcererLM 8x22B","description":{"en":"SorcererLM is an advanced RP and storytelling model, built as a Low-rank 16-bit LoRA fine-tuned on [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b). - Advanced reasoning and emotional intelligence for engaging and immersive interactions - Vivid writing capabilities enriched with spatial and contextual awareness - Enhanced narrative depth, promoting creative and dynamic storytelling","de":"SorcererLM ist ein fortschrittliches RP- und Storytelling-Modell, das als Low-rank 16-bit LoRA aufgebaut ist und auf [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b) abgestimmt ist. - Fortgeschrittenes logisches Denken und emotionale Intelligenz für fesselnde und eindringliche Interaktionen - Lebendige Schreibfähigkeiten, angereichert mit räumlichem und kontextuellem Bewusstsein - Verbesserte erzählerische Tiefe zur Förderung kreativer und dynamischer Geschichtenerzählung"},"knowledge":"2024-11-08","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":16000,"price":{"usd":{"currency":"usd","input":"4.5","output":"4.5"},"eur":{"currency":"eur","input":"3.8822013","output":"3.8822013"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"spotlight","aliases":["arcee-ai/spotlight","spotlight"],"name":"Arcee AI: Spotlight","description":{"en":"Spotlight is a 7‑billion‑parameter vision‑language model derived from Qwen 2.5‑VL and fine‑tuned by Arcee AI for tight image‑text grounding tasks. It offers a 32 k‑token context window, enabling rich multimodal conversations that combine lengthy documents with one or more images. Training emphasized fast inference on consumer GPUs while retaining strong captioning, visual‐question‑answering, and diagram‑analysis accuracy. As a result, Spotlight slots neatly into agent workflows where screenshots, charts or UI mock‑ups need to be interpreted on the fly. Early benchmarks show it matching or out‑scoring larger VLMs such as LLaVA‑1.6 13 B on popular VQA and POPE alignment tests.","de":"Spotlight ist ein 7-Milliarden-Parameter-Vision-Language-Modell, das von Qwen 2.5-VL abgeleitet und von Arcee AI für enge Bild-Text-Grounding-Aufgaben feinabgestimmt wurde. Es bietet ein Kontextfenster mit 32 k-Token, das umfangreiche multimodale Konversationen ermöglicht, die lange Dokumente mit einem oder mehreren Bildern kombinieren. Beim Training wurde besonderer Wert auf eine schnelle Inferenz auf Consumer-GPUs gelegt, wobei die Genauigkeit von Untertiteln, visuellen Fragen und Diagrammanalysen beibehalten wurde. So fügt sich Spotlight nahtlos in die Arbeitsabläufe von Agenten ein, die Screenshots, Diagramme oder UI-Mock-ups im Handumdrehen interpretieren müssen. Erste Benchmarks zeigen, dass Spotlight bei den gängigen VQA- und POPE-Alignment-Tests mit größeren VLMs wie LLaVA-1.6 13 B mithalten oder diese sogar übertreffen kann."},"knowledge":"2025-05-05","input":["image","text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.18","output":"0.18"},"eur":{"currency":"eur","input":"0.155288052","output":"0.155288052"}}}],"lastImportedAt":"2025-11-19T12:06:32.765Z"},{"id":"sqlcoder-7b-2","aliases":["sqlcoder-7b-2"],"name":"@cf/defog/sqlcoder-7b-2","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":10000,"outputLimit":10000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.659Z","outputLimit":10000,"contextLength":10000},{"id":"stable-diffusion-v1-5-img2img","aliases":["stable-diffusion-v1-5-img2img"],"name":"@cf/runwayml/stable-diffusion-v1-5-img2img","openWeights":true,"input":["text"],"output":["image"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.027Z"},{"id":"stable-diffusion-v1-5-inpainting","aliases":["stable-diffusion-v1-5-inpainting"],"name":"@cf/runwayml/stable-diffusion-v1-5-inpainting","openWeights":true,"input":["text"],"output":["image"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.642Z"},{"id":"stable-diffusion-xl-base-1.0","aliases":["stable-diffusion-xl-base-1.0"],"name":"@cf/stabilityai/stable-diffusion-xl-base-1.0","openWeights":true,"input":["text"],"output":["image"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.560Z"},{"id":"stable-diffusion-xl-lightning","aliases":["stable-diffusion-xl-lightning"],"name":"@cf/bytedance/stable-diffusion-xl-lightning","openWeights":true,"input":["text"],"output":["image"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.669Z"},{"id":"stablediffusionxl","aliases":["stabilityai/stablediffusionxl","stablediffusionxl"],"name":"StableDiffusionXL","toolCalling":true,"input":["text","image"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":200,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:43.058Z","contextLength":200},{"id":"starling-lm-7b-beta","aliases":["starling-lm-7b-beta"],"name":"@hf/nexusflow/starling-lm-7b-beta","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.605Z","outputLimit":4096,"contextLength":4096},{"id":"step3","aliases":["stepfun-ai/step3","step3"],"name":"StepFun: Step3","description":{"en":"Step3 is a cutting-edge multimodal reasoning model—built on a Mixture-of-Experts architecture with 321B total parameters and 38B active. It is designed end-to-end to minimize decoding costs while delivering top-tier performance in vision–language reasoning. Through the co-design of Multi-Matrix Factorization Attention (MFA) and Attention-FFN Disaggregation (AFD), Step3 maintains exceptional efficiency across both flagship and low-end accelerators.","de":"Step3 ist ein hochmodernes multimodales Schlussfolgerungsmodell, das auf einer Mixture-of-Experts-Architektur mit insgesamt 321B Parametern und 38B aktiven Parametern basiert. Es ist von Anfang bis Ende darauf ausgelegt, die Dekodierungskosten zu minimieren und gleichzeitig eine erstklassige Leistung bei bildsprachlichen Schlussfolgerungen zu erbringen. Durch das Co-Design von Multi-Matrix Factorization Attention (MFA) und Attention-FFN Disaggregation (AFD) erreicht Step3 eine außergewöhnliche Effizienz sowohl bei Flaggschiff- als auch bei Low-End-Beschleunigern."},"knowledge":"2025-08-28","reasoning":true,"toolCalling":true,"input":["image","text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","reasoning","response_format","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":65536,"price":{"usd":{"currency":"usd","input":"0.57","output":"1.42"},"eur":{"currency":"eur","input":"0.491745498","output":"1.225050188"}}}],"lastImportedAt":"2025-11-19T12:06:32.763Z"},{"id":"stepfun-ai-step3","aliases":["stepfun-ai-step3"],"name":"stepfun-ai/step3","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":66000,"outputLimit":66000,"price":{"usd":{"currency":"usd","input":"0.57","output":"1.42"},"eur":{"currency":"eur","input":"0.5","output":"1.23"}}}],"lastImportedAt":"2025-11-26T00:20:48.401Z","outputLimit":66000,"contextLength":66000},{"id":"tako","aliases":["trytako/tako","tako"],"name":"Tako","toolCalling":true,"input":["text"],"output":["text"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":2048,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:47.566Z","contextLength":2048},{"id":"tencent-hunyuan-a13b-instruct","aliases":["tencent-hunyuan-a13b-instruct"],"name":"tencent/Hunyuan-A13B-Instruct","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.57"},"eur":{"currency":"eur","input":"0.12","output":"0.5"}}}],"lastImportedAt":"2025-11-26T00:20:50.639Z","outputLimit":131000,"contextLength":131000},{"id":"tencent-hunyuan-mt-7b","aliases":["tencent-hunyuan-mt-7b"],"name":"tencent/Hunyuan-MT-7B","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":33000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-26T00:20:47.504Z","outputLimit":33000,"contextLength":33000},{"id":"text-embedding-3-large","aliases":["text-embedding-3-large"],"name":"text-embedding-3-large","knowledge":"2024-01-01","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"azure","contextLength":8191,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.00"},"eur":{"currency":"eur","input":"0.11","output":"0"}}},{"providerId":"openai","contextLength":8191,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.00"},"eur":{"currency":"eur","input":"0.11","output":"0"}}},{"providerId":"azure-cognitive-services","contextLength":8191,"outputLimit":3072,"price":{"usd":{"currency":"usd","input":"0.13","output":"0.00"},"eur":{"currency":"eur","input":"0.11","output":"0"}}}],"lastImportedAt":"2025-11-30T00:23:20.696Z","outputLimit":3072,"contextLength":8191},{"id":"text-embedding-3-small","aliases":["text-embedding-3-small"],"name":"text-embedding-3-small","knowledge":"2024-01-01","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"azure","contextLength":8191,"outputLimit":1536,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.00"},"eur":{"currency":"eur","input":"0.02","output":"0"}}},{"providerId":"openai","contextLength":8191,"outputLimit":1536,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.00"},"eur":{"currency":"eur","input":"0.02","output":"0"}}},{"providerId":"azure-cognitive-services","contextLength":8191,"outputLimit":1536,"price":{"usd":{"currency":"usd","input":"0.02","output":"0.00"},"eur":{"currency":"eur","input":"0.02","output":"0"}}}],"lastImportedAt":"2025-11-30T00:23:20.544Z","outputLimit":1536,"contextLength":8191},{"id":"text-embedding-ada-002","aliases":["text-embedding-ada-002"],"name":"text-embedding-ada-002","knowledge":"2022-12-01","input":["text"],"output":["text"],"parameters":[],"providers":[{"providerId":"azure","contextLength":8192,"outputLimit":1536,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.00"},"eur":{"currency":"eur","input":"0.09","output":"0"}}},{"providerId":"openai","contextLength":8192,"outputLimit":1536,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.00"},"eur":{"currency":"eur","input":"0.09","output":"0"}}},{"providerId":"azure-cognitive-services","contextLength":8192,"outputLimit":1536,"price":{"usd":{"currency":"usd","input":"0.10","output":"0.00"},"eur":{"currency":"eur","input":"0.09","output":"0"}}}],"lastImportedAt":"2025-11-30T00:23:20.772Z","outputLimit":1536,"contextLength":8192},{"id":"thudm-glm-4-32b-0414","aliases":["thudm-glm-4-32b-0414"],"name":"THUDM/GLM-4-32B-0414","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":33000,"price":{"usd":{"currency":"usd","input":"0.27","output":"0.27"},"eur":{"currency":"eur","input":"0.23","output":"0.23"}}}],"lastImportedAt":"2025-11-26T00:20:50.724Z","outputLimit":33000,"contextLength":33000},{"id":"thudm-glm-4-9b-0414","aliases":["thudm-glm-4-9b-0414"],"name":"THUDM/GLM-4-9B-0414","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":33000,"outputLimit":33000,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.09"},"eur":{"currency":"eur","input":"0.08","output":"0.08"}}}],"lastImportedAt":"2025-11-26T00:20:48.234Z","outputLimit":33000,"contextLength":33000},{"id":"thudm-glm-4.1v-9b-thinking","aliases":["thudm-glm-4.1v-9b-thinking"],"name":"THUDM/GLM-4.1V-9B-Thinking","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":66000,"outputLimit":66000,"price":{"usd":{"currency":"usd","input":"0.04","output":"0.14"},"eur":{"currency":"eur","input":"0.03","output":"0.12"}}}],"lastImportedAt":"2025-11-26T00:20:48.484Z","outputLimit":66000,"contextLength":66000},{"id":"thudm-glm-z1-32b-0414","aliases":["thudm-glm-z1-32b-0414"],"name":"THUDM/GLM-Z1-32B-0414","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.57"},"eur":{"currency":"eur","input":"0.12","output":"0.5"}}}],"lastImportedAt":"2025-11-26T00:20:47.255Z","outputLimit":131000,"contextLength":131000},{"id":"thudm-glm-z1-9b-0414","aliases":["thudm-glm-z1-9b-0414"],"name":"THUDM/GLM-Z1-9B-0414","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.09"},"eur":{"currency":"eur","input":"0.08","output":"0.08"}}}],"lastImportedAt":"2025-11-26T00:20:45.814Z","outputLimit":131000,"contextLength":131000},{"id":"tinyllama-1.1b-chat-v1.0","aliases":["tinyllama-1.1b-chat-v1.0"],"name":"@cf/tinyllama/tinyllama-1.1b-chat-v1.0","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":2048,"outputLimit":2048,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.426Z","outputLimit":2048,"contextLength":2048},{"id":"tng-r1t-chimera","aliases":["tngtech/tng-r1t-chimera:free","tngtech/tng-r1t-chimera","tng-r1t-chimera:free","tng-r1t-chimera"],"name":"TNG: R1T Chimera (free)","description":{"en":"TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interaction. It is a derivate of the original TNG/DeepSeek-R1T-Chimera released in April 2025 and is available exclusively via Chutes and OpenRouter. Characteristics and improvements include: We think that it has a creative and pleasant personality. It has a preliminary EQ-Bench3 value of about 1305. It is quite a bit more intelligent than the original, albeit a slightly slower. It is much more think-token consistent, i.e. reasoning and answer blocks are properly delineated. Tool calling is much improved. TNG Tech, the model authors, ask that users follow the careful guidelines that Microsoft has created for their \"MAI-DS-R1\" DeepSeek-based model. These guidelines are available on Hugging Face (https://huggingface.co/microsoft/MAI-DS-R1).","de":"TNG-R1T-Chimera ist ein experimentelles LLM mit einem Faible für kreatives Storytelling und Charakterinteraktion. Es ist ein Derivat des ursprünglichen TNG/DeepSeek-R1T-Chimera, das im April 2025 veröffentlicht wurde und ausschließlich über Chutes und OpenRouter erhältlich ist. Zu den Merkmalen und Verbesserungen gehören: Wir denken, dass er eine kreative und angenehme Persönlichkeit hat. Er hat einen vorläufigen EQ-Bench3-Wert von etwa 1305. Es ist um einiges intelligenter als das Original, wenn auch ein wenig langsamer. Es ist viel konsistenter in Bezug auf Think-Token, d.h. Argumentations- und Antwortblöcke sind korrekt abgegrenzt. Der Aufruf von Werkzeugen wurde deutlich verbessert. TNG Tech, die Autoren des Modells, bitten die Benutzer, die sorgfältigen Richtlinien zu befolgen, die Microsoft für sein DeepSeek-basiertes Modell \"MAI-DS-R1\" erstellt hat. Diese Richtlinien sind auf Hugging Face (https://huggingface.co/microsoft/MAI-DS-R1) verfügbar."},"knowledge":"2025-11-26","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":163840,"price":{"usd":{"currency":"usd","input":"0.3","output":"1.2"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}}],"lastImportedAt":"2025-12-12T00:21:33.762Z","contextLength":163840},{"id":"tng-r1t-chimera-tee","aliases":["tngtech/tng-r1t-chimera-tee","tng-r1t-chimera-tee"],"name":"TNG R1T Chimera TEE","reasoning":true,"toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"chutes","contextLength":163840,"outputLimit":163840,"price":{"usd":{"currency":"usd","input":"0.30","output":"1.20"},"eur":{"currency":"eur","input":"0.26","output":"1.03"}}}],"lastImportedAt":"2025-12-12T00:21:33.614Z","outputLimit":163840,"contextLength":163840},{"id":"tongyi-deepresearch-30b-a3b","aliases":["alibaba/tongyi-deepresearch-30b-a3b:free","alibaba-nlp/tongyi-deepresearch-30b-a3b","alibaba/tongyi-deepresearch-30b-a3b","tongyi-deepresearch-30b-a3b:free","tongyi-deepresearch-30b-a3b"],"description":{"en":"Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, with 30 billion total parameters activating only 3 billion per token. It's optimized for long-horizon, deep information-seeking tasks and delivers state-of-the-art performance on benchmarks like Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA, GAIA, xbench-DeepSearch, and FRAMES. This makes it superior for complex agentic search, reasoning, and multi-step problem-solving compared to prior models. The model includes a fully automated synthetic data pipeline for scalable pre-training, fine-tuning, and reinforcement learning. It uses large-scale continual pre-training on diverse agentic data to boost reasoning and stay fresh. It also features end-to-end on-policy RL with a customized Group Relative Policy Optimization, including token-level gradients and negative sample filtering for stable training. The model supports ReAct for core ability checks and an IterResearch-based 'Heavy' mode for max performance through test-time scaling. It's ideal for advanced research agents, tool use, and heavy inference workflows.","de":"Tongyi DeepResearch ist ein von Tongyi Lab entwickeltes agentisches Großsprachenmodell mit insgesamt 30 Milliarden Parametern, die nur 3 Milliarden pro Token aktivieren. Es ist für Aufgaben mit langem Zeithorizont und tiefer Informationssuche optimiert und liefert Spitzenleistungen bei Benchmarks wie Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA, GAIA, xbench-DeepSearch und FRAMES. Dies macht es im Vergleich zu früheren Modellen zu einem überlegenen Werkzeug für komplexe agenturische Suche, logisches Denken und mehrstufige Problemlösungen. Das Modell umfasst eine vollautomatische Pipeline mit synthetischen Daten für skalierbares Vortraining, Feinabstimmung und Verstärkungslernen. Es nutzt ein umfangreiches, kontinuierliches Vortraining mit verschiedenen agentenbasierten Daten, um das Denkvermögen zu verbessern und auf dem neuesten Stand zu bleiben. Darüber hinaus bietet es End-to-End-On-Policy-RL mit einer angepassten Group Relative Policy Optimization, einschließlich Gradienten auf Token-Ebene und Negative Sample Filtering für stabiles Training. Das Modell unterstützt ReAct für Kernfähigkeitsüberprüfungen und einen IterResearch-basierten \"Heavy\"-Modus für maximale Leistung durch Testzeitskalierung. Es ist ideal für fortgeschrittene Forschungsagenten, den Einsatz von Tools und umfangreiche Inferenz-Workflows."},"name":"Tongyi DeepResearch 30B A3B","reasoning":true,"toolCalling":true,"openWeights":true,"knowledge":"2025-09-18","input":["text"],"output":["text"],"parameters":["temperature","tools","frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","tool_choice","top_k","top_p","min_p"],"providers":[{"providerId":"chutes","contextLength":131072,"outputLimit":131072,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.09","output":"0.4"},"eur":{"currency":"eur","input":"0.077644026","output":"0.34508456"}}}],"freeProviders":[{"providerId":"openrouter","contextLength":131072,"price":{"currency":"usd","input":"0","output":"0"}}],"defaultParameters":{},"lastImportedAt":"2025-11-19T12:06:32.719Z"},{"id":"tongyi-intent-detect-v3","aliases":["tongyi-intent-detect-v3"],"name":"Tongyi Intent Detect V3","knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"alibaba-cn","contextLength":8192,"outputLimit":1024,"price":{"usd":{"currency":"usd","input":"0.06","output":"0.14"},"eur":{"currency":"eur","input":"0.051762684","output":"0.120779596"}}}],"lastImportedAt":"2025-11-19T12:06:32.715Z"},{"id":"topazlabs","aliases":["topazlabs-co/topazlabs","topazlabs"],"name":"TopazLabs","toolCalling":true,"input":["text"],"output":["image"],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":204,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:43.291Z","contextLength":204},{"id":"trinity-mini","aliases":["arcee-ai/trinity-mini-20251201","arcee-ai/trinity-mini:free","trinity-mini-20251201","arcee-ai/trinity-mini","trinity-mini:free","trinity-mini"],"name":"Arcee AI: Trinity Mini (free)","description":{"en":"Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 experts with 8 active per token. Engineered for efficient reasoning over long contexts (131k) with robust function calling and multi-step agent workflows.","de":"Trinity Mini ist ein 26B-Parameter (3B aktiv) Sparse Mixture-of-Experts Sprachmodell mit 128 Experten mit 8 aktiven pro Token. Es wurde für effiziente Schlussfolgerungen über lange Kontexte (131k) mit robusten Funktionsaufrufen und mehrstufigen Agenten-Workflows entwickelt."},"knowledge":"2025-12-01","reasoning":true,"toolCalling":true,"input":["text"],"output":["text"],"parameters":["include_reasoning","max_tokens","reasoning","response_format","structured_outputs","temperature","tool_choice","tools","top_k","top_p","frequency_penalty","logit_bias","min_p","presence_penalty","repetition_penalty","stop"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.045","output":"0.15"},"eur":{"currency":"eur","input":"0.04","output":"0.13"}}}],"lastImportedAt":"2025-12-02T00:21:07.418Z","contextLength":131072},{"id":"tstars2.0","aliases":["tstars2.0"],"name":"TStars-2.0","toolCalling":true,"knowledge":"2024-01-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"iflowcn","contextLength":128000,"outputLimit":64000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.753Z"},{"id":"uform-gen2-qwen-500m","aliases":["uform-gen2-qwen-500m"],"name":"@cf/unum/uform-gen2-qwen-500m","openWeights":true,"input":["image","text"],"output":["text"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.929Z"},{"id":"ui-tars-1.5-7b","aliases":["bytedance-seed/ui-tars-1.5-7b","bytedance/ui-tars-1.5-7b","ui-tars-1.5-7b"],"name":"ByteDance: UI-TARS 7B ","description":{"en":"UI-TARS-1.5 is a multimodal vision-language agent optimized for GUI-based environments, including desktop interfaces, web browsers, mobile systems, and games. Built by ByteDance, it builds upon the UI-TARS framework with reinforcement learning-based reasoning, enabling robust action planning and execution across virtual interfaces. This model achieves state-of-the-art results on a range of interactive and grounding benchmarks, including OSworld, WebVoyager, AndroidWorld, and ScreenSpot. It also demonstrates perfect task completion across diverse Poki games and outperforms prior models in Minecraft agent tasks. UI-TARS-1.5 supports thought decomposition during inference and shows strong scaling across variants, with the 1.5 version notably exceeding the performance of earlier 72B and 7B checkpoints.","de":"UI-TARS-1.5 ist ein multimodaler Vision-Language-Agent, der für GUI-basierte Umgebungen wie Desktop-Oberflächen, Web-Browser, mobile Systeme und Spiele optimiert ist. Er wurde von ByteDance entwickelt und baut auf dem UI-TARS-Framework auf, das auf Reinforcement Learning basiert und eine robuste Aktionsplanung und -ausführung über virtuelle Schnittstellen hinweg ermöglicht. Dieses Modell erzielt bei einer Reihe interaktiver und grundlegender Benchmarks, darunter OSworld, WebVoyager, AndroidWorld und ScreenSpot, Spitzenergebnisse. Es zeigt außerdem eine perfekte Aufgabenerfüllung bei verschiedenen Poki-Spielen und übertrifft frühere Modelle bei Minecraft-Agentenaufgaben. UI-TARS-1.5 unterstützt die Gedankenzerlegung während der Inferenz und zeigt eine starke Skalierung über alle Varianten hinweg, wobei die Version 1.5 die Leistung früherer 72B- und 7B-Checkpoints deutlich übertrifft."},"knowledge":"2025-07-22","input":["image","text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":128000,"price":{"usd":{"currency":"usd","input":"0.1","output":"0.2"},"eur":{"currency":"eur","input":"0.08627114","output":"0.17254228"}}}],"lastImportedAt":"2025-11-19T12:06:32.764Z"},{"id":"una-cybertron-7b-v2-bf16","aliases":["una-cybertron-7b-v2-bf16"],"name":"@cf/fblgit/una-cybertron-7b-v2-bf16","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":15000,"outputLimit":15000,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.923Z","outputLimit":15000,"contextLength":15000},{"id":"unslopnemo-12b","aliases":["thedrummer/unslopnemo-12b-v4.1","thedrummer/unslopnemo-12b","unslopnemo-12b-v4.1","unslopnemo-12b"],"name":"TheDrummer: UnslopNemo 12B","description":{"en":"UnslopNemo v4.1 is the latest addition from the creator of Rocinante, designed for adventure writing and role-play scenarios.","de":"UnslopNemo v4.1 ist das neueste Produkt des Schöpfers von Rocinante, das für das Schreiben von Abenteuern und Rollenspielszenarien entwickelt wurde."},"knowledge":"2024-11-08","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","presence_penalty","response_format","stop","structured_outputs","temperature","tool_choice","tools","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":32768,"price":{"usd":{"currency":"usd","input":"0.4","output":"0.4"},"eur":{"currency":"eur","input":"0.34508456","output":"0.34508456"}}}],"lastImportedAt":"2025-11-19T12:06:32.766Z"},{"id":"v0-1.0-md","aliases":["vercel/v0-1.0-md","v0-1.0-md"],"name":"v0-1.0-md","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.5881342","output":"12.940671"}}},{"providerId":"v0","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.5881342","output":"12.940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.706Z"},{"id":"v0-1.5-lg","aliases":["v0-1.5-lg"],"name":"v0-1.5-lg","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"v0","contextLength":512000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"15.00","output":"75.00"},"eur":{"currency":"eur","input":"12.940671","output":"64.703355"}}}],"lastImportedAt":"2025-11-19T12:06:32.753Z"},{"id":"v0-1.5-md","aliases":["vercel/v0-1.5-md","v0-1.5-md"],"name":"v0-1.5-md","reasoning":true,"toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"vercel","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.5881342","output":"12.940671"}}},{"providerId":"v0","contextLength":128000,"outputLimit":32000,"price":{"usd":{"currency":"usd","input":"3.00","output":"15.00"},"eur":{"currency":"eur","input":"2.5881342","output":"12.940671"}}}],"lastImportedAt":"2025-11-19T12:06:32.706Z"},{"id":"venice-uncensored","aliases":["venice-uncensored"],"name":"Venice Uncensored 1.1","openWeights":true,"knowledge":"2023-10-01","input":["text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"venice","contextLength":32768,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.20","output":"0.90"},"eur":{"currency":"eur","input":"0.17","output":"0.77"}}}],"lastImportedAt":"2025-12-10T00:21:34.096Z","outputLimit":8192,"contextLength":32768},{"id":"veo-2","aliases":["google/veo-2","veo-2"],"name":"Veo-2","toolCalling":true,"input":["text"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:42.379Z","contextLength":480},{"id":"veo-3","aliases":["google/veo-3","veo-3"],"name":"Veo-3","toolCalling":true,"input":["text"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:41.689Z","contextLength":480},{"id":"veo-3-fast","aliases":["google/veo-3-fast","veo-3-fast"],"name":"Veo-3-Fast","toolCalling":true,"input":["text"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:41.923Z","contextLength":480},{"id":"veo-3.1","aliases":["google/veo-3.1","veo-3.1"],"name":"Veo-3.1","toolCalling":true,"input":["text"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:40.999Z","contextLength":480},{"id":"veo-3.1-fast","aliases":["google/veo-3.1-fast","veo-3.1-fast"],"name":"Veo-3.1-Fast","toolCalling":true,"input":["text"],"output":[],"parameters":["tools"],"providers":[{"providerId":"poe","contextLength":480,"price":{"usd":{"currency":"usd"}}}],"lastImportedAt":"2025-11-21T07:05:42.606Z","contextLength":480},{"id":"virtuoso-large","aliases":["arcee-ai/virtuoso-large","virtuoso-large"],"name":"Arcee AI: Virtuoso Large","description":{"en":"Virtuoso‑Large is Arcee's top‑tier general‑purpose LLM at 72 B parameters, tuned to tackle cross‑domain reasoning, creative writing and enterprise QA. Unlike many 70 B peers, it retains the 128 k context inherited from Qwen 2.5, letting it ingest books, codebases or financial filings wholesale. Training blended DeepSeek R1 distillation, multi‑epoch supervised fine‑tuning and a final DPO/RLHF alignment stage, yielding strong performance on BIG‑Bench‑Hard, GSM‑8K and long‑context Needle‑In‑Haystack tests. Enterprises use Virtuoso‑Large as the \"fallback\" brain in Conductor pipelines when other SLMs flag low confidence. Despite its size, aggressive KV‑cache optimizations keep first‑token latency in the low‑second range on 8× H100 nodes, making it a practical production‑grade powerhouse.","de":"Virtuoso-Large ist Arcees Top-Tier-Allzweck-LLM mit 72 B-Parametern, der für bereichsübergreifendes Reasoning, kreatives Schreiben und QA in Unternehmen optimiert ist. Im Gegensatz zu vielen 70-B-Parametern behält er den von Qwen 2.5 geerbten 128-k-Kontext bei, so dass er Bücher, Codebases oder Finanzberichte in großem Umfang aufnehmen kann. Das Training ist eine Mischung aus DeepSeek R1-Destillation, überwachter Feinabstimmung mit mehreren Epochen und einer abschließenden DPO/RLHF-Anpassungsphase, die eine starke Leistung bei BIG-Bench-Hard-, GSM-8K- und Needle-In-Haystack-Tests mit langem Kontext ergibt. Unternehmen setzen Virtuoso-Large als \"Fallback\"-Gehirn in Conductor-Pipelines ein, wenn andere SLMs geringes Vertrauen signalisieren. Trotz seiner Größe sorgen aggressive KV-Cache-Optimierungen dafür, dass die First-Token-Latenzzeit auf 8× H100-Knoten im niedrigen Sekundenbereich liegt, was ihn zu einem praktischen, produktionsgerechten Kraftpaket macht."},"knowledge":"2025-05-05","toolCalling":true,"input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","max_tokens","min_p","presence_penalty","repetition_penalty","stop","temperature","tool_choice","tools","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":131072,"price":{"usd":{"currency":"usd","input":"0.75","output":"1.2"},"eur":{"currency":"eur","input":"0.64703355","output":"1.03525368"}}}],"lastImportedAt":"2025-11-19T12:06:32.765Z"},{"id":"weaver","aliases":["mancer/weaver","weaver"],"name":"Mancer: Weaver (alpha)","description":{"en":"An attempt to recreate Claude-style verbosity, but don't expect the same level of coherence or memory. Meant for use in roleplay/narrative situations.","de":"Ein Versuch, die Wortgewalt im Stil von Claude nachzubilden, aber erwarten Sie nicht das gleiche Maß an Kohärenz oder Gedächtnis. Zur Verwendung in Rollenspielen/Erzählungen gedacht."},"knowledge":"2023-08-02","input":["text"],"output":["text"],"parameters":["frequency_penalty","logit_bias","logprobs","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","temperature","top_a","top_k","top_logprobs","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":8000,"price":{"usd":{"currency":"usd","input":"1.125","output":"1.125"},"eur":{"currency":"eur","input":"0.970550325","output":"0.970550325"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"whisper","aliases":["workers-ai/whisper","whisper"],"name":"@cf/openai/whisper","openWeights":true,"input":["audio","text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:40.506Z","outputLimit":16384,"contextLength":128000},{"id":"whisper-large-v3","aliases":["openai/whisper-large-v3","whisper-large-v3"],"description":{"en":"Whisper large-v3 is a multilingual automatic speech recognition (ASR) model trained on 5M hours of audio data. It supports transcription and translation across multiple languages, with improved performance over previous versions. Use cases include speech transcriptions, translations, and potential applications in accessibility, though caution is advised regarding ethical implications.","de":"Whisper large-v3 ist ein mehrsprachiges automatisches Spracherkennungsmodell (ASR), das auf 5 Millionen Stunden Audiodaten trainiert wurde. Es unterstützt die Transkription und Übersetzung in mehrere Sprachen, wobei die Leistung gegenüber früheren Versionen verbessert wurde. Zu den Anwendungsfällen gehören Sprachtranskriptionen, Übersetzungen und potenzielle Anwendungen im Bereich der Barrierefreiheit, wobei hinsichtlich der ethischen Implikationen Vorsicht geboten ist."},"name":"Whisper Large v3","openWeights":true,"knowledge":"2023-09-01","input":["audio"],"output":["text"],"parameters":[],"providers":[{"providerId":"nvidia","outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"scaleway","outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.693Z"},{"id":"whisper-large-v3-turbo","aliases":["workers-ai/whisper-large-v3-turbo","whisper-large-v3-turbo"],"name":"@cf/openai/whisper-large-v3-turbo","openWeights":true,"input":["audio","text"],"output":["text"],"parameters":["temperature"],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}},{"providerId":"cloudflare-ai-gateway","contextLength":128000,"outputLimit":16384,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.555Z","outputLimit":16384,"contextLength":128000},{"id":"whisper-tiny-en","aliases":["whisper-tiny-en"],"name":"@cf/openai/whisper-tiny-en","openWeights":true,"input":["audio"],"output":["text"],"parameters":[],"providers":[{"providerId":"cloudflare-workers-ai","price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.652Z"},{"id":"wizardlm-2-8x22b","aliases":["microsoft/wizardlm-2-8x22b","wizardlm-2-8x22b"],"name":"WizardLM-2 8x22B","description":{"en":"WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models. It is an instruct finetune of [Mixtral 8x22B](/models/mistralai/mixtral-8x22b). To read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/). #moe","de":"WizardLM-2 8x22B ist das fortschrittlichste Wizard-Modell von Microsoft AI. Es zeigt eine äußerst wettbewerbsfähige Leistung im Vergleich zu führenden proprietären Modellen und übertrifft durchweg alle existierenden Open-Source-Modelle auf dem neuesten Stand der Technik. Es handelt sich um eine Feinabstimmung von [Mixtral 8x22B](/models/mistralai/mixtral-8x22b). Um mehr über die Veröffentlichung des Modells zu erfahren, [klicken Sie hier](https://wizardlm.github.io/WizardLM2/). #moe"},"knowledge":"2024-04-16","input":["text"],"output":["text"],"parameters":["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","temperature","top_k","top_p"],"defaultParameters":{},"providers":[{"providerId":"openrouter","contextLength":65536,"price":{"usd":{"currency":"usd","input":"0.48","output":"0.48"},"eur":{"currency":"eur","input":"0.414101472","output":"0.414101472"}}}],"lastImportedAt":"2025-11-19T12:06:32.767Z"},{"id":"z-ai-glm-4.5","aliases":["z-ai-glm-4.5"],"name":"z-ai/GLM-4.5","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.40","output":"2.00"},"eur":{"currency":"eur","input":"0.35","output":"1.74"}}}],"lastImportedAt":"2025-11-26T00:20:46.995Z","outputLimit":131000,"contextLength":131000},{"id":"z-ai-glm-4.5-air","aliases":["z-ai-glm-4.5-air"],"name":"z-ai/GLM-4.5-Air","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.86"},"eur":{"currency":"eur","input":"0.12","output":"0.75"}}}],"lastImportedAt":"2025-11-26T00:20:45.177Z","outputLimit":131000,"contextLength":131000},{"id":"zai-glm-4.6","aliases":["zai-glm-4.6"],"name":"Z.AI GLM-4.6","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cerebras","contextLength":131072,"outputLimit":40960,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-11-19T12:06:32.760Z"},{"id":"zai-org-glm-4.5","aliases":["zai-org-glm-4.5"],"name":"zai-org/GLM-4.5","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.40","output":"2.00"},"eur":{"currency":"eur","input":"0.35","output":"1.74"}}}],"lastImportedAt":"2025-11-26T00:20:48.740Z","outputLimit":131000,"contextLength":131000},{"id":"zai-org-glm-4.5-air","aliases":["zai-org-glm-4.5-air"],"name":"zai-org/GLM-4.5-Air","toolCalling":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":131000,"outputLimit":131000,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.86"},"eur":{"currency":"eur","input":"0.12","output":"0.75"}}}],"lastImportedAt":"2025-11-26T00:20:48.097Z","outputLimit":131000,"contextLength":131000},{"id":"zai-org-glm-4.5v","aliases":["zai-org-glm-4.5v"],"name":"zai-org/GLM-4.5V","toolCalling":true,"input":["text","image"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"siliconflow","contextLength":66000,"outputLimit":66000,"price":{"usd":{"currency":"usd","input":"0.14","output":"0.86"},"eur":{"currency":"eur","input":"0.12","output":"0.75"}}}],"lastImportedAt":"2025-11-26T00:20:47.090Z","outputLimit":66000,"contextLength":66000},{"id":"zai-org-glm-4.6","aliases":["zai-org-glm-4.6"],"name":"GLM 4.6","toolCalling":true,"openWeights":true,"knowledge":"2024-04-01","input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"venice","contextLength":202752,"outputLimit":8192,"price":{"usd":{"currency":"usd","input":"0.85","output":"2.75"},"eur":{"currency":"eur","input":"0.73","output":"2.37"}}},{"providerId":"siliconflow","contextLength":205000,"outputLimit":205000,"price":{"usd":{"currency":"usd","input":"0.50","output":"1.90"},"eur":{"currency":"eur","input":"0.43","output":"1.64"}}}],"lastImportedAt":"2025-11-30T00:23:18.828Z","outputLimit":8192,"contextLength":202752},{"id":"zephyr-7b-beta-awq","aliases":["zephyr-7b-beta-awq"],"name":"@hf/thebloke/zephyr-7b-beta-awq","toolCalling":true,"openWeights":true,"input":["text"],"output":["text"],"parameters":["temperature","tools"],"providers":[{"providerId":"cloudflare-workers-ai","contextLength":4096,"outputLimit":4096,"price":{"usd":{"currency":"usd","input":"0.00","output":"0.00"},"eur":{"currency":"eur","input":"0","output":"0"}}}],"lastImportedAt":"2025-12-08T00:21:41.854Z","outputLimit":4096,"contextLength":4096}],"providers":[{"id":"agentrouter","name":"AgentRouter"},{"id":"aihubmix","name":"AIHubMix"},{"id":"alibaba","name":"Alibaba (China)"},{"id":"alibaba-cn","name":"Alibaba (China)"},{"id":"amazon-bedrock","name":"Amazon Bedrock"},{"id":"anthropic","name":"Anthropic"},{"id":"azure","name":"Azure"},{"id":"azure-cognitive-services","name":"Azure Cognitive Services"},{"id":"bailing","name":"Bailing"},{"id":"baseten","name":"Baseten"},{"id":"cerebras","name":"Cerebras"},{"id":"chutes","name":"Chutes"},{"id":"cloudflare-ai-gateway","name":"Cloudflare AI Gateway"},{"id":"cloudflare-workers-ai","name":"Cloudflare Workers AI"},{"id":"cortecs","name":"Cortecs"},{"id":"deepinfra","name":"Deep Infra"},{"id":"deepseek","name":"DeepSeek"},{"id":"fastrouter","name":"FastRouter"},{"id":"fireworks-ai","name":"Fireworks AI"},{"id":"github-copilot","name":"GitHub Copilot"},{"id":"github-models","name":"GitHub Models"},{"id":"google","name":"Google"},{"id":"google-vertex","name":"Vertex"},{"id":"groq","name":"Groq"},{"id":"helicone","name":"Helicone"},{"id":"huggingface","name":"Hugging Face"},{"id":"iflowcn","name":"iFlow"},{"id":"inception","name":"Inception"},{"id":"inference","name":"Inference"},{"id":"io-intelligence","name":"IO Intelligence"},{"id":"io-net","name":"IO.NET"},{"id":"kimi-for-coding","name":"Kimi For Coding"},{"id":"llama","name":"Llama"},{"id":"lmstudio","name":"LMStudio"},{"id":"lucidquery","name":"LucidQuery AI"},{"id":"minimax","name":"Minimax"},{"id":"mistral","name":"Mistral"},{"id":"modelscope","name":"ModelScope"},{"id":"moonshotai","name":"Moonshot AI (China)"},{"id":"morph","name":"Morph"},{"id":"nebius","name":"Nebius Token Factory"},{"id":"nvidia","name":"Nvidia"},{"id":"ollama-cloud","name":"Ollama Cloud"},{"id":"openai","name":"OpenAI"},{"id":"opencode","name":"OpenCode Zen"},{"id":"openrouter","name":"OpenRouter"},{"id":"ovhcloud","name":"OVHcloud AI Endpoints"},{"id":"perplexity","name":"Perplexity"},{"id":"poe","name":"Poe"},{"id":"requesty","name":"Requesty"},{"id":"sap-ai-core","name":"SAP AI Core"},{"id":"scaleway","name":"Scaleway"},{"id":"siliconflow","name":"SiliconFlow"},{"id":"submodel","name":"submodel"},{"id":"synthetic","name":"Synthetic"},{"id":"togetherai","name":"Together AI"},{"id":"upstage","name":"Upstage"},{"id":"v0","name":"v0"},{"id":"venice","name":"Venice AI"},{"id":"vercel","name":"Vercel AI Gateway"},{"id":"vultr","name":"Vultr"},{"id":"wandb","name":"Weights & Biases"},{"id":"xai","name":"xAI"},{"id":"zai","name":"Z.AI"},{"id":"zai-coding-plan","name":"Z.AI Coding Plan"},{"id":"zenmux","name":"ZenMux"},{"id":"zhipuai","name":"Zhipu AI Coding Plan"}]}